{"meta":{"title":"Postlude's IT Blog","subtitle":"개발 블로그","description":null,"author":"HSD","url":"https://postlude.github.io","root":"/"},"pages":[],"posts":[{"title":"Mac에서 키보드 세팅 변경하기","slug":"mac-key-change","date":"2022-02-06T08:59:51.000Z","updated":"2022-02-06T14:20:58.822Z","comments":true,"path":"2022/02/06/mac-key-change/","link":"","permalink":"https://postlude.github.io/2022/02/06/mac-key-change/","excerpt":"","text":"최근 이직을 하게 되면서 처음으로 맥을 사용하게 되었습니다.(어떻게 보면 신기하게 보일 수도 있을 것 같습니다. 개발 일을 하면서 맥을 처음 쓴다는 게..) 그런데 키보드 배열이나 단축키 같은 것들이 윈도우와는 완전히 달라서 처음에 적응하는데 시간이 좀 걸렸습니다.처음에는 맥에 적응을 할까도 생각해봤지만 이직을 해서 바쁜 와중에 단축키에 시간을 뺏기느니, 차라리 맥 세팅을 바꿔서 빠르게 사용할 수 있도록 하는게 낫겠다는 생각이 들었습니다.(물론 이것도 전체 시간을 다 합치면 좀 걸리긴 했습니다.) 저는 맥을 무조건 저와 같은 방식으로 세팅해야 한다고 생각하지 않습니다.그냥 이런 방식으로 세팅하는 방법이 있다 정도로만 이해해주시면 될 것 같습니다. 0. 키 입력 확인세팅을 하면서 키 입력을 확인하고 싶으시면 아래 사이트에서 확인이 가능합니다. Keyboard Checker 1. Control &#x2F; Command 변경제일 처음에 한 작업은 Control 키와 Command 키를 변경한 것입니다.기본 키 배열로 쓰시는 분들도 있을 수 있겠지만 저는 윈도우의 복사, 붙여넣기의 ctrl c, v가 너무 익숙해져서 변경했습니다. [설정 - 키보드 - 보조 키]에서 Control과 Command키를 서로 바꿔줍니다.(저는 다른 세팅을 변경해서 다른 키보드로 나오는데 신경쓰지 말고 변경해주시면 됩니다.) 2. 한&#x2F;영 키 변경맥에서 한&#x2F;영 키는 caps lock과 같은 키를 씁니다. 저는 윈도우에 맞춰진 외부 키보드를 사용하기 때문에이걸 윈도우에서 쓰던 한&#x2F;영 키로 변경하도록 하겠습니다. 먼저 여기에서 Karabiner를 설치합니다. 그 후 Preferences에서 아래와 같이 세팅합니다. 이 세팅의 목적은 right option 키를 존재하지 않는 다른 키로 맵핑하기 위함입니다.(다만 이유는 알 수 없으나 F17이 아닌 다른 키로 맵핑했을 때는 정상적으로 동작하지 않더군요.) 이후 [설정 - 키보드 - 단축키]에서 이전 입력 소스 선택을 F17로 변경하시면 키보드의 한&#x2F;영 키를 사용할 수 있게 됩니다. 추가적으로 한&#x2F;영 키를 사용할 수 있게 되었으니 caps lock키를 이용한 한&#x2F;영 변경은 필요가 없어졌기 때문에[설정 - 키보드 - 입력 소스]에서 해제하도록 하겠습니다. 3. 추가 키 맵핑위에서 설치한 Karabiner를 통해 추가적으로 다른 키 맵핑들을 세팅할 수 있습니다. [Preferences - Complex modifications]에서 Add rule을 통해 키를 맵핑할 수 있습니다. 제가 추가한 것은 위 스크린샷에 있는 4가지이며 각각의 내용은 다음과 같습니다. 첫 번째 : ctrl + 방향키로 띄어쓰기 단위 커서 이동 가능 두 번째 : ctrl + 백스페이스 키로 띄어쓰기 단위 문자 삭제 가능 세 번째 : ctrl + shift + 방향키로 띄어쓰기 단위 문자 선택 가능 네 번째 : finder에서 F2키로 이름 변경 가능 rule에 관한 내용은 여기에서 확인하실 수 있습니다.다만, 적용하실 때는 하나씩 직접 해보고 원하는 작동 여부를 확인하시기 바랍니다.제가 했을때는 rule이 추가되어도 동작하지 않는 것들도 많았습니다. 4. 크롬 새로고침크롬에서 새로고침을 할 때 윈도우처럼 F5키로 할 수 있도록 세팅하는 방법입니다. [설정 - 키보드 - 단축키 - 앱 단축키]에서 +키를 눌러 아래와 같이 추가해줍니다. 이런 방식이면 다른 단축키도 세팅할 수 있을 것 같은데 더 찾지는 못했습니다. 아시는 분들은 댓글 부탁드리겠습니다.(탭 이동을 ctrl + page up&#x2F;down으로 변경하고 싶은데 말이죠..) 5. 터미널 세팅저 같은 경우는 터미널에서 사용하는 단축키가 리눅스 단축키에 익숙해져 있습니다.그래서 맥 기본 터미널인 zsh의 단축키를 변경하고 싶었는데 방법을 찾지 못했습니다. 제가 찾은 방법은 iTerm2를 설치해서 키 세팅을 변경하는 방법입니다. 5.1. iTerm2 설치공식 홈페이지에서 다운받아 설치합니다. 5.2. 키 맵핑iTerm2를 실행한 후 ctrl(command) + ,를 이용해 설정을 엽니다.(대부분의 맥 프로그램들은 command + ,가 설정을 여는 단축키라고 합니다.) Keys에 있는 항목들을 통해 원하는 형태로 키 맵핑을 할 수 있습니다.원하시는데로 세팅하시면 되는데 저는 아래와 같이 세팅했습니다. ctrl + 방향키로 단어 단위 커서 이동 ctrl + 백스페이스로 단어 단위 문자 삭제 여러 개의 탭이 있을 경우 ctrl + page up/down으로 이동 home / end 키를 이용해 커서를 문장 처음 &#x2F; 끝으로 이동 ctrl + k로 커서 이후 문자 삭제 ctrl + u로 현재 입력 모두 삭제(이건 리눅스 명령어와 조금 동작이 다릅니다.) 관련해서는 링크를 남겨드릴테니 참고하시기 바랍니다. iTerm2와 zsh로 깔끔한 Mac 터미널 환경 만들기 5 must-have key mappings on iTerm2 to be more productive iTerm key bindings 6. 마치며사실 아직 좀 아쉬운 점이 몇 가지 있습니다.크롬 탭 이동이라던가 finder에서 delete로 삭제하기, 터미널에서 ctrl + l로 커서 올리기 등등. 변경할 수 있는 방법을 찾으면 그 때 다시 포스팅을 올리도록 하겠습니다.","categories":[{"name":"기타","slug":"기타","permalink":"https://postlude.github.io/categories/%EA%B8%B0%ED%83%80/"}],"tags":[{"name":"etc","slug":"etc","permalink":"https://postlude.github.io/tags/etc/"}]},{"title":"JS의 이벤트 루프, Task, Microtask","slug":"js-eventloop-microtask","date":"2021-11-16T08:06:37.000Z","updated":"2022-02-06T08:39:50.127Z","comments":true,"path":"2021/11/16/js-eventloop-microtask/","link":"","permalink":"https://postlude.github.io/2021/11/16/js-eventloop-microtask/","excerpt":"","text":"지난 포스트에서 JS의 실행 컨텍스트(Execution Context)에 대해 정리했습니다. 이번에는 이벤트 루프와 Task, Micro Task에 대해 정리해보고자 합니다. 1. 이벤트 루프(Event Loop)이벤트 루프에 관해서 이 영상보다 더 좋은 설명은 보지 못한 것 같습니다.한글 자막도 잘되어 있으니 꼭 시청해보시는 걸 권장합니다. 영상에 나온 내용과 기타 구글링을 통해 찾은 내용들을 정리하면 다음과 같습니다. JS 엔진은 이벤트 루프가 없다. 이벤트 루프와 WebAPI(Node.js의 경우 C++ API) 등은 브라우저(혹은 Node.js)에서 제공한다. 이벤트 루프는 call stack이 비어있으면 task queue에서 task를 하나 꺼내서 stack에 쌓아 실행될 수 있도록 한다. JS가 single thread라는 말은 곧 call stack이 하나다라는 말과 같다. 2. Task Queue vs. Microtask QueueTask는 아래와 같은 작업을 말합니다. 흔히 우리가 알고 있는 JS 작업들입니다. 스크립트 실행 이벤트 HTML 파싱 콜백 Fetch, Ajax DOM 조작 위 작업들은 task queue에 쌓여 있다가 이벤트 루프에 의해 stack이 비어 있을 때 stack으로 올라가게 됩니다. 반면에 Microtask는 task queue가 아닌 microtask queue(job queue)라는 별개의 queue에 쌓였다가 이벤트 루프에 의해 call stack으로 올라가게 됩니다. 이 microtask에 해당하는 것 중 하나가 바로 Promise입니다.(그 외에도 process.nextTick, Object.observe, MutationObserver가 있다고 합니다.) 그리고 microtask가 task보다 높은 우선 순위를 가지고 있습니다. 3. 예시아래의 코드를 예시로 들어보겠습니다. 123456789101112131415161718192021222324252627console.log('start');setTimeout(() =&gt; &#123; console.log('settimeout 1');&#125;, 0);Promise.resolve() .then(() =&gt; &#123; console.log('promise 1'); &#125;) .then(() =&gt; &#123; console.log('promise 2'); &#125;);setTimeout(() =&gt; &#123; console.log('settimeout 2');&#125;, 0);console.log('end');// 실행결과// start// end// promise 1// promise 2// settimeout 1// settimeout 2 setTimeout과 Promise의 콜백으로 전달된 함수들은 각각 task queue와 micro task queue에 쌓이게 됩니다.start, end가 출력된 후 이벤트 루프는 비어있는 call stack에 올릴 task를 결정하게 되는데 micro task의 우선 순위가 더 높기 때문에 promise 1, 2가 먼저 출력되게 됩니다. ※ 참고 링크 [Javascript] 자바스크립트의 호출 스택과 이벤트 루프 비동기와 Promise #3 [JavaScript] Task Queue말고 다른 큐가 더 있다고? (MicroTask Queue, Animation Frames)","categories":[{"name":"JavaScript","slug":"javascript","permalink":"https://postlude.github.io/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://postlude.github.io/tags/javascript/"}]},{"title":"JavaScript의 실행 컨텍스트(Execution Context)","slug":"javascript-execution-context","date":"2021-11-07T07:54:31.000Z","updated":"2022-02-06T08:39:50.125Z","comments":true,"path":"2021/11/07/javascript-execution-context/","link":"","permalink":"https://postlude.github.io/2021/11/07/javascript-execution-context/","excerpt":"","text":"0. 배경이전 이야기를 조금 해보겠습니다. 회사에 들어오고 얼마 지나지 않았을 때 js에 대해 공부를 해야겠다고 생각했었습니다.업무를 하면서 뭔가 언어의 근본적인 부분에 대해 모르고 쓰고 있다는 생각이 들었기 때문입니다. 그래서 js관련 오프라인 강의를 들었습니다. 그 때 실행 컨텍스트라는 단어도 처음 들었습니다. 강의를 들을 때도 100% 이해가 된 건 아니었습니다. 강의가 끝난 후에 공부를 해야지라는 생각을 하고 시도를 했었는데 잘 이해가 안되더군요(…) 그래서 그 당시에 정리를 위한 draft를 만들었었는데 그걸 이때까지 publish를 하지 못하고 있다가 최근에 다시 공부하면서 조금씩 이해가 되어서 이렇게 정리를 해보려고 합니다. 완벽하게 이해하지 못한 부분도 있습니다. 언제나 그렇듯 잘못된 정보가 있으면 알려주시면 감사하겠습니다. 1. 실행 컨텍스트(Execution Context)란?ECMAScript 2015 문서에 따르면 Excution Context(이하 EC)의 정의는 아래와 같습니다. An execution context is a specification device that is used to track the runtime evaluation of code by an ECMAScript implementation. At any point in time, there is at most one execution context that is actually executing code. This is known as the running execution context. A stack is used to track execution contexts. The running execution context is always the top element of this stack. A new execution context is created whenever control is transferred from the executable code associated with the currently running execution context to executable code that is not associated with that execution context. The newly created execution context is pushed onto the stack and becomes the running execution context. ECMAScript® 2015 Language SpecificationExecution Contexts 첫 문장부터 의문이 생겼습니다. runtime evaluation은 뭘까요? 구글링을 통해 찾은 의미는 다음과 같았습니다. Demonstrate a language’s ability for programs to execute code written in the language provided at runtime. rosettacode.orgRuntime evaluation 즉, 실행 평가(runtime evaluation)란 어떤 코드가 런타임에 어떤 기능을 하는지 판단하는 것입니다. 다시 EC로 돌아오면, EC의 역할은 이 실행 평가를 추적하는 것입니다.마지막 두 문장을 보면 컨트롤이 실행 가능한 코드(executable code)로 옮겨가면 새로운 EC가 생성되고 stack에 push된다고 나와있습니다. 그리고 실행 중인 EC는 새로 생성된 EC가 된다고 합니다. 문서에 나와있는 소스 코드의 종류는 다음과 같습니다. Global code is source text that is treated as an ECMAScript Script. The global code of a particular Script does not include any source text that is parsed as part of a FunctionDeclaration, FunctionExpression, GeneratorDeclaration, GeneratorExpression, MethodDefinition, ArrowFunction, ClassDeclaration, or ClassExpression. Eval code is the source text supplied to the built-in eval function. More precisely, if the parameter to the built-in eval function is a String, it is treated as an ECMAScript Script. The eval code for a particular invocation of eval is the global code portion of that Script. Function code is source text that is parsed to supply the value of the [[ECMAScriptCode]] and [[FormalParameters]] internal slots (see 9.2) of an ECMAScript function object. The function code of a particular ECMAScript function does not include any source text that is parsed as the function code of a nested FunctionDeclaration, FunctionExpression, GeneratorDeclaration, GeneratorExpression, MethodDefinition, ArrowFunction, ClassDeclaration, or ClassExpression. Module code is source text that is code that is provided as a ModuleBody. It is the code that is directly evaluated when a module is initialized. The module code of a particular module does not include any source text that is parsed as part of a nested FunctionDeclaration, FunctionExpression, GeneratorDeclaration, GeneratorExpression, MethodDefinition, ArrowFunction, ClassDeclaration, or ClassExpression. ECMAScript® 2015 Language SpecificationTypes of Source Code 일단 Global code와 Function code에 집중해서 이해해보도록 하겠습니다. 2. Execution StackEC가 쌓이는 stack을 Execution Stack이라고 부릅니다. 사실 처음에 단어를 들었을 때는 이해가 가지 않았는데 좀 더 익숙한 단어로 바꾸면 call stack입니다.(Is Call stack the same as Execution context stack in JavaScript?) 아래와 같은 코드가 있다고 가정해보겠습니다. 123456789101112131415let gb = 'global variable';function func1() &#123; console.log('start func1'); func2(); console.log('end func1');&#125;function func2() &#123; console.log('func2');&#125;func1();console.log('global execution context'); 맨 처음 글로벌 ec가 만들어지고 func1이 호출될 때 해당 함수의 ec가 만들어 집니다. func1 내부에서 func2를 호출할 때도 마찬가지로 func2의 ec가 만들어 집니다. ec가 만들어지는 순서를 보면 아래와 같습니다. 3. EC의 구성 요소EC는 아래와 같은 구조로 되어 있습니다. 1234ExecutionContext: &#123; LexicalEnvironment, VariableEnvironment&#125; 3.1 Lexical EnvironmentLE의 정의는 다음과 같습니다. A Lexical Environment is a specification type used to define the association of Identifiers to specific variables and functions based upon the lexical nesting structure of ECMAScript code. ECMAScript® 2015 Language SpecificationLexical Environments 다시 말해 변수, 함수에 대한 식별자 바인딩 정보를 담고 있는 곳입니다. LE는 다시 EnvironmentRecord와 OuterLexicalEnvironment로 구성되어 있습니다. 1234LexicalEnvironment: &#123; EnvironmentRecord, OuterLexicalEnvironment&#125; EnvironmentRecord는 위에서 말한 LE의 식별자 바인딩 정보를 가지고 있습니다. OuterLexicalEnvironment는 이름에서 상위 LE에 대한 정보를 가지고 있습니다. 함수 내에 존재하지 않지만 자신이 속한 상위 함수나 글로벌 영역에 있는 변수나 함수를 참조할 수 있는 것은 바로 OuterLexicalEnvironment가 있기 때문에 가능한 것입니다. 3.2 Variable EnvironmentES6 에서 LexicalEnvironment와 VariableEnvironment의 차이점은 전자가 함수 선언과 변수(let, const)의 바인딩을 저장하고 후자는 변수(var) 만 저장하는 것이라고 합니다. 전체 구조로 보면 EC는 아래와 같은 모습입니다.(사실 이것 외에도 다른 부분들이 더 있으나 제가 이해한 영역만 작성했습니다.) 12345678910ExecutionContext: &#123; LexicalEnvironment: &#123; EnvironmentRecord, OuterLexicalEnvironment &#125;, VariableEnvironment: &#123; EnvironmentRecord, OuterLexicalEnvironment &#125;&#125; 4. 예시아래의 코드를 예시로 이해해보도록 하겠습니다. 12345678910let a = 20;const b = 30;var c;function multiply(e, f) &#123; var g = 20; return e * f * g;&#125;c = multiply(20, 30); 4.1 Global EC 생성과 Hoisting가장 먼저 global EC가 아래와 같이 만들어집니다. 12345678910111213141516GlobalExectionContext = &#123; LexicalEnvironment: &#123; EnvironmentRecord: &#123; a: &lt; uninitialized &gt;, b: &lt; uninitialized &gt;, multiply: &lt; func &gt; &#125; OuterLexicalEnvironment: null &#125;, VariableEnvironment: &#123; EnvironmentRecord: &#123; c: undefined &#125; OuterLexicalEnvironment: null &#125;&#125; 여기서 중요한 점은 a, b, c 변수의 차이점입니다.let, const로 선언된 a, b 변수는 초기화 되지 않은 상태이며 var로 선언된 c는 undefined가 할당되어 있습니다. 이런 차이로 인해 var로 선언된 변수는 코드 상에서 선언된 위치보다 위에서 접근할 수 있고, let과 const로 선언된 변수는 선언 위치보다 먼저 접근할 경우 reference error가 발생하게 됩니다.Hoisting이라고 부르는 현상이 바로 이것입니다. 4.2 코드 실행이제 코드가 첫 줄부터 실행되고 a, b 변수에 값이 할당됩니다. 12345678910111213141516GlobalExecutionContext = &#123; LexicalEnvironment: &#123; EnvironmentRecord: &#123; a: 20, b: 30, multiply: &lt; func &gt; &#125; OuterLexicalEnvironment: null &#125;, VariableEnvironment: &#123; EnvironmentRecord: &#123; c: undefined &#125; OuterLexicalEnvironment: null &#125;&#125; 그리고 multiply(20, 30) 코드를 만난 순간 multiply 함수의 EC가 만들어지게 됩니다. 123456789101112131415FunctionExecutionContext = &#123; LexicalEnvironment: &#123; EnvironmentRecord: &#123; e: 20, f: 30 &#125; OuterLexicalEnvironment: GlobalLexicalEnvironment &#125;, VariableEnvironment: &#123; EnvironmentRecord: &#123; g: undefined &#125; OuterLexicalEnvironment: GlobalLexicalEnvironment &#125;&#125; multiply 함수가 실행되면 g 변수에 값을 할당하고 계산 값을 리턴하게 됩니다. 123456789101112131415FunctionExecutionContext = &#123; LexicalEnvironment: &#123; EnvironmentRecord: &#123; e: 20, f: 30 &#125; OuterLexicalEnvironment: GlobalLexicalEnvironment &#125;, VariableEnvironment: &#123; EnvironmentRecord: &#123; g: 20 &#125; OuterLexicalEnvironment: GlobalLexicalEnvironment &#125;&#125; 그리고 계산된 값은 c에 할당되게 되고 모든 코드 실행이 완료되면 프로그램이 종료됩니다. 5. 참고 사항제가 참고한 글을 포함해 많은 글들이 EC가 생성되는 과정을 Creation Phase와 Execution Phase라는 단어를 이용해 설명하고 있습니다.그런데 spec에서 아무리 검색을 해도 해당 단어들이 나오지 않았습니다. 이상하게 여기던 와중 stackoverflow의 글을 찾을 수 있었습니다. There are no “phases” in the spec and they’re not called like that, but the concept still exists. stackoverflowCreation and execution phase in ECMAScript Specification [closed] 한 마디로, phase라는 단어를 이용하진 않았지만 개념적으로 존재한다는 의미였습니다. MDN의 Hoisting 페이지에서도 creation and execution phases라는 단어를 쓴 것을 보면 이 단어들을 써서 설명을 해도 오류가 없을 듯 합니다. ※ 참고 링크 ECMAScript® 2015 Language Specification Understanding Execution Context and Execution Stack in Javascript [JS] 자바스크립트의 The Execution Context (실행 컨텍스트) 와 Hoisting (호이스팅) Execution Context [JS] Execution Context(실행 컨텍스트)","categories":[{"name":"JavaScript","slug":"javascript","permalink":"https://postlude.github.io/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://postlude.github.io/tags/javascript/"}]},{"title":"Kubernetes에 MySQL Pod 띄우기","slug":"k8s-mysql","date":"2021-08-10T07:56:03.000Z","updated":"2022-02-06T08:39:50.131Z","comments":true,"path":"2021/08/10/k8s-mysql/","link":"","permalink":"https://postlude.github.io/2021/08/10/k8s-mysql/","excerpt":"","text":"이번 포스트에서는 kubernetes 위에 mysql pod를 띄우는 일을 정리해보도록 하겠습니다. 1. 기본 Deployment 작성처음에는 가장 기본적인 deployment를 작성해보도록 하겠습니다. 아래와 같이 작성하면 됩니다. deployment.yaml123456789101112131415161718192021222324apiVersion: apps/v1kind: Deploymentmetadata: name: mysql labels: app: mysqlspec: replicas: 1 selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:8.0.26 env: - name: MYSQL_ROOT_PASSWORD value: root ports: - containerPort: 3306 apply deployment.yaml1kubectl apply -f deployment.yaml 여기서 중요한 것은 MYSQL_ROOT_PASSWORD입니다.mysql을 실행시킬 때 MYSQL_ROOT_PASSWORD을 세팅하지 않으면 pod가 정상적으로 뜨지 않게 됩니다. 따라서 환경변수를 세팅하도록 합니다. 이렇게 하면 dockerhub로 부터 mysql 이미지를 pull 받아 pod를 띄우게 됩니다. 2. Secret 사용루트 패스워드를 세팅한 것은 좋은데 이렇게 되면 deployment.yaml에 패스워드를 그대로 노출하게 됩니다.이것보다는 kubernetes의 secret을 이용하는 것이 보안적으로 더 좋은 방법이므로 사용해보도록 하겠습니다. 기본적으로 secret은 아래와 같은 형태로 만들면 됩니다. secret.yaml1234567apiVersion: v1kind: Secretmetadata: name: mysql-roottype: Opaquedata: password: [ROOT PASSWORD] data 하위에 key-value 형태로 원하는 값을 넣으면 됩니다. 저는 password라는 이름으로 사용할 비밀번호를 작성했습니다.여기서 중요한 점은 secret에 넣는 value 값들은 base64로 인코딩한 값을 넣어야 한다는 것입니다.예를 들어 패스워드를 root로 사용한다고 하면 password: root로 작성하는 것이 아니라 password: [root를 base64로 인코딩한 문자열]로 작성하셔야 합니다. base64로 인코딩한 문자열은 리눅스의 echo 를 통해서 아래와 같이 알아낼 수 있습니다. encoding base641echo -n root | base64 여기서 중요한 점은 -n 옵션입니다.echo 명령어는 자동으로 trailing newline을 삽입한다고 합니다. 따라서 -n 옵션으로 trailing newline을 없앤 후에 인코딩을 해야 정상적으로 인코딩된 문자열을 얻을 수 있습니다.실제로 -n 있는 상태에서 인코딩된 문자열은 cm9vdA== 이고, -n 옵션이 없는 상태에서 인코딩된 문자열은 cm9vdAo= 으로 서로 다른 것을 알 수 있습니다. 아래와 같은 secret을 만들고 deployment.yaml도 수정하도록 하겠습니다. secret.yaml1234567apiVersion: v1kind: Secretmetadata: name: mysql-roottype: Opaquedata: password: cm9vdA== deployment.yaml12345678...env:- name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-root key: password... 만약 -n 옵션이 없는 상태에서 인코딩된 문자열을 사용하면 어떻게 될까요?secret과 deployment 모두 정상적으로 생성이 되지만 mysql pod가 뜨면서 mysql: unknown option ‘–”‘ 과 같은 에러가 납니다. 그리고 pod가 계속해서 생성되었다가 종료되었다가를 반복하게 됩니다.실제로 제가 이런 실수를 해서 원인을 한참을 찾았었습니다.다행히 이 글을 보고 원인을 찾아 해결할 수 있었는데요, 저와 같은 실수를 하지 않으시길 바라겠습니다. 3. Volume 사용하기지금까지 작업을 통해 띄운 mysql pod는 volume을 사용하지 않고 있기 때문에 만약 pod 종료되었다가 다시 시작되면 그 동안 DB에 저장된 모든 내용은 날아가게 됩니다.따라서 PV, PVC를 이용해 volume을 연결해주도록 하겠습니다. 저는 kubernetes를 kops를 이용해 EC2 서버 위에 구동하고 있기 때문에 EBS 볼륨을 생성하고 그 볼륨을 연결하려고 했지만… 볼륨을 사용하는 것 자체가 비용이 추가가 되기 때문에 node의 특정 디렉토리를 사용하는 volume(local-storage)을 만들어 사용하기로 했습니다. 먼저 node로 사용 중인 ec2에 접속해 /volume/pv1 디렉토리를 만들었습니다. 또한 pv가 특정 node에 연결된 것을 세팅하기 위해 node에 label을 추가했습니다.저 같은 경우 name=node1로 label을 추가했습니다. add label1kubectl label nodes [NODE_NAME] name=node1 그리고 아래와 같이 pv, pvc를 만들어주었습니다. pv.yaml12345678910111213141516171819202122apiVersion: v1kind: PersistentVolumemetadata: name: pv1spec: capacity: storage: 10Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local-storage local: path: /volume/pv1 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: name operator: In values: - node1 pv.yaml1234567891011apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvc1spec: storageClassName: local-storage accessModes: - ReadWriteOnce resources: requests: storage: 10Gi 그리고 deployment.yaml을 아래와 같이 수정하도록 하겠습니다. deployment.yaml123456789101112131415161718192021...spec: containers: - name: mysql image: mysql:8.0.26 volumeMounts: - name: volume1 mountPath: /var/lib/mysql env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-root key: password ports: - containerPort: 3306 volumes: - name: volume1 persistentVolumeClaim: claimName: pvc1... pod가 재시작된 후 node의 /volume/pv1 디렉토리에 데이터들이 쌓이면 정상적으로 연결된 것입니다. ※ 참고 사항application 서버에서 mysql pod에 접속하기 위해서는 mysql pod와 동일한 네임스페이스에 pod를 띄운 후 mysql 서비스 이름과 port로 접속이 가능합니다. ※ 참고 링크 kubernetes docs Kubernetes에서 Local Persistent Volume을 사용하여 local disk 사용","categories":[{"name":"K8S","slug":"k8s","permalink":"https://postlude.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://postlude.github.io/tags/k8s/"},{"name":"db","slug":"db","permalink":"https://postlude.github.io/tags/db/"}]},{"title":"GitHub Actions와 GitHub Container Registry를 이용한 도커 빌드","slug":"github-actions-container-registry","date":"2021-07-20T13:05:56.000Z","updated":"2022-02-06T08:39:50.118Z","comments":true,"path":"2021/07/20/github-actions-container-registry/","link":"","permalink":"https://postlude.github.io/2021/07/20/github-actions-container-registry/","excerpt":"","text":"이번 포스트에서는 GitHub Actions와 GitHub Container Registry를 이용해서 CI를 구축해보도록 하겠습니다. 1. 선택의 이유회사에서는 CI를 젠킨스를 이용해 구축해서 사용중입니다. 그래서 개인적으로 사용할 때도 젠킨스를 이용한 방법을 사용하려고 했습니다.그러던 중 몇 가지 이유로 GitHub Actions와 GitHub Container Registry를 선택하게 되었습니다. 젠킨스 세팅 문제젠킨스에서 docker build가 가능하도록 세팅하는 것은 다른 포스트에서도 해봤기 때문에 어려운 문제는 아니었습니다.다만, docker run 을 실행할 때 호스트의 docker를 사용 가능하도록 옵션을 주는 부분이 뭔가 깔끔하지 않다고 느껴졌습니다.그리고 이걸 쿠버네티스 위에 올려서 사용해야하는데 그 부분도 뭔가 찝찝하기도 했고요.(물론 제가 모르는 젠킨스 컨테이너에서 도커 빌드가 가능하도록 하는 방법이 있을수도 있습니다.) docker hub의 repository 제한 및 정책 변경사실 개인적으로만 쓰는 용도기 때문에 docker hub의 repository가 많이 필요한 것은 아닙니다. private repository도 무료로 쓰는 유저는 1개까지만 만들 수 있지만 이것도 그렇게 불편한 것은 아닙니다.그렇다고 하더라도 약간은 신경이 쓰이긴 했습니다.여기에 더해 docker hub가 최근 무료 계정의 이미지에 대해 몇 가지 정책을 변경했다고 합니다.(관련 내용) AWS ECRECR 또한 고려했었습니다. 많진 않겠지만 추가 비용이 발생하는 것이 신경이 쓰였습니다. 여기에 더해 다음 이유로 github의 기능들을 선택하게 되었습니다. github actions + github container registry = github에서 모두 처리가 가능 쿠버네티스에 추가적으로 젠킨스와 같은 빌드용 pod를 띄울 필요가 없음 제일 큰 이유는 바로 비용에 관한 것입니다. 저는 이미 github을 pro로 사용중이기 때문에 추가 비용이 없습니다. 2. 기본 세팅우선 빌드에 사용할 아주 간단한 node 서버를 만들어보도록 하겠습니다. 패키지는 아래와 같이 3개만 설치하겠습니다. npm install1npm i express morgan pm2 그리고 아래와 같이 app.js와 ecosystem.config.js 파일을 작성합니다. app.js1234567891011121314const express = require('express');const morgan = require('morgan');const app = express();const port = 3000;app.use(express.json());app.use(morgan('dev'));app.listen(port, async () =&gt; &#123; console.log('==================== [NODE SAMPLE] ===================='); console.log(`- PORT : $&#123;port&#125;`); console.log('========================================================');&#125;); ecosystem.config.js1234567891011module.exports = &#123; apps: [&#123; name: 'node-sample', script: './app.js', watch: true, ignore_watch: ['.git', 'log', 'node_modules'], log_date_format: 'YYYY-MM-DD HH:mm:ss', out_file: './log/pm2_out.log', error_file: './log/pm2_err.log' &#125;]&#125;; 3. Personal Access Token 생성github container regitstry에 이미지를 push 하는 등의 작업을 위해서는 PAT가 필요합니다. [github setting - Developer settings - Personal access tokens] 로 들어가 Generate new token을 클릭합니다. 아래와 같이 권한을 선택하고 생성합니다. write:packages read:packages delete:packages 이제 이 토큰을 이용해 github container registry에 push 하거나 pull 할 수 있습니다.예를 들어 굳이 github actions를 이용하지 않더라도 개인적으로 빌드한 도커 이미지를 docker push를 이용해 push 할 수 있다는 이야기입니다.(단, docker 로그인시 토큰을 비밀번호로 사용하여 로그인해야 합니다.) 아래는 예시 코드입니다. sample code123docker login https://ghcr.io -u [계정명]docker tag node-sample:latest ghcr.io/[계정명]/node-sample:latestdocker push ghcr.io/[계정명]/node-sample:latest 4. Dockerfile 작성이제 샘플 코드의 app.js와 동일한 위치에 Dockerfile을 작성하도록 하겠습니다. Dockerfile123456789101112131415FROM node:14ENV SERVER_HOME /usr/src/node-sampleRUN mkdir $&#123;SERVER_HOME&#125;COPY . $&#123;SERVER_HOME&#125;WORKDIR $&#123;SERVER_HOME&#125;RUN npm i \\ &amp;&amp; npx pm2 install pm2-logrotate@latest \\ &amp;&amp; npx pm2 set pm2-logrotate:rotateInterval '0 0 * * *'CMD [\"npx\", \"pm2-runtime\", \"start\", \"ecosystem.config.js\"] 5. GitHub Actions workflow 작성workflow에서 container registry에 push 하기 위해서는 위에서 만든 PAT가 필요합니다.이걸 workflow에 직접 넣는 것이 아니라 repository에서 사용 가능한 secret을 만들어 사용하도록 하겠습니다.[repository의 Settings - Secrets] 로 들어가 New repository secret을 클릭합니다. 원하는데로 Name을 입력하고 Value에는 PAT 값을 입력합니다. 저는 GHCR_PAT 라는 이름으로 생성했습니다. 이제 github actions workflow를 작성하도록 하겠습니다.repository의 Actions 탭으로 들어가 New workflow를 클릭합니다. 제일 하단의 Manual workflow 를 선택합니다. 아래와 같이 입력하도록 하겠습니다. workflow.yml123456789101112131415161718192021222324252627282930# workflow 이름name: Docker Build Teston: push: # master branch에 push했을 때 workflow가 돌게 됨 branches: [master]jobs: # job 이름. job은 여러 개가 설정되어 돌아갈 수 있음. node-sample-build: runs-on: ubuntu-latest steps: - name: Source Code Checkout uses: actions/checkout@master - name: Set up Docker Buildx uses: docker/setup-buildx-action@v1 - name: Login to GitHub Container Registry uses: docker/login-action@v1 with: registry: ghcr.io username: $ password: $ - name: Build and push uses: docker/build-push-action@v2 with: context: . file: ./Dockerfile push: true tags: ghcr.io/$/node-sample:master workflow 작성에 관한 자세한 사항은 github 문서를 통해 확인할 수 있습니다. github에서 workflow를 작성하게 되면 .github/workflows/ 디렉토리 하위에 작성한 yml 파일이 생성됩니다.변경 사항이 생겼으므로 git pull 후 아무 변경 사항을 추가한 후에 push 하면 workflow가 돌게 됩니다. workflow가 도는 것은 repository의 Actions 탭에서 확인할 수 있습니다. 정상적으로 push까지 완료되었으므로 Packages 에서 확인할 수 있습니다. ※ 참고 문서 GitHub 문서 GitHub Container Registry 사용하기 GitHub Actions에서 GitHub Container Registry에 이미지 푸시하기 Github Actions를 이용한 Docker Image Build 및 Push","categories":[{"name":"Git","slug":"git","permalink":"https://postlude.github.io/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://postlude.github.io/tags/git/"},{"name":"docker","slug":"docker","permalink":"https://postlude.github.io/tags/docker/"},{"name":"github","slug":"github","permalink":"https://postlude.github.io/tags/github/"}]},{"title":"kops 쿠버네티스 클러스터에 cert-manager 설치하기","slug":"kops-k8s-cert-manager","date":"2021-07-07T14:18:19.000Z","updated":"2022-02-06T08:39:50.131Z","comments":true,"path":"2021/07/07/kops-k8s-cert-manager/","link":"","permalink":"https://postlude.github.io/2021/07/07/kops-k8s-cert-manager/","excerpt":"","text":"지난 포스트에서 ingress를 세팅하고 접속하는 것까지 진행했습니다. 이번에는 클러스터에 cert-manager addon을 설치하는 작업을 진행하도록 하겠습니다.이 작업을 마치면 ingress를 통해 접속시 https로 접속이 가능해집니다. 1. addon 설치기본적으로 kops 문서 참고하여 진행하겠습니다. 먼저 아래 명령어를 통해 cert-manager 를 활성화합니다. edit cluster1kops edit cluster set cert-manager addon12345...spec: certManager: enabled: true... 이후 update 명령어를 통해 변경될 설정을 적용합니다. update cluster1kops update cluster --yes 적용이 완료되면 kube-system 네임스페이스에 cert-manager 관련 pod가 뜬 것을 확인할 수 있습니다. 2. issuer 생성https 통신을 위해서는 인증서가 필요합니다. 이 인증서를 무료로 발급해주는 Let’s Encrypt를 사용하도록 하겠습니다. cert manager 문서에 나와 있는 방법에 따라 진행하도록 하겠습니다. 아래와 같이 staging-issuer.yaml 파일을 작성합니다. staging-issuer.yaml1234567891011121314apiVersion: cert-manager.io/v1kind: ClusterIssuermetadata: name: letsencrypt-stagingspec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: [이메일 주소] privateKeySecretRef: name: letsencrypt-staging solvers: - http01: ingress: class: nginx 문서와 다른 점은 kind: Issuer가 아닌 kind: ClusterIssuer 라는 것입니다.Issuer는 네임스페이스에 포함된 리소스라 네임스페이스가 다른 ingress에서는 해당 Issuer를 사용하지 못합니다.따라서 ClusterIssuer로 만들어 모든 네임스페이스에서 사용이 가능하도록 하겠습니다. 아래 명령어로 생성합니다. create clusterissuer1kubectl apply -f ./staging-issuer.yaml 생성 후 kubectl describe clusterissuers letsencrypt-staging 명령어를 통해 확인했을때 맨아래에 다음과 같이 Status: True라고 나오면 정상적으로 생성된 것입니다. 3. ingress 생성이전에 test 네임스페이스에 만든 ingress는 삭제하도록 하겠습니다. delete ingress1kubectl delete ingress test-ingress -n test 그리고 아래와 같은 ingress를 생성하도록 하겠습니다. 이 문서를 참고했습니다. cert-test-ingress.yaml1234567891011121314151617181920212223apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: cert-test-ingress annotations: kubernetes.io/ingress.class: \"nginx\" cert-manager.io/cluster-issuer: \"letsencrypt-staging\"spec: tls: - hosts: - [ELB에 연결된 HOST 이름] secretName: letsencrypt-staging rules: - host: [ELB에 연결된 HOST 이름] http: paths: - path: / pathType: Prefix backend: service: name: [test 네임스페이스의 서비스 이름] port: number: 80 저는 cluster issuer로 만들었기 때문에 annotation이 issuer가 아닌 cluster-issuer로 작성했습니다.아래 명령어로 생성하겠습니다. create ingress1kubectl apply -f ./cert-test-ingress.yaml -n test 아래 명령어로 확인했을때 마지막 부분에 Status: True라고 나오면 정상적으로 생성된 것입니다.(몇 초 가량 시간이 걸릴 수 있습니다.) describe ingress1kubectl describe certificate letsencrypt-staging -n test 만약 위 yaml 파일에서 path에 /가 아닌 다른 경로를 주었다면 annotation에 ingress.kubernetes.io/rewrite-target: /을 추가하면 됩니다. cert-test-ingress.yaml123456789101112131415161718192021222324apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: cert-test-ingress annotations: kubernetes.io/ingress.class: \"nginx\" cert-manager.io/cluster-issuer: \"letsencrypt-staging\" ingress.kubernetes.io/rewrite-target: /spec: tls: - hosts: - [ELB에 연결된 HOST 이름] secretName: letsencrypt-staging rules: - host: [ELB에 연결된 HOST 이름] http: paths: - path: /test pathType: Prefix backend: service: name: [test 네임스페이스의 서비스 이름] port: number: 80 ELB에 연결한 도메인에 접속하면 아래와 같이 정상적으로 접속되는 것을 알 수 있습니다. 4. prod issuer 적용위의 과정까지 거치면 정상적으로 접속은 됩니다. 하지만 여전히 인증서가 유효하지 않는 것으로 나옵니다.이유는 Let’s Encrypt 문서에서 확인할 수 있었습니다. Let’s Encrypt는 속도 제한에 영향을 주지 않고 인증서 요청을 테스트할 수 있는 스테이징 API를 제공합니다.스테이징 환경에서 생성된 인증서는 공개적으로 신뢰되지 않습니다. Let's Encryptdef-staging 따라서 운영 환경에서 쓸 수 있는 issuer를 생성하면 됩니다.과정의 위의 staging issuer를 만들고 적용하는 과정과 동일합니다. 아래와 같이 prod-issuer.yaml 파일을 생성합니다. prod-issuer.yaml1234567891011121314apiVersion: cert-manager.io/v1kind: ClusterIssuermetadata: name: letsencrypt-prodspec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: [이메일] privateKeySecretRef: name: letsencrypt-prod solvers: - http01: ingress: class: nginx create prod issuer1kubectl apply -f ./prod-issuer.yaml 아래 명령어를 통해 정상적으로 생성되었는지 체크합니다. describe prod issuer1kubectl describe clusterissuer letsencrypt-prod test 네임스페이스의 ingress를 수정합니다. edit ingress1kubectl edit ingress cert-test-ingress -n test cert-test-ingress1234567...metadata: annotations: cert-manager.io/cluster-issuer: letsencrypt-prod...secretName: letsencrypt-prod... 이렇게 하면 동일하게 접속했을때 staging 일때와 다르게 https 인증서 경고가 뜨지 않는 것을 확인할 수 있습니다. 이렇게 모든 설정을 마쳤습니다. 쿠버네티스에 배포되는 앱이 추가되면 ingress rule을 추가해 적용하면 됩니다. ※ 참고 문서 ingress annotaion 관련","categories":[{"name":"K8S","slug":"k8s","permalink":"https://postlude.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://postlude.github.io/tags/k8s/"},{"name":"kops","slug":"kops","permalink":"https://postlude.github.io/tags/kops/"}]},{"title":"kops로 구축한 k8s 클러스터에 ingress 설정하기","slug":"kops-k8s-ingress","date":"2021-07-05T09:17:59.000Z","updated":"2022-02-06T08:39:50.138Z","comments":true,"path":"2021/07/05/kops-k8s-ingress/","link":"","permalink":"https://postlude.github.io/2021/07/05/kops-k8s-ingress/","excerpt":"","text":"지난 포스트에서는 nginx pod를 띄우고 NodePort 타입의 서비스를 통해 접속했습니다. NodePort로 접속하는 방법은 worker node의 특정 포트로 직접 접속하는 것인데이번에는 ingress를 이용해 nginx에 접속하는 것으로 변경해보도록 하겠습니다. ingress는 간단히 설명하자면 kubernetes의 모든 요청을 설정한 룰에 따라 한 곳에서 처리하도록 하는 것입니다.자세한 설명은 쿠버네티스 문서를 참고하시면 됩니다. 1. ingress addon 설치kops github을 보면 ingress-nginx를 설치할 수 있는 addon이 있는 것을 알 수 있습니다. 저는 AWS를 사용중이므로 github에 나와 있는 것과 동일하게 아래 명령어를 사용해 설치해보도록 하겠습니다. install ingress-nginx addon1kubectl apply -f https://raw.githubusercontent.com/kubernetes/kops/master/addons/ingress-nginx/v1.6.0.yaml 그러면 아래와 같이 Warning이 나오면서 뭔가 제대로 되지 않습니다. 이걸 해결하기 위해서는 해당 yaml파일을 조금 수정해야 합니다.일단 아래 명령어를 통해 전부 삭제하도록 하겠습니다. delete ingress-nginx addon1kubectl delete -f https://raw.githubusercontent.com/kubernetes/kops/master/addons/ingress-nginx/v1.6.0.yaml 아래 명령어로 yaml 파일을 다운받습니다. download ingress yaml1curl -Lo ingress-nginx-v1.6.0.yaml https://raw.githubusercontent.com/kubernetes/kops/master/addons/ingress-nginx/v1.6.0.yaml 우선, yaml파일의 v1beta1로 되어 있는 부분을 전부 v1으로 변경합니다. 또한 저는 ingress-nginx replicas를 1로 줄이도록 하겠습니다. modify yaml12345678910kind: DeploymentapiVersion: apps/v1metadata: name: ingress-nginx namespace: kube-ingress labels: k8s-app: nginx-ingress-controller k8s-addon: ingress-nginx.addons.k8s.iospec: replicas: 1 # 3에서 1로 변경 그 후 수정한 yaml 파일로 다시 ingress를 생성합니다. create ingress-nginx addon1kubectl apply -f ./ingress-nginx-v1.6.0.yaml 그럼 아래와 같이 정상적으로 생성된 것을 볼 수 있습니다. 그리고 중요한 것이 이 ingress를 생성한 순간 AWS의 ELB가 생성되게 됩니다. AWS 콘솔의 EC2에서 로드밸런서에서 확인할 수 있으며 ELB는 과금 요소이기 때문에 사용량이 많으면 비용을 소모하게 됩니다.(물론 개인이 사용할 경우 그렇게 큰 비용이 청구되지는 않는 것 같습니다. 저의 경우 ELB 과금을 포함해도 한 달에 100달러 정도 청구되었습니다.) 2. ingress 생성위 과정으로 통해 ingress controller가 생성되었으니 이제 ingress를 생성해보도록 하겠습니다. 이전 포스트에서 만든 test 네임스페이스에 ingress를 생성하고 NodePort가 아닌 ingress를 통해서 접속하도록 세팅하겠습니다. 우선 기존에 NodePort 타입으로 생성되어 있던 서비스를 ClusterIP 타입으로 변경하겠습니다. edit service1kubectl edit svc nginx-deployment -n test edit service1234...spec: ... type: ClusterIP 그리고 kubernetes 문서에서 기본적인 ingress yaml 파일을 가져와 아래와 같이 수정했습니다. test-ingress.yaml1234567891011121314151617apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: test-ingress annotations: ingress.kubernetes.io/rewrite-target: /spec: rules: - http: paths: - path: /test pathType: Prefix backend: service: name: nginx-deployment port: number: 80 쿠버네티스 문서에 있는 yaml에는 annotaions에 nginx.ingress.kubernetes.io/rewrite-target: /으로 설정되어 있습니다.해당 annotation은 ingress의 path로 들어왔을때 /(루트)로 들어온 것으로 처리해주는 것입니다. 그런데 이유는 알 수 없으나 이렇게 설정하면 정상적으로 동작하지 않아서 아래와 같이 변경했습니다.ingress.kubernetes.io/rewrite-target: / 그리고 아래 명령어를 통해 test 네임스페이스에 ingress를 생성합니다. create ingress1kubectl apply -f ./test-ingress.yaml -n test 아래 명령어를 통해 확인하면 get ingress1kubectl get ingress -n test 아래와 같이 접속할 수 있는 url을 확인할 수 있습니다. test-ingress.yaml에서 path를 /test로 설정했으므로 위 URL/test 로 접속하면 정상적으로 접속되는 것을 확인할 수 있습니다. ingress로 접속이 되니 이전에 worker node ec2에 NodePort 접속을 위해 포트 오픈한 인바운드 규칙은 제거합니다. 3. 도메인에 ELB 연결하기모든 설정을 완료하긴 했는데 뭔가 찜찜합니다. 접속하는 URL이 너무 지저분합니다.그래서 이 URL을 처음 k8s를 구축할 때 구매했던 도메인의 서브 도메인에 등록하도록 하겠습니다. [Route53 - 호스팅 영역 - 구입한 도메인]으로 접속합니다.레코드 생성을 클릭하고 아래와 같이 설정합니다. 레코드 이름을 입력 별칭 선택 [Application/Classic Load Balancer에 대한 별칭] 선택 리전 선택 elb 선택 이렇게 설정하면 [설정한 주소/test]로 접속했을 때 동일하게 nginx welcome 페이지를 볼 수 있습니다.","categories":[{"name":"K8S","slug":"k8s","permalink":"https://postlude.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://postlude.github.io/tags/k8s/"},{"name":"kops","slug":"kops","permalink":"https://postlude.github.io/tags/kops/"}]},{"title":"쿠버네티스 클러스터에 nginx pod 띄우기","slug":"kops-k8s-deploy-nginx","date":"2021-07-03T08:53:54.000Z","updated":"2022-02-06T08:39:50.133Z","comments":true,"path":"2021/07/03/kops-k8s-deploy-nginx/","link":"","permalink":"https://postlude.github.io/2021/07/03/kops-k8s-deploy-nginx/","excerpt":"","text":"지난 포스트에서 kops로 kubernetes 클러스터를 구축했었습니다. 이번에는 해당 클러스터에 간단한 nginx 를 띄워보도록 하겠습니다. 1. 클러스터 세팅먼저 클러스터에 몇 가지 세팅을 해주도록 하겠습니다.(이 부분들은 반드시 필요한 것은 아닙니다.) 1.1. 보안 그룹 변경우선, 저와 동일하게 public 네트워크 형태로 클러스터를 구축하셨다면 쿠버네티스의 모든 master, worker node가 아래와 같이 모든 ip에 대해 ssh 접속이 가능하도록 되어있을겁니다. 이 부분을 저희 집과 bastion 서버에서만 접속이 가능하도록 수정하도록 하겠습니다. 1.2. 백업 설정 변경다음은 백업에 관한 설정입니다. 문서에 따르면 hourly backup은 1주일간 유지되며 daily backup은 1년간 유지된다고 합니다. 백업 내용은 클러스터 구축시 만들었던 S3에 저장됩니다. 저는 개인적으로 클러스터를 구축한 것이고 (아마도) 백업 파일이 많아지면 S3 비용 부담이 생길지도 몰라서 이 기간을 줄이도록 하겠습니다. 이 문서를 참고했습니다. bastion 서버에서 아래 명령어를 입력합니다. edit cluster1kops edit cluster 그러면 클러스터 설정을 세팅할 수 있도록 나올텐데요, 아래와 같이 변경하도록 하겠습니다. set backup12345678910111213141516171819202122232425262728293031etcdClusters:- cpuRequest: 200m etcdMembers: - encryptedVolume: true instanceGroup: master-ap-northeast-2a name: a memoryRequest: 100Mi name: main # ===== 추가 ===== manager: env: - name: ETCD_MANAGER_HOURLY_BACKUPS_RETENTION value: 1d - name: ETCD_MANAGER_DAILY_BACKUPS_RETENTION value: 7d # ===== 추가 =====- cpuRequest: 100m etcdMembers: - encryptedVolume: true instanceGroup: master-ap-northeast-2a name: a memoryRequest: 100Mi name: events # ===== 추가 ===== manager: env: - name: ETCD_MANAGER_HOURLY_BACKUPS_RETENTION value: 1d - name: ETCD_MANAGER_DAILY_BACKUPS_RETENTION value: 7d # ===== 추가 ===== edit 명령어는 설정자체를 저장한 것이고 이것을 실제로 적용하려면 아래 명령어를 실행해야 합니다. update cluster1kops update cluster --yes 2. 샘플 nginx pod 생성이제 테스트 네임스페이스를 만들어서 nginx pod 를 띄워보도로 하겠습니다. 먼저 아래 명령어로 테스트 네임스페이스를 만듭니다. create namespace1kubectl create ns test 그리고 nginx pod를 띄우기 위한 deployment yaml 파일을 생성합니다. kubernetes 문서에서 참고했습니다. 다만, 저는 예시로 샘플을 만들기 위함이므로 pod 개수는 1개로 생성하도록 하겠습니다. nginx-deployment.yaml123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 명령어를 통해 deployment를 생성합니다. create deployment1kubectl apply -f ./nginx-deployment.yaml -n test kubectl 명령어를 통해 deployment와 pod가 생성된 것을 확인할 수 있습니다. 12kubectl get deploy -n testkubectl get po -n test 3. 서비스 생성이렇게 생성한 pod에 외부에서 접속하기 위해서는 서비스가 필요합니다.아래 명령어를 통해 서비스를 생성합니다. 외부에서 접속하기 위함이므로 NodePort 타입으로 생성하겠습니다. create service12kubectl expose deployment nginx-deployment -n test --type NodePortkubectl get svc -n test 저는 worker node가 1개 뿐이고 NodePort 타입으로 생성했으니 worker node ec2 ip에 생성된 NodePort로 접속이 가능합니다. 그런데 막상 접속해보니, 접속이 되지 않았습니다. 구글링을 통해 이유를 찾을 수 있었는데요, 원인은 간단합니다.kops로 생성한 kubernetes 클러스터의 ec2 worker node는 NodePort 타입의 서비스에서 사용하는 범위의 포트(30000-32767)를 자동으로 오픈시켜주지 않기 때문입니다. 포트를 열어주기 위해 ec2 콘솔로 접속해 worker node ec2의 보안그룹을 수정하도록 하겠습니다. 이렇게 수정하면 정상적으로 접속되는 것을 확인할 수 있습니다.","categories":[{"name":"K8S","slug":"k8s","permalink":"https://postlude.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://postlude.github.io/tags/k8s/"},{"name":"kops","slug":"kops","permalink":"https://postlude.github.io/tags/kops/"}]},{"title":"kops를 이용해 쿠버네티스 클러스터 구축하기","slug":"k8s-by-kops","date":"2021-06-22T14:22:42.000Z","updated":"2022-02-06T08:39:50.129Z","comments":true,"path":"2021/06/22/k8s-by-kops/","link":"","permalink":"https://postlude.github.io/2021/06/22/k8s-by-kops/","excerpt":"","text":"0. 안내이 글은 부정확한 정보를 담고 있을 수 있습니다.글 내용 중 잘못된 정보나 더 좋은 방법을 알고 계신 분들은 댓글로 알려주시면 감사하겠습니다. 1. 배경kubernetes를 공부하고 싶은 생각은 전부터 있었습니다. 다만, 항상 방법이 문제였습니다.일단 minikube가 있었기 때문에 기본적인 부분에 대한 공부는 가능했습니다.minikube는 기초적인 기능을 공부하는데는 좋으나구축 자체를 연습하거나 worker node를 다룰수는 없기 때문에 해당 부분을 연습하지 못한다는 것은 단점으로 느껴졌습니다. 그 다음으로 선택한 것은 AWS에서 제공하는 EKS였습니다.일단 무작정 EKS를 구축해보고 여러 설정을 변경해가며 연습해보기 위해 문서를 보며 무작정 구축을 시도했습니다.시행착오 끝에 EKS를 구축하는데는 성공했었습니다. 다만 문제가 있었는데요, 바로.. 요금이었습니다.. 어떤 형태든 kubernetes를 구축하면 비용이 나갈 것은 각오하고 있었고, 제가 허용 가능한 범위는 달에 10만원 ~ 최대 15만원 정도 였습니다.EKS를 구축할 때도 대략적인 금액 계산은 하고 비용을 최소화하기 위한 조치(worker node를 1개로 하는 등)를 하고 구축을 하긴 했었는데요,예상치 못한 부분이 위 이미지에서도 볼 수 있는 NAT Gateway 금액이었습니다.EKS를 구축하면 VPC를 설정하게 되고 이 과정에서 자동으로 NAT Gateway 쓰게 되면서 과금이 되는 것 같습니다.(vpc 세팅 중 public과 private를 모두 사용하는 것으로 사용하면 일단 저처럼 NAT Gateway를 사용하게 됩니다. public만 사용하게 설정한다면 어떻게 될지는 모르겠네요.) 일단 요금이 제 예상을 훨씬 웃돌게 되면서 일단 바로 EKS는 삭제했습니다.(…)그리고 다른 방법을 찾기 시작했습니다. 그러던 중 찾은게 kubeadm과 kops입니다.그 중에 저는 kops로 k8s를 구축하기로 했습니다. 이유는 2가지였습니다. 첫 번째로 공식 문서에 ‘AWS가 공식적으로 지원된다’고 나와 있으며,두 번째로 요금을 계산해보니 월에 10만원 초반대로 이용이 가능할 것 같았기 때문입니다. 2. 기본 세팅일단 저는 단순히 kubernetes를 ‘구축하는 것’을 목표로 두었습니다.여러 세팅의 상세한 의미나 방법은 구축 후에 공부하고, 일단 제가 원하는 정도까지만 구축하는 것으로 방향을 잡았습니다. 기본적으로 공식 문서를 보고 진행했습니다. 2.1. IAM 계정 생성제일 처음 작업은 AWS 작업시 필요한 IAM 계정 생성입니다. 문서에 따르면 아래와 같은 권한이 필요하다고 합니다. AmazonEC2FullAccessAmazonRoute53FullAccessAmazonS3FullAccessIAMFullAccessAmazonVPCFullAccess AWS Console에서 IAM으로 들어가 kops라는 이름의 유저 그룹을 만들고 위 권한을 부여한 후 kops라는 이름의 사용자를 만들었습니다. 사용자를 생성하면 kops.csv 라는 파일을 다운받을 수 있는데요, 이 파일을 잘 가지고 계셔야 합니다.(아마 최초 1회 밖에 다운이 되지 않을겁니다.) 또한 EC2에 접근하기 위해 kops라는 이름의 키 페어도 만들었습니다. 2.2. Bastion 서버 생성처음에 저는 로컬 PC(Windows)에 kops와 kubectl 명령어를 설치하고 진행을 했습니다.그런데 윈도우라 그런지 뭔가 진행이 매끄럽게 되지 않는 부분이 있었습니다.(제 설정이 이상했을 수 있습니다.)어쨌든 별 수 없이 명령어를 실행하고 접속하기 위한 Linux EC2 서버를 하나 생성해서 진행하니 매끄럽게 진행이 되었습니다. 사실 엄밀한 의미의 Bastion 서버는 아닙니다. 비용을 위해 네트워크를 public으로 세팅할 것이기 때문입니다. 대신 요금 절감을 위해 프리티어 사용이 가능한 EC2 인스턴스를 생성해서 사용하도록 하겠습니다. 키 페어는 위에서 만든 kops 키 페어를 선택해 생성합니다. Amazon Linux EC2의 기본 유저는 ec2-user입니다. 생성 후 키 파일을 이용해 접속할 수 있습니다. 2.3. aws-cli, kops, kubectl 설치먼저 kubectl 부터 설치하도록 하겠습니다.과정은 문서에 나와 있는 내용과 동일합니다. 아래 명령어를 순차적으로 실행합니다. install kubectl123curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectlkubectl version --client kubectl 사용시 편리함을 위해 자동완성을 위한 작업도 진행하겠습니다.(문서) set kubectl123curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectlkubectl version --client 다음은 aws-cli입니다. 문서를 참고했습니다. install aws-cli123curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"unzip awscliv2.zipsudo ./aws/install aws-cli 설치 후 iam 계정을 연결하는 설정을 해야 합니다.위에서 kops 계정을 만든 후 다운받은 kops.csv의 파일에서 내용을 확인해 아래 명령어를 실행합니다. configure aws-cli12345aws configureAWS Access Key ID [None]: [csv파일에서 확인]AWS Secret Access Key [None]: [csv파일에서 확인]Default region name [None]: ap-northeast-2Default output format [None]: json kops 설치입니다.(문서) install kops123curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4)/kops-linux-amd64chmod +x kopssudo mv kops /usr/local/bin/kops 3. DNS 세팅문서에 따르면 kops를 이용해 쿠버네티스 클러스터를 구축하기 위해서는 도메인이 필요하다고 합니다. route53에서 검색해보니 제가 사용할만한 적당한 도메인이 그렇게 비싸진 않아서(연 $12) 구매를 했습니다. 실제 도메인을 구매하지 않고 진행하는 방법이 있다고 하는데 일단 저는 도메인 가격이 엄청 비싸다는 생각은 들지 않아서 구매 후 진행했습니다. 4. S3 bucket 생성kops로 쿠버네티스를 사용하기 위해서는 S3 버킷이 필요합니다. 문서에는 명령어로 버킷을 만드는 방법이 설명되어 있지만 저는 콘솔에서 만들도록 하겠습니다. 아래와 같이 세팅합니다. 리전 : 서울 퍼블릭 엑세스 차단 버전 관리 : 활성화 서버측 암호화 : 활성화 - Amazon S3 키(SSE-S3) 여기서 언급해야할 부분은 2가지입니다. 버전 관리는 문서에서 강하게 권장한다고 하니 설정했습니다.서버측 암호화는 설정하지 않으면 SSE-S3와 함께 서버 측 AES256 버킷 암호화를 사용한다고 되어 있어서 설정했습니다. 5. Cluster 생성5.1. 환경변수 세팅Bastion 서버에 접속합니다. 명령어 사용시 자주 사용하기 때문에 아래처럼 /home/ec2-user/.bash_profile에 내용을 추가합니다. 환경변수 추가12345PATH=$PATH:$HOME/.local/bin:$HOME/binKOPS_CLUSTER_NAME=[구매한 도메인]KOPS_STATE_STORE=s3://[S3 버킷 이름]export PATH KOPS_CLUSTER_NAME KOPS_STATE_STORE 5.2. availability zone 확인kops 명령어를 통해서 클러스터를 생성하기 위해서는 aws의 availability zone을 알아야 합니다. 서울 리전이라면 아래 명령어로 확인합니다. availability zone 확인1aws ec2 describe-availability-zones --region ap-northeast-2 명령어를 실행해서 나온 결과 중 State가 available인 경우 해당 존이 사용가능한 것 같습니다.(저의 경우 전부 사용 가능한 것으로 나왔습니다.) 5.3. public key 세팅저는 비용을 위해 네트워크를 모두 public 네트워크를 사용할 예정입니다.(private 네트워크를 사용하면 NAT gateway를 사용하게 될 것이고, 제가 처음 EKS를 구축했을 때처럼 비용이 나가게 될겁니다.) 맨 처음 생성한 kops 키 페어를 /home/ec2-user/.ssh 하위에 두고 권한을 600으로 줍니다.그리고 아래 명령어를 실행합니다. create public key1ssh-keygen -f ./kops.pem -y &gt; kops.pub 그리고 생성된 public key도 600 권한으로 변경합니다. 5.4. 클러스터 생성아래 명령어를 실행해 실제 쿠버네티스 클러스터를 생성합니다.단, 디폴트 옵션으로 실행하면 제가 원하는 환경과 다른 경우가 생겨서 아래와 같이 실행했습니다.옵션 종류와 디폴트 값에 대한 내용은 여기에서 확인할 수 있습니다. set cluster12345678910111213kops create cluster \\--name $KOPS_CLUSTER_NAME \\--state $KOPS_STATE_STORE \\--cloud aws \\--zones ap-northeast-2a \\--image ami-086a3601cd7a83590 \\--master-zones ap-northeast-2a \\--master-size t3a.medium \\--master-volume-size 40 \\--node-size t3a.medium \\--node-volume-size 40 \\--container-runtime docker \\--ssh-public-key ./.ssh/kops.pub 옵션 중 이미지 아이디는 (더 좋은 방법이 있을 것 같지만) 단순하게 EC2 생성화면에서 찾아서 사용했습니다. 위 명령어는 단순히 클러스터를 어떤 환경으로 만들지에 대한 세팅을 하는 것입니다.실제로 클러스터를 생성하기 위해서 아래 명령어를 추가적으로 실행합니다. create cluster1kops update cluster --name $KOPS_CLUSTER_NAME --yes --admin AWS console에 접속해 확인해보면 t3a.medium 으로 ec2 인스턴스가 2개 생성된 것을 확인할 수 있습니다. 또한 처음에 만든 key 파일을 이용해 생성된 ec2 인스턴스(master, worker node)에도 접속할 수 있습니다. 다음 포스트에서는 추가적인 세팅 및 테스트를 해보도록 하겠습니다. ※ 비용저와 같은 방식으로 쿠버네티스 클러스터를 생성하셨다면 한 달에 청구되는 비용은 아래와 같습니다.(30일 기준) 도메인 : 연 $12 Bastion 서버(t2.micro) : $10.368 master(t3a.medium) : $33.696 worker node(t3a.medium) : $33.696 +Load Balancer, EBS, S3 등에서 청구되는 비용 총합 : 월 $77.76 + a EBS 볼륨(gp3 * 120GB + gp2 * 10GB) : $12.08 Load Balancer : $16 +S3, Route53, 세금 등 경험상 한 달에 약 $100 정도 청구되었습니다. 제가 간과한 것이 EBS 볼륨으로 청구되는 금액이었습니다.아무런 세팅을 하지 않은 저의 경우 master에 etcd용으로 gp3 20GB씩 2개가 생성되었습니다.거기에 더해 master와 node의 볼륨으로 쓰는 것도 gp3로 생성되었으며 Bastion에서 쓰는 볼륨은 gp2로 생성되었습니다. 여기에 LB 요금과 세금까지 더하면 한 달에 $120 내외로 비용이 청구됩니다.","categories":[{"name":"K8S","slug":"k8s","permalink":"https://postlude.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://postlude.github.io/tags/k8s/"},{"name":"kops","slug":"kops","permalink":"https://postlude.github.io/tags/kops/"}]},{"title":"Mongoose alias 사용해보기","slug":"mongoose-alias","date":"2021-04-26T15:00:01.000Z","updated":"2022-02-06T08:39:50.165Z","comments":true,"path":"2021/04/27/mongoose-alias/","link":"","permalink":"https://postlude.github.io/2021/04/27/mongoose-alias/","excerpt":"","text":"0. 계기저희 회사에서는 MariaDB와 MongoDB를 사용 중입니다. 그 중 MongoDB는 제가 들어오기도 전 아주 초창기부터 사용 중이다보니 Collection의 몇몇 필드의 네이밍이 현재 사용하는 네이밍과 맞지 않는 것들이 있었습니다.이미 데이터가 굉장히 많이 쌓여있는 상태라 명령어를 이용해 필드명을 바꾸는 것은 부담이 되었습니다. 그러던 중 Mongoose의 alias를 알게 되었고 이에 대해 정리해보고자 합니다. 1. alias란?우선 공식 문서에 적힌 내용은 다음과 같습니다. Aliases are a particular type of virtual where the getter and setter seamlessly get and set another property. This is handy for saving network bandwidth, so you can convert a short property name stored in the database into a longer name for code readability. Mongoose Docsaliases 간단하게 말하자면 virtual의 특별한 형태라고 보면 됩니다. virtual은 MongoDB에 실제로 저장되지는 않지만 마치 존재하는 값인 것처럼 사용할 수 있는 필드입니다.(참고) 이 alias를 이용해 필드의 이름을 제가 원하는 다른 이름으로 바꿔서 사용할 수 있습니다. 2. 적용우선 예시를 적용하기 위해 간단한 node 서버를 만들었습니다.MongoDB의 testdb에 test라는 collection을 사용하는 코드입니다. app.js1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950const express = require('express');const morgan = require('morgan');const mongoose = require('mongoose');const Test = require('./model/test')const app = express();const port = 3000;app.use(express.json());app.use(morgan('dev'));app.get('/', async (req, res) =&gt; &#123; try &#123; const result = await Test.findOne( &#123; str: 'a' &#125; ); res.send(result); &#125; catch (err) &#123; console.error(err); res.sendStatus(500); &#125;&#125;);app.post('/', async (req, res) =&gt; &#123; try &#123; const test = new Test(&#123; str: 'a', num: 1 &#125;); await test.save(); res.sendStatus(200); &#125; catch (err) &#123; console.error(err); res.sendStatus(500); &#125;&#125;);app.listen(port, async () =&gt; &#123; console.log('==================== [NODE SAMPLE] ===================='); console.log(`- PORT : $&#123;port&#125;`); await mongoose.connect('mongodb://localhost:27017/testdb', &#123; user: 'postlude', pass: 'postlude', useNewUrlParser: true, // to fix deprecation warning useUnifiedTopology: true // to fix deprecation warning &#125;); console.log('========================================================');&#125;); test.js12345678910111213141516const mongoose = require('mongoose');const &#123; Schema &#125; = mongoose;const modelNm = 'Test';const TestSchema = new Schema(&#123; str: &#123; type: String &#125;, num: &#123; type: Number &#125;&#125;, &#123; collection: 'test'&#125;);module.exports = mongoose.model(modelNm, TestSchema); 이 상태에서 Test 모델의 각 필드에 alias를 적용하면 다음과 같습니다. test.js123456789101112131415161718const mongoose = require('mongoose');const &#123; Schema &#125; = mongoose;const modelNm = 'Test';const TestSchema = new Schema(&#123; str: &#123; type: String, alias: 'txt' &#125;, num: &#123; type: Number, alias: 'int' &#125;&#125;, &#123; collection: 'test'&#125;);module.exports = mongoose.model(modelNm, TestSchema); 적용은 이게 전부입니다. 하지만 이렇게만 하면 alias를 이용해 조회하거나 alias로 적용된 이름으로 API 리턴 값을 전달할수는 없습니다. 아래와 같이 코드 상에서 접근해 사용하는 것만 가능합니다. app.js1234567891011121314 const result = await Test.findOne( &#123; str: 'a' &#125; ); console.log(result.int); // 1// ------------------------------------ const test = new Test(&#123; str: 'a', num: 1 &#125;); console.log(test.txt); // a 3. alias로 조회하기alias로 find를 하기 위해서는 모델에 내장된 translateAliases 함수를 이용하면 됩니다. app.js12345const result = await Test.findOne( Test.translateAliases(&#123; txt: 'a' &#125;)); 위와 같이 find 조건에 해당하는 객체를 translateAliases 함수로 한 번 감싸서 사용하면 동일한 결과 값을 얻을 수 있습니다. 4. alias를 리턴하기alias가 리턴에 포함되지 않는 이유는 Mongoose 문서의 virtual 부분을 보면 알 수 있습니다. If you use toJSON() or toObject() mongoose will not include virtuals by default. This includes the output of calling JSON.stringify() on a Mongoose document, because JSON.stringify() calls toJSON(). Pass { virtuals: true } to either toObject() or toJSON(). Mongoose Docsmongoosejs.com/docs/guide.html#virtuals 문서에 따라 모델에 옵션을 추가합니다. test.js123456789101112131415const TestSchema = new Schema(&#123; str: &#123; type: String, alias: 'txt' &#125;, num: &#123; type: Number, alias: 'int' &#125;&#125;, &#123; collection: 'test', toJSON: &#123; virtuals: true &#125;&#125;); 이렇게 하면 GET 요청의 응답이 아래와 같은 형태로 받게 됩니다. 123456789&#123; &quot;_id&quot;: &quot;6086d2de1d846b1490f84474&quot;, &quot;str&quot;: &quot;a&quot;, &quot;num&quot;: 1, &quot;__v&quot;: 0, &quot;txt&quot;: &quot;a&quot;, &quot;int&quot;: 1, &quot;id&quot;: &quot;6086d2de1d846b1490f84474&quot;&#125; 여기서 의문이 생겼었는데 왜 하필 response 응답을 보낼 때 일까? 였습니다. find를 통해 조회한 값을 console.log() 로 출력해보면 alias 값은 포함되어 있지 않습니다.그런데 그 객체를 res.send() 를 통해 응답을 보내면 위와 같이 alias 값이 포함되어 있습니다. app.js1234567891011121314app.get('/', async (req, res) =&gt; &#123; try &#123; const result = await Test.findOne( Test.translateAliases(&#123; txt: 'a' &#125;) ); console.log(result); // &#123; _id: 6086d2de1d846b1490f84474, str: 'a', num: 1, __v: 0 &#125; res.send(result); &#125; catch (err) &#123; console.error(err); res.sendStatus(500); &#125;&#125;); 이것에 대한 답은 express의 reponse 문서에서 찾을 수 있었습니다. When the parameter is an Array or Object, Express responds with the JSON representation Express Docsexpressjs.com/ko/api.html#res.send Sends a JSON response. This method sends a response (with the correct content-type) that is the parameter converted to a JSON string using JSON.stringify(). Express Docsexpressjs.com/ko/api.html#res.json res.send() 를 통해 응답을 보낼 때 보내는 데이터의 타입이 객체(혹은 배열)인 경우 내부적으로 res.json() 을 호출하게 됩니다.res.json() 에서는 JSON.stringify() 를 이용해 응답을 보냅니다. 그렇기 때문에 위의 Mongoose 세팅대로 alias 값이 포함되게 되는 것입니다. 5. 원본 값 제거하기일단 alias 값을 리턴 받는 것까지는 성공했습니다.그런데 원본 값까지 리턴 받는 것이 아무리 봐도 찝찝합니다.이제부터 이걸 제거해보도록 하겠습니다. 이 방법이 세련된 방법인지는 모르겠습니다만 어쨌든 제가 찾은 방법은 transform을 이용하는 방법입니다. We may need to perform a transformation of the resulting object based on some criteria, say to remove some sensitive information or return a custom object. In this case we set the optional transform function. Transform functions receive three arguments function (doc, ret, options) {} doc The mongoose document which is being converted ret The plain object representation which has been converted options The options in use (either schema options or the options passed inline) Mongoose Docsmongoosejs.com/docs/api.html#document_Document-toJSON 위의 내용에 따라 아래와 같이 작성합니다. test.js12345678910111213141516171819const TestSchema = new Schema(&#123; str: &#123; type: String, alias: 'txt' &#125;, num: &#123; type: Number, alias: 'int' &#125;&#125;, &#123; collection: 'test', toJSON: &#123; virtuals: true, transform(doc, ret, options) &#123; delete ret.str; delete ret.num; &#125; &#125;&#125;); 이 상태에서 API 요청을 보내면 아래와 같이 원본 이름을 제외한 alias로 결과 값을 받게 됩니다. 1234567&#123; \"_id\": \"6086d2de1d846b1490f84474\", \"__v\": 0, \"txt\": \"a\", \"int\": 1, \"id\": \"6086d2de1d846b1490f84474\"&#125; 6. 한계지금까지의 방법을 적용하면 alias로 데이터를 조회할수도 있고 API 리턴 값도 받을 수 있습니다.그런데 한 가지 해결하지 못한 부분이 있습니다. 바로 특정 필드만 조회할 때 alias를 적용하는 것입니다. app.js123456const result = await Test.findOne( Test.translateAliases(&#123; txt: 'a' &#125;), 'txt' // alias); 위와 같이 조회하면 alias로 정상적으로 조회는 됐지만 없는 필드를 읽으려고 하기 때문에 결과 값은 아래와 같이 아무 필드도 받지 못하게 됩니다. 1234&#123; \"_id\": \"6086d2de1d846b1490f84474\", \"id\": \"6086d2de1d846b1490f84474\"&#125; 따라서 특정 필드만 조회하기 위해선 어쩔 수 없이 원본 이름을 사용해야 합니다. app.js123456const result = await Test.findOne( Test.translateAliases(&#123; txt: 'a' &#125;), 'str' // 원본 필드 이름); 12345&#123; \"_id\": \"6086d2de1d846b1490f84474\", \"txt\": \"a\", \"id\": \"6086d2de1d846b1490f84474\"&#125; 계속 찾아봤지만 이 부분까지 alias를 이용하는 방법은 찾지 못했습니다. 해결 방법을 아시는 분은 댓글로 남겨주시면 감사하겠습니다.","categories":[{"name":"DB","slug":"db","permalink":"https://postlude.github.io/categories/db/"}],"tags":[{"name":"mongodb","slug":"mongodb","permalink":"https://postlude.github.io/tags/mongodb/"}]},{"title":"pm2 watch가 동작하지 않을 때","slug":"pm2-watch","date":"2021-01-08T09:59:09.000Z","updated":"2022-02-06T08:39:50.173Z","comments":true,"path":"2021/01/08/pm2-watch/","link":"","permalink":"https://postlude.github.io/2021/01/08/pm2-watch/","excerpt":"","text":"제가 개인 프로젝트를 위해 신규 Node.js 프로젝트를 만들고 pm2를 이용해 환경을 세팅했는데pm2 watch가 제대로 동작하지 않은 적이 있습니다. 이에 대한 포스팅을 해보고자 합니다. 1. 세팅일단 기본적으로 아주 간단한 node 서버를 만들었습니다. package.json12345678910111213&#123; \"name\": \"test1\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"app.js\", \"scripts\": &#123; &#125;, \"author\": \"\", \"license\": \"ISC\", \"dependencies\": &#123; \"express\": \"^4.17.1\" &#125;&#125; app.js12345678const express = require('express');const app = express();const port = 3000;app.listen(port, async () =&gt; &#123; console.log('test1 start');&#125;); pm2는 글로벌로 설치했으며, 버전은 4.5.0입니다.ecosystem 파일을 생성합니다. 1pm2 ecosystem ecosystem 파일에는 watch 설정만 했습니다. ecosystem.config.js1234567module.exports = &#123; apps: [&#123; name: 'test1', script: 'app.js', watch: true &#125;],&#125;; 2. 테스트위 상태에서 ecosystem 파일을 이용해 서버를 구동합니다. 1pm2 start ecosystem.config.js 로그를 보면 정상적으로 구동된 것을 확인할 수 있습니다. 1pm2 log 이 상태에서 코드를 수정하면 자동적으로 pm2가 재시작하면서 수정된 코드가 반영됩니다. app.js1234app.listen(port, async () =&gt; &#123; console.log('test1 start'); console.log('modify code');&#125;); 12345678PM2 | Change detected on path app.js for app test1 - restartingPM2 | Stopping app:test1 id:0PM2 | App [test1:0] exited with code [1] via signal [SIGINT]PM2 | pid=412 msg=process killedPM2 | App [test1:0] starting in -fork mode-PM2 | App [test1:0] online0|test1 | test1 start0|test1 | modify code 그런데 pm2 stop을 하고 다시 start하면 watch가 동작하지 않아서 코드를 수정해도 restart가 일어나지 않습니다. 12pm2 stop ecosystem.config.js // 정지 후pm2 start ecosystem.config.js // 다시 시작 3. 해결좀 더 세련된 방법이 있을지는 모르겠으나 어쨌든 제가 찾아서 해결한 방법은 다음과 같습니다. pm2 stop후 pm2 delete를 통해 프로세스를 아예 삭제 후 start하면 watch가 정상적으로 동작합니다. 12pm2 stop ecosystem.config.js &amp;&amp; pm2 delete ecosystem.config.js // 정지 및 삭제pm2 start ecosystem.config.js // 다시 시작 ※ 참고 https://pm2.keymetrics.io/docs/usage/application-declaration/ https://github.com/Unitech/pm2/issues/3578","categories":[{"name":"기타","slug":"기타","permalink":"https://postlude.github.io/categories/%EA%B8%B0%ED%83%80/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://postlude.github.io/tags/nodejs/"}]},{"title":"간단하게 쓰는 2020년 회고","slug":"2020-review","date":"2021-01-01T11:54:15.000Z","updated":"2022-02-06T08:39:50.091Z","comments":true,"path":"2021/01/01/2020-review/","link":"","permalink":"https://postlude.github.io/2021/01/01/2020-review/","excerpt":"","text":"스타트업에서 개발자로서 일을 한지 만 2년이 조금 넘었다. 쓸 내용이 많진 않지만 그래도 20년도를 보내면서 회고를 해보려고 한다. 1. 내가 무슨 일을 했는지는 되도록 빨리 정리해두자.이 생각은 예전부터 가지고 있었던 것이었는데 잘 하지 못했던 것이다.지금 직장이 엄청 불만족스럽거나 하진 않지만 그렇다고 평생 직장은 아닐 것이다.나중을 위해서라도 규모가 큰 단위의 일은 포트폴리오로써 정리해두는게 좋을 것 같다.아마 이번 연휴 동안 제일 먼저 이 작업부터 할 것 같다. 2. 토이 프로젝트를 하는 것은 좋다. 하지만 처음부터 완벽하게 하려고 하지 말자.토이 프로젝트 겸 내가 필요로 하는 기능을 직접 만들어서 써볼 겸 해서 간단한 사이트를 만들어서 사용중이다. 처음 생각했던 기능은 대략 다음과 같았다. 구글 docs에 여러 문서로 정리했던 각종 명령어 통합 정리 기능 개발에 유용한 링크 저장 기능 특정 상황에서 자주 쓰이는 명령어(실행문) 자체를 저장하고 검색하는 기능 기능이 뭐 대단한게 아니어서 며칠만 작업을 하면 금방 만들 수 있을 줄 알았다. 당연하게도, 전혀 그렇지 않았다. 명령어 저장 기능부터 작업을 했는데 DB 테이블 설계, api 작업, 화면 작업 이렇게 크게는 3가지의 일이었지만, 생각보다 시간이 걸렸다.무엇보다도, 작업을 하다가 테이블 스키마부터 수정한 게 두어번 있었다.마음은 급한데 작업 속도는 안나오니 답답했다.결국 작업의 방향을 바꿔서 현재 내가 가장 필요하면서 간단한 기능부터 만들기 시작했다.그게 개발 관련 링크 저장, 검색 기능이었고 빠른 시간에 서버를 띄워서 사용할 수 있게 되었다. 이렇게 작업을 해서 얻은 이점이 몇 가지가 있었는데, 첫 번째는 일단 의욕이 생긴다.간단하게나마 결과물로서 서버가 띄워져 있고, 그걸 내가 직접 사용하다보니 추가 수정 작업 같은 것도 더 의욕이 붙었다. 두 번째는 어떤 부분을 추가할 지, 수정할 지에 대해 피부로 느끼게 되었다.이건 작업물을 내가 직접 사용하다보니 느끼는 것이긴 한데, 직접 만든 기능을 쓰다 보니 수정할 부분이 눈에 바로 보인다.UI를 좀 수정해야겠다거나 API 서버를 이런 걸 수정해야겠다거나 하는 점들이 바로 바로 느껴졌다. 세 번째는 익숙한 언어(환경)으로 작업하는 게 좋다는 것이다.물론 새로운 언어나 환경을 익히기 위한 토이 프로젝트를 하는 경우도 있을 것이다.나 같은 경우는 처음엔 JAVA로 작업을 하려고 했다. 이유는 예전에 공부했던 걸 잊지 않기 위해서.그런데 이렇게 하다보니 작업 진행속도가 너무 더뎠다.내가 많이 까먹은 것도 있고, 새로 공부해야할 부분도 있는데 거기다 내가 원하는 기능까지 만들어야 하다보니 그랬다.결국 지금 회사에서 쓰는 것과 동일한게 NodeJS로 언어를 변경했고 그 결과 훨씬 빠른 속도로 결과물을 만들 수 있었다. 3. 회사는 돈을 벌어야 하는 조직이다.굉장히 당연한 말인데 이 말의 의미를 좀 더 체감하게 된 것 같다. 다시 말하자면, 내가 개발적으로 시도해보고 싶은 것이 있어도 회사에게 충분한 시간을 얻기는 쉽지 않다는 말이다. 그것이 설령 궁극적으로 회사에 도움이 될지라도 당장의 매출 증대를 위한 작업이 우선되는 경우가 훨씬 많았다.어디까지나 개발자로서는 아쉬운 부분이 있을 수 있겠지만, 굉장히 당연한 것이다.물론 정말로 필요한 작업이라서 해당 작업을 위한 시간을 받는 경우도 더러 있었다. 이 경우에는 타당한 이유와 커뮤니케이션이 필요했다. 4. 2021년20년 초에는 개인적인 측면에서는 목표로 잡은 게 약간 옅은 느낌이었다.대략적으로 토이 프로젝트해서 기능 만들어서 쓰고, 블로그 글쓰고 해야지.. 정도. 21년도는 좀 더 구체적으로 공부하고자 하는 분야가 있다, 쿠버네티스.21년 말이 되었을 때 쿠버네티스에 대한 지식이 많이 늘어 있도록 노력해야겠다.","categories":[{"name":"기타","slug":"기타","permalink":"https://postlude.github.io/categories/%EA%B8%B0%ED%83%80/"}],"tags":[{"name":"job","slug":"job","permalink":"https://postlude.github.io/tags/job/"}]},{"title":"docker build시 특정 레이어 이후로만 캐시 비활성화 하기","slug":"docker-build-disable-cache","date":"2020-12-31T08:47:33.000Z","updated":"2022-02-06T08:39:50.091Z","comments":true,"path":"2020/12/31/docker-build-disable-cache/","link":"","permalink":"https://postlude.github.io/2020/12/31/docker-build-disable-cache/","excerpt":"","text":"Dockerfile을 작성해 빌드를 할 때 캐시와 관련된 내용을 정리해보도록 하겠습니다. 1. 일반적인 상황예시로 아래와 같은 Dockerfile을 만들었습니다. Dockerfile12345678910FROM nginx:latestCOPY ./test1.txt /RUN apt-get update \\ &amp;&amp; apt-get install -y vimCOPY ./test2.txt /CMD [\"nginx\", \"-g\", \"daemon off;\"] 이 Dockerfile을 아래 명령어로 첫 빌드를 하게 되면 여러 메시지와 함께 빌드가 됩니다. 1docker build -t ngins:test . 123456789101112131415161718Step 1/5 : FROM nginx:latest---&gt; ae2feff98a0cStep 2/5 : COPY ./test1.txt /---&gt; 4a3488e1f867Step 3/5 : RUN apt-get update &amp;&amp; apt-get install -y vim---&gt; Running in 4c3e3e6b8ce2Get:1 http://security.debian.org/debian-security buster/updates InRelease [65.4 kB]...Step 4/5 : COPY ./test2.txt /---&gt; 3fc5a63670e8Step 5/5 : CMD [\"nginx\", \"-g\", \"daemon off;\"]---&gt; Running in 6d4383aa7c6fRemoving intermediate container 6d4383aa7c6f---&gt; 4f7c8d7872fcSuccessfully built 4f7c8d7872fcSuccessfully tagged nginx:test 만약 Dockerfile이 변경되지 않은 상태로 다시 빌드하게 되면 아래와 같이 캐시를 사용하는 것으로 빌드 메시지가 나오게 됩니다. 1234567891011121314151617Sending build context to Docker daemon 9.216kBStep 1/5 : FROM nginx:latest---&gt; ae2feff98a0cStep 2/5 : COPY ./test1.txt /---&gt; Using cache---&gt; 4a3488e1f867Step 3/5 : RUN apt-get update &amp;&amp; apt-get install -y vim---&gt; Using cache---&gt; ea3bc604e29bStep 4/5 : COPY ./test2.txt /---&gt; Using cache---&gt; 3fc5a63670e8Step 5/5 : CMD [\"nginx\", \"-g\", \"daemon off;\"]---&gt; Using cache---&gt; 4f7c8d7872fcSuccessfully built 4f7c8d7872fcSuccessfully tagged nginx:test 2. 캐시 비활성화위 상황에서 캐시를 사용하지 않으려면 어떻게 해야할까요?docker build 명령어에 --no-cache 옵션을 주면 됩니다. 1docker build --no-cache -t ngins:test . 이렇게 하면 처음 빌드를 할 때와 동일하게 모든 레이어를 다시 빌드하게 됩니다. 3. 특정 레이어 이후로만 캐시 비활성화이런 상황을 가정해보겠습니다. 예를 들어 어떤 레이어는 빌드가 오래 걸리지만 변경될 일이 없어서 캐시를 사용해야하고 싶을 수도 있습니다.(실제로 제가 회사 작업 중에 맞닥뜨린 상황이었습니다.) --no-cache 옵션은 모든 레이어의 캐시를 사용하지 않는 옵션이기 때문에 이 상황에서는 사용할수가 없습니다. 명령어 혹은 옵션으로 해결 방법이 존재하는 것은 아니지만, 우회하여 해결할 수 있는 방법은 있습니다. 바로 ARG 를 통해 argument를 전달하는 것입니다. Dockerfile123456789101112FROM nginx:latestCOPY ./test1.txt /ARG DISABLE_CACHERUN apt-get update \\ &amp;&amp; apt-get install -y vimCOPY ./test2.txt /CMD [\"nginx\", \"-g\", \"daemon off;\"] 12CUR_TIME=$(date +%s)docker build --build-arg DISABLE_CACHE=$CUR_TIME -t nginx:test . 위와 같이 DISABLE_CACHE argument 값을 준 후 빌드를 하면 ARG로 이전의 레이어는 캐시를 사용하고,이후는 캐시를 사용하지 않게 되어 RUN 으로 실행한 명령어가 실행되게 됩니다. 1234567891011121314151617181920212223Sending build context to Docker daemon 9.216kBStep 1/6 : FROM nginx:latest---&gt; ae2feff98a0cStep 2/6 : COPY ./test1.txt /---&gt; Using cache---&gt; 4a3488e1f867Step 3/6 : ARG DISABLE_CACHE---&gt; Using cache---&gt; b6ab6ed6f1e4Step 4/6 : RUN apt-get update &amp;&amp; apt-get install -y vim---&gt; Running in e62eeb8aa185Get:1 http://security.debian.org/debian-security buster/updates InRelease [65.4 kB]...Step 5/6 : COPY ./test2.txt /---&gt; b5019df04f18Step 6/6 : CMD [\"nginx\", \"-g\", \"daemon off;\"]---&gt; Running in 946f777160a4Removing intermediate container 946f777160a4---&gt; 42787d4961b9Successfully built 42787d4961b9Successfully tagged nginx:test 단, 여기도 몇 가지 제약이 있습니다. 캐시 비활성화를 위해 넘겨주는 ARG 는 위의 시간 값처럼 항상 변하는 값이어야 합니다.고정된 값을 줄 경우 처음에만 캐시를 사용하지 않고, 두 번째 이후부터는 다시 캐시를 사용합니다. 이 방법은 RUN 명령어 이후만 캐시 비활성화가 가능합니다.즉, COPY 에는 적용되지 않기 때문에 ARG 선언부가 RUN 밑으로 오게 되면 ‘COPY ./test2.txt’ 부분은 계속 캐시를 사용하게 됩니다.만약 COPY에도 적용하고 싶다면 ARG - RUN - COPY 순으로 Dockerfile을 작성하면 RUN 이후의 COPY까지도 캐시를 사용하지 않게 됩니다. 4. 참고 Selectively disable caching for specific RUN commands in Dockerfile","categories":[{"name":"Docker","slug":"docker","permalink":"https://postlude.github.io/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://postlude.github.io/tags/docker/"}]},{"title":"Jenkins를 docker 컨테이너로 구축하기(Docker in Docker)","slug":"docker-in-docker","date":"2020-12-26T13:58:48.000Z","updated":"2022-02-06T08:39:50.091Z","comments":true,"path":"2020/12/26/docker-in-docker/","link":"","permalink":"https://postlude.github.io/2020/12/26/docker-in-docker/","excerpt":"","text":"0. 계기올해 초에 회사 jenkins 서버를 docker로 재구축한 적이 있었습니다. 그 때의 경험을 블로그에 반드시 남겨야겠다고 생각했었는데, 이제서야 글을 쓰게 되었네요. 일단 제가 겪은 상황은 다음과 같습니다. docker 로 jenkins 서버를 구축합니다. 단, jenkins 공식 이미지가 아니라 ubuntu를 기본 이미지로 해 직접 설치하는 식으로 Dockerfile을 작성합니다. jenkins에서는 docker build를 실행하도록 설정되어야 합니다. 이 과정에서 제가 겪은 내용들을 정리해보고자 합니다.(잘못된 내용이 있다면 댓글로 지적해주시면 감사하겠습니다.) 1. 과정jenkins 이미지를 만드는 것은 그다지 어렵지 않았습니다. 그런데 docker build를 실행하는 jenkins job을 만들 때 문득 이런 생각이 들었습니다. jenkins에서 docker build를 하려면 docker 명령어를 써야하는데 그러면 docker 안에 docker를 설치해야 하나? 2. Docker in Docker구글링을 하던 도중 아주 좋은 글을 발견했습니다. 간단하게 정리하면 다음과 같습니다. docker 안에 docker daemon을 설치하는 것은 가능하지만, 권장하지 않는다.목적은 컨테이너 내부에서 docker 명령어를 사용하는 것이므로 호스트의 도커 명령어를 사용할 수 있도록 세팅한다. 3. 세팅처음에는 구글링해서 나오는 여러 글들처럼 docker run 명령어 사용시에 아래와 같은 세팅을 추가했습니다. 123docker run \\-v /var/run/docker.sock:/var/run/docker.sock \\... 젠킨스에는 docker 관련 플러그인들이 존재하기 때문에 젠킨스 컨테이너에 위의 세팅 후 플러그인 설치 및 세팅을 거치면 플러그인에서 제공하는 docker 관련 기능들을 사용할 수 있게 됩니다. 다만, 플러그인에서 모든 도커 관련 명령어에 대한 기능을 제공해주지는 않아서 제 경우에는 직접 도커 명령어를 젠킨스 컨테이너 내부에서 사용이 가능해야 했습니다. 그런데 이렇게 설정을 해도 컨테이너 내부에서 docker 명령어 사용시 command not found 라는 메시지만 나왔습니다. 이리 저리 찾아보다가 결국 컨테이너 내부에서도 docker daemon이 아닌 docker cli는 설치해야 한다는 것을 알게 되었습니다.(docker: not found after mounting &#x2F;var&#x2F;run&#x2F;docker.sock) 그에 따라 Dockerfile에 docker를 설치하는 내용을 그대로 작성하되 docker-ce-cli 만 설치하도록 작성했습니다. 1apt-get install -y docker-ce-cli 이렇게 한 결과 컨테이너 내부에서도 도커 명령어를 사용할 수 있게 되었습니다. 정확히는 호스트의 도커 데몬으로 명령을 내리는 것이기 때문에 컨테이너 내부에서 docker images 명령어를 치면 호스트의 이미지 목록이 나오게 됩니다. 만약 위의 docker run -v 옵션 없이 docker-ce-cli만 설치 후 도커 명령어를 사용하면 도커 데몬에 연결할 수 없다는 메시지가 나오게 됩니다.(Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?) 4. 권한 설정위의 세팅까지 적용하면 ‘젠킨스 컨테이너에서 플러그인으로 도커 빌드하기’ + ‘컨테이너 내부에서 도커 명령어 사용하기’ 까지는 가능합니다. 젠킨스 job에서 직접 도커 명령어를 사용하려고 하면 아마 권한 문제로 실행이 되지 않을 겁니다. 그 이유는 컨테이너 내부/외부의 /var/run/docker.sock 을 보면 알 수 있습니다. 호스트의 /var/run/docker.sock 은 다음과 같습니다. 반면에 컨테이너 내부는 다음과 같습니다. 컨테이너 내부에는 docker 라는 그룹이 없습니다. 따라서 호스트의 그룹 아이디(994)가 그대로 나오게 됩니다. 또한, jenkins 유저는 root도 아니고 994 라는 그룹에 속해있지도 않기 때문에 docker 명령어를 사용할 수 없는 것입니다. 컨테이너 내부에 994 아이디로 docker 라는 그룹을 만들고 jenkins 유저를 docker 그룹에도 속하게 함으로써 젠킨스 job에서도 도커 명령어를 사용할 수 있게 되었습니다. 5. 결론당시에 이 문제를 해결할 때는 꽤 시간을 잡아먹었는데 그래도 얻은 것도 많은 좋은 경험이었습니다. 참고한 링크들도 남겨드리니 한 번쯤 보시는 것도 좋을 것 같습니다. DinD(docker in docker)와 DooD(docker out of docker) Using Docker-in-Docker for your CI or testing environment? Think twice. Docker-in-Docker Docker in Docker?","categories":[{"name":"Docker","slug":"docker","permalink":"https://postlude.github.io/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://postlude.github.io/tags/docker/"}]},{"title":"ngrok 사용시 Invalid Host header 에러","slug":"ngrok-invalid-host-header","date":"2020-10-25T13:50:01.000Z","updated":"2022-02-06T08:39:50.165Z","comments":true,"path":"2020/10/25/ngrok-invalid-host-header/","link":"","permalink":"https://postlude.github.io/2020/10/25/ngrok-invalid-host-header/","excerpt":"","text":"최근 개인 프로젝트를 진행하던 중 ngrok을 사용할 일이 있었습니다.(ngrok 포스트) 저는 Vue를 이용해서 프로젝트를 진행중이었고, localhost:8080 으로 띄운 서버를 ngrok을 통해 확인하고 싶었습니다. 이 상황에서 ngrok을 띄웠습니다. 1ngrok.exe http 8080 ngrok에 뜬 주소로 접속해보니 아래와 같은 화면이 나왔습니다. 다행히도 검색을 통해 손쉽게 해결 방법을 찾았습니다. ngrok http 8080 -host-header=”localhost:8080” stakoverflow링크 위와 같은 명령어로 ngrok을 실행후 접속해보았습니다. 정상적으로 접속되는 것을 확인할 수 있습니다.","categories":[{"name":"기타","slug":"기타","permalink":"https://postlude.github.io/categories/%EA%B8%B0%ED%83%80/"}],"tags":[{"name":"tool","slug":"tool","permalink":"https://postlude.github.io/tags/tool/"}]},{"title":"git checkout 에러 - cannot create directory Permission denied","slug":"git-checkout-permission-denied","date":"2020-08-11T13:50:27.000Z","updated":"2022-02-06T08:39:50.092Z","comments":true,"path":"2020/08/11/git-checkout-permission-denied/","link":"","permalink":"https://postlude.github.io/2020/08/11/git-checkout-permission-denied/","excerpt":"","text":"얼마 전 개인적으로 node 프로젝트를 진행하던 도중 겪었던 일입니다. 1. 증상 git status에서는 분명히 클린한 상태인데 checkout을 하면 에러가 납니다. 에러 메시지는 아래와 유사한 형태로 나옵니다. cannot create directory: Permission denied 이후 git status로 상태를 보면 마치 checkout을 시도했던 브랜치의 커밋이checkout으로 변경하려고 했던 브랜치로 강제로 merge되려다 실패한 것처럼 보입니다.(없던 파일이나 디렉토리를 생성하려다 실패한 것처럼 보입니다.) 2. 검색열심히 구글링을 해서 저와 비슷한 현상을 겪은 사람을 찾았습니다. stackoverflow 링크 여기에 달린 답변들이 여러 가지가 있었습니다. git reset –hard 를 사용해라 어딘가에서 파일을 사용 중일 것이다. 그러므로 vscode를 전부 종료하고 시도해봐라 (위와 같은 이유로) 작업 관리자에서 관련있는 프로세스를 강제 종료해라 stackoverflowstackoverflow.com/questions/39650678/git-checkout-error-cannot-create-directory-permission-denied 그런데 전 이 방법을 모두 시도해봤지만 해결되지 않았습니다. 3. 미궁속으로..여러 가지 방법을 시도 중 설마 정말로 권한 문제일까 싶어서 vscode를 관리자 권한으로 실행시키니 해결이 되는 것 같았습니다. 문제는 checkout이 한 번은 되는데 한 번 더 시도하면 똑같은 현상이 발생한다는 것이었지요. 4. 해결그러던 중 다음과 같은 프로그램을 찾았습니다. process explorer 링크에 들어가보시면 아시겠지만, 윈도우에서 각종 프로세스의 상태를 볼 수 있는 프로그램입니다.(MS 홈페이지에 있는 것이니 믿고 사용해도 되지 않을까 싶습니다.) 어쨌든, 해당 프로그램을 설치하고 실행해보니 떡하고 node 프로그램이 떠있는 것을 발견했습니다.. vscode를 모두 종료한 상태였는데도 말이죠. 해당 프로세스를 강제로 종료시킨 후 checkout을 시도해보니 너무나 깔끔하게 잘 실행되었습니다. 5. 결론아주 드물게 node 프로세스가 정상적으로 종료되지 않는 경우가 발생한 것이 아닌가 싶습니다.(사실 node의 문제인지 pm2 문제인지 vscode 문제인지는 잘 모르겠습니다.) 뭐, 덕분에 유용한 프로그램을 알게 됐으니 좋게 생각해야겠네요..","categories":[{"name":"Git","slug":"git","permalink":"https://postlude.github.io/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://postlude.github.io/tags/git/"}]},{"title":"VirtualBox에 CentOS 7 minimal로 설치하기","slug":"virtualbox-centos-minimal","date":"2020-03-09T14:08:45.000Z","updated":"2022-02-06T08:39:50.196Z","comments":true,"path":"2020/03/09/virtualbox-centos-minimal/","link":"","permalink":"https://postlude.github.io/2020/03/09/virtualbox-centos-minimal/","excerpt":"","text":"예전 포스트에서 virtualbox에 CentOS7을 설치하는 것을 포스팅한 적이 있습니다. 솔직히 말하자면 해당 포스트에서 CentOS에 gui를 설치한 것은 다음과 같은 이유였습니다. MobaXterm을 알기 전이라 포트 포워딩으로 사용할만한 적절한 툴을 알지 못했습니다. minimal로 설치했을 때 공유 디렉토리를 사용하기 위한 설정이 자꾸 실패했습니다. 테스트를 하던 도중 드디어 CentOS minimal에 공유 폴더 설정(VBox 게스트 확장 설치)에 성공했습니다. 조금 더 정리된 VBox CentOS 7 설치 포스트라고 봐주시면 될 것 같습니다. 포스트 내용은 제가 오류를 범한 프로세스를 그대로 따라 갑니다.한 번에 하시고 싶으신 분들은 맨 마지막 정리 부분만 읽어주시면 됩니다. 1. CentOS 설치이 부분은 이전과 거의 동일합니다. 이전 포스트를 참고하시면 됩니다.다른 부분은 다음과 같습니다. Extension Pack은 필요하지 않습니다.어차피 포트 포워딩 후 MobaXterm을 사용할 예정이므로 창 크기 조절에 신경쓸 필요가 없습니다. SOFTWARE SELECTION에서 Minimal Install로 선택 후 설치하시면 됩니다. 2. 기본 설정설정은 이전과 크게 다르지 않습니다. 다만, MobaXterm을 사용하기 위해 포트포워딩 설정을 해주도록 합니다. 호스트 포트는 1023 이하가 아니라면 크게 중요하지 않습니다. ssh로 접속해야 하므로 게스트 포트는 22번으로 설정합니다. 여기까지만 설정하면 사용하는데 큰 지장이 없습니다. 딱 한 가지, 공유 폴더를 제외하면 말이죠. 일단, 이전에 했던데로 VBox에서 공유 폴더 설정도 미리 해주도록 하겠습니다. 3. VirtualBox 게스트 확장 설치3.1 마운트GUI 설치 버전에서 했던 것과 동일하게 설치를 시도하겠습니다. 장치 - 게스트 확장 CD 이미지 삽입 이전에는 CD 삽입만 해도 설치 화면이 나왔지만 minimal 설치에서는 아무런 반응이 없습니다. 직접 마운트를 시킨 후에 설치를 해야합니다. 우선 마운트시킬 디렉토리를 /mnt 하위에 cdrom 이라는 이름으로 생성 후 /dev/cdrom을 마운트 했습니다. 12mkdir &#x2F;mnt&#x2F;cdrommount &#x2F;dev&#x2F;cdrom &#x2F;mnt&#x2F;cdrom 그러면 아래와 같이 exe 파일과 sh 파일이 보이게 됩니다. 3.2 패키지 설치여기서 우리가 실행해야할 것은 VBoxLinuxAdditions.run 입니다. 1.&#x2F;VBoxLinuxAdditions.run bzip2 이라는 패키지가 없다고 나오네요. yum으로 설치하도록 하겠습니다. 1yum install -y bzip2 설치 후 다시 VBoxLinuxAdditions.run 을 실행합니다. 이번에는 조금 더 실행이 됐지만 fail이 났습니다. 메시지를 보면 이유를 알 수 있는데, target kernel 버전 3.10.0-1062.el7.x86_64에 맞는 kernel header가 없다고 합니다. 위에서 보이는 것과 같이 kernel-headers라는 패키지가 설치되어 있지 않습니다. 설치하도록 하겠습니다. 1yum install -y kernel-headers 다시 VBoxLinuxAdditions.run 을 실행하도록 하겠습니다. 그런데 또 다시 동일한 에러가 납니다. 확인해보겠습니다. 설치된 kernel(3.10.0-1062.el7) 버전과 kernel-headers(3.10.0-1062.18.1.el7) 버전이 다릅니다. 이 버전을 맞춰줘야 합니다. kernel을 업데이트 하고 완료되면 재부팅하도록 합니다. 12yum update -y kernelreboot 재부팅을 하면 선택화면에서 기존에는 2개였던 것이 3개로 선택지가 늘어난 것을 알 수 있습니다. 자세히 보면 하나는 이전 버전의 커널이고, 다른 하나는 업데이트 후 설치된 버전의 커널입니다. 신규 버전으로 부팅합니다. 아래의 명령어를 통해서도 현재 커널 버전을 알 수 있습니다. 1uname -r 신규 버전으로 잘 부팅되었습니다. 이제 다시 VBoxLinuxAdditions.run 을 실행해야 하는데 재부팅했으므로 다시 마운트를 시켜야 합니다. 1234cd &#x2F;mntmount &#x2F;dev&#x2F;cdrom .&#x2F;cdromcd cdrom.&#x2F;VBoxLinuxAdditions.run 다시 실행했는데도 이게 왠걸, 동일한 에러가 납니다. 구글링을 해봤더니 아래와 같은 내용이 나옵니다. I suspect the version of kernel-devel does not match the version of your running kernel https://forums.virtualbox.org/viewtopic.php?f=3&t=91563 kernel-devel을 설치하도록 하겠습니다. 1yum install -y kernel-devel VBoxLinuxAdditions.run 을 다시 실행합니다. 이번엔 에러 메시지가 조금 달라졌습니다. 몇 가지 필요한 패키지들이 없다고 하네요. 설치하겠습니다. 1yum install -y gcc make perl VBoxLinuxAdditions.run 을 다시 실행하면 드디어! 정상적으로 설치된 것을 볼 수 있습니다. 게스트 확장 설치가 완료되었기 때문에 /media 경로로 가면 이전에 설정한 공유 폴더가 보이게 됩니다. 3.3 이전 버전 커널 삭제이제부터는 새롭게 업데이트한 버전의 커널을 사용할 것이므로 이전 버전은 필요하지 않습니다. 따라서 yum remove 나 package-cleanup 명령어를 통해 이전 버전 커널을 삭제하도록 하겠습니다.(단, package-cleanup 명령어를 사용하기 위해서는 yum-utils 패키지가 설치되어 있어야 합니다.) 1yum remove kernel.x86_64-3.10.0-1062.el7 12yum install -y yum-utilspackage-cleanup --oldkernels --count&#x3D;1 참고 4. 정리정리하자면 아래의 프로세스로 진행됩니다. CentOS 7 minimal 설치 패키지 설치 1yum install -y bzip2 kernel-headers kernel-devel gcc make perl 커널 업데이트 후 재부팅 - 재부팅시 업데이트한 버전의 커널 선택(보통 제일 위에 것일 겁니다.) 1yum update -y kernel 장치 - 게스트 확장 CD 이미지 삽입 마운트 후 VBoxLinuxAdditions.run 실행 12345mkdir &#x2F;mnt&#x2F;cdromcd &#x2F;mntmount &#x2F;dev&#x2F;cdrom .&#x2F;cdromcd cdrom.&#x2F;VBoxLinuxAdditions.run 이전 버전 커널 삭제 12yum install -y yum-utilspackage-cleanup --oldkernels --count&#x3D;1","categories":[{"name":"기타","slug":"기타","permalink":"https://postlude.github.io/categories/%EA%B8%B0%ED%83%80/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://postlude.github.io/tags/linux/"}]},{"title":"window와 BOM, DOM","slug":"javascript-object","date":"2020-02-16T13:43:36.000Z","updated":"2022-02-06T08:39:50.126Z","comments":true,"path":"2020/02/16/javascript-object/","link":"","permalink":"https://postlude.github.io/2020/02/16/javascript-object/","excerpt":"","text":"javascript 관련된 여러가지 객체에 대해 한 번쯤 정리를 하려고 했었다. 처음에는 MDN의 관련 문서를 처음부터 끝까지 읽어보려고 시도하다가 이건 아니 것 같다는 느낌이 강하게 들면서(…) 개별 property나 세세하게 읽어보기 보다는 전체적인 개념을 한 번 정리해보고자 한다. 1. window자바스크립트는 본래 웹 브라우저에서 사용하려고 만든 언어입니다. 이후 진화를 거쳐 다양한 사용처와 플랫폼을 지원하는 언어로 변모하였습니다. 자바스크립트가 돌아가는 플랫폼은 호스트(host) 라고 불립니다. 호스트는 브라우저, 웹서버, 심지어는 커피 머신이 될 수도 있습니다. 각 플랫폼은 해당 플랫폼에 특정되는 기능을 제공하는데, 자바스크립트 명세서에선 이를 호스트 환경(host environment) 이라고 부릅니다. 호스트 환경은 랭귀지 코어(ECMAScript)에 더하여 플랫폼에 특정되는 객체와 함수를 제공합니다. 웹브라우저는 웹페이지를 제어하기 위한 수단을 제공하고, Node.js는 서버 사이드 기능을 제공해주죠. 아래 그림은 호스트 환경이 웹 브라우저일 때 사용할 수 있는 기능을 개괄적으로 보여줍니다. 최상단엔 window라 불리는 ‘루트’ 객체가 있습니다. window 객체는 2가지 역할을 합니다. 전역 객체 챕터에서 설명한 바와 같이, 자바스크립트 코드의 전역 객체입니다. ‘브라우저 창(browser window)’을 대변하고, 이를 제어할 수 있는 메서드를 제공합니다. javascript.info브라우저 환경과 다양한 명세서 각각의 탭은 각각의 Window 객체를 가진다. JavaScript 코드가 실행되는 전역 window는 항상 코드가 실행되는 탭을 가리킨다.몇몇 properties와 함수는 탭을 포함한 전체 창에 대해 적용된다.(ex. resizeTo(), innerHeight) 호스트에 따라 DOM과 BOM은 존재하지 않을수도 있다. 하지만 Javascript Core는 공통적으로 존재. 2. DOM문서 객체 모델(Document Object Model, DOM)은 웹 페이지 내의 모든 콘텐츠를 객체로 나타내줍니다. 이 객체는 수정 가능합니다.document 객체는 페이지의 기본 ‘진입점’ 역할을 합니다. document 객체를 이용해 페이지 내 그 무엇이든 변경할 수 있고, 원하는 것을 만들 수도 있습니다. javascript.info문서 객체 모델(DOM) document를 통해 문서(html)를 조작할 수 있음. 3. BOM브라우저 객체 모델(Browser Object Model, BOM)은 문서(document) 이외의 모든 것을 제어하기 위해 브라우저(호스트 환경)가 제공하는 추가 객체를 나타냅니다.alert/confirm/prompt 역시 BOM의 일부입니다. 문서(document)와 직접 연결되어 있지 않지만, 사용자와 브라우저 사이의 커뮤니케이션을 도와주는 순수 브라우저 메서드이죠. javascript.info브라우저 객체 모델(BOM) 3.1 navigatornavigator 객체는 브라우저와 운영체제에 대한 정보를 제공합니다. 객체엔 다양한 프로퍼티가 있는데, 가장 잘 알려진 프로퍼티는 현재 사용 중인 브라우저 정보를 알려주는 navigator.userAgent와 브라우저가 실행 중인 운영체제(Windows, Linux, Mac 등) 정보를 알려주는 navigator.platform입니다. javascript.info브라우저 객체 모델(BOM) 3.2 locationlocation 객체는 현재 URL을 읽을 수 있게 해주고 새로운 URL로 변경(redirect)할 수 있게 해줍니다. javascript.info브라우저 객체 모델(BOM) 3.3 screen화면에 대한 정보를 알려줍니다. 너비(width), 높이(height), 픽셀(pixelDepth), 컬러(colorDepth), 화면 방향(orientation), 작업표시줄을 제외한 너비와 높이(availWidth, availHeight) 등이 있습니다. 화면 크기에 따라 다른 동작을 하고 싶을 때 사용합니다. www.zerocho.comWindow 객체와 BOM 3.4 historyHistory 인터페이스는 브라우저의 세션 기록, 즉 현재 페이지를 불러온 탭 또는 프레임의 방문 기록을 조작할 수 있는 방법을 제공합니다. MDNHistory history.pushState(객체, 제목, 주소)와 history.replaceState(객체, 제목, 주소)는 HTML5에서 추가되었는데요. 페이지를 이동하지 않고 단순히 주소만 바꿔줍니다. 대신 객체 부분에 페이지에 대한 정보를 추가할 수 있습니다. www.zerocho.comWindow 객체와 BOM","categories":[{"name":"JavaScript","slug":"javascript","permalink":"https://postlude.github.io/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://postlude.github.io/tags/javascript/"}]},{"title":"도커 버전 업그레이드 후 Trouble Shooting","slug":"docker-trouble-shooting","date":"2020-02-12T14:44:57.000Z","updated":"2022-02-06T08:39:50.092Z","comments":true,"path":"2020/02/12/docker-trouble-shooting/","link":"","permalink":"https://postlude.github.io/2020/02/12/docker-trouble-shooting/","excerpt":"","text":"최근 회사에서 사용하던 도커 버전을 업그레이드했습니다. 오늘은 그 과정에서 겪었던 Trouble Shooting을 적어보려고 합니다. 1. 컨테이너 구동 시 아래와 같은 에러가 발생할 경우docker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused “process_linux.go:430: container init caused &quot;write /proc/self/attr/keycreate: permission denied&quot;“: unknown. 구글링을 통해 알아본 결과 container-selinux package의 버전이 무언가 맞지 않아 생긴 문제로 보였습니다. 따라서 해당 패키지를 update(혹은 downgrade)를 해서 해결했습니다. 1234yum list container-selinux yum update container-selinuxyum downgrade container-selinux 검색한 다른 해결 방법으론 selinux를 끄는 방법도 있었지만, 왠지 내키지 않아 시도해보지는 않았습니다.참고1 : 특정 버전으로 설치참고2 : containerd를 1.2.5로 downgrade (해당 방법으로는 직접해보지는 않아서 확실하진 않습니다.) 2. docker images 에는 이미지 목록이 나오는데 docker rmi 명령어로 이미지를 지울 경우 ‘No such image’ 메시지가 나올 경우버전 업을 하면서 어딘가 꼬여서 이렇게 나오는 것으로 보입니다. 도커를 내리고 /var/lib/docker 디렉토리를 삭제 후 다시 실행하면 됩니다.(/var/lib/docker 디렉토리는 도커 실행시 다시 생성됩니다.) 123456systemctl stop docker # &#x2F;var&#x2F;lib&#x2F;docker 디렉토리는 docker 엔진 재시작시 새로 생성됨rm -rf &#x2F;var&#x2F;lib&#x2F;dockersystemctl start docker 참고 : stackoverflow","categories":[{"name":"Docker","slug":"docker","permalink":"https://postlude.github.io/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://postlude.github.io/tags/docker/"}]},{"title":"Windows에 OpenJDK 설치하기","slug":"openjdk","date":"2019-08-26T14:03:22.000Z","updated":"2022-02-06T08:39:50.168Z","comments":true,"path":"2019/08/26/openjdk/","link":"","permalink":"https://postlude.github.io/2019/08/26/openjdk/","excerpt":"","text":"Oracle JDK의 유료화 이슈(정확히는 기업에서 쓰는 용도가 유료화) 이후로 많은 기업들이 OpenJDK로 변경한 것으로 알고 있습니다.(사실 회사에서 JAVA를 쓰지 않아서 정확하지는 않습니다만, 예전에 면접볼 때 물어본 기업은 변경해서 큰 이슈없이 잘 쓰고 있다고 했었습니다.) 사실 개인적으로 공부할 때는 상관없지만, 그래도 환경을 맞춰보고자 로컬 환경에 OpenJDK를 설치해보고자 합니다. 1. OpenJDK 다운로드Oracle JDK처럼 공식 사이트에서 받기 위해 구글링을 하던 도중 2개의 사이트를 찾았습니다. 첫 번째는 github 저장소입니다. 두 번째는 이 사이트입니다.왼쪽의 Archive에서 다운받으실 수 있습니다. 양쪽 다 테스트하기 위해 github 저장소에서는 1.8 버전 msi 파일을 받았고,jdk 사이트에서는 10.0.2 버전의 tar.gz 파일을 받았습니다. 2.1 설치msi 파일은 간단합니다. 그대로 실행시켜서 설치하시면 됩니다. tar 파일의 경우에는 원하는 경로에 압축을 풀어 설치합니다. 2.2 환경변수 설정아래와 같은 경로로 접근해 JAVA_HOME 경로를 설정해줍니다. 내 컴퓨터 우클릭 - 속성 - 고급 시스템 설정 - 환경 변수 - 새로 만들기 JAVA_HOME 생성 후 Path를 수정해줍니다. Path 선택 후 편집 - 새로 만들기 3. 테스트세팅한 JAVA_HOME을 변경해 가면서 cmd 창에서 아래 명령어를 실행했습니다. 12java -versionjavac -version 8버전과 10버전 모두 정상적으로 세팅되는 것을 확인했습니다.","categories":[{"name":"JAVA","slug":"java","permalink":"https://postlude.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://postlude.github.io/tags/java/"}]},{"title":"ngrok 사용방법","slug":"ngrok","date":"2019-08-20T14:54:37.000Z","updated":"2022-02-06T08:39:50.166Z","comments":true,"path":"2019/08/20/ngrok/","link":"","permalink":"https://postlude.github.io/2019/08/20/ngrok/","excerpt":"","text":"이번 포스팅은 개발시 유용한 툴에 대해 소개하고자 합니다. 로컬 환경에서 개발을 할 때 이런 경험이 있으실 겁니다. 개발한 화면을 실제 모바일 기기에서 확인해보고 싶을 때 로컬 환경에 접속할 수 없는 외부에서 개발 내용을 확인하고 싶을 때 이를 위해서는 실제로 서버를 띄워서 확인해야하는데, 시간도 걸리고 여러모로 번거로운 면이 있습니다.이런 경우에 사용하는 툴이 바로 ngrok 입니다. 1. 다운로드여기서 다운 받으실 수 있습니다. 저는 윈도우에서 사용을 할 것이므로 윈도우용 ngrok을 다운 받았습니다.다운을 받고 압축을 풀면 정말 심플하게 ngrok.exe 파일 하나가 있습니다. 2. 테스트용 코드 작성테스트를 위해서 아주 간단한 node 코드를 작성했습니다. ngrok-test1234567891011121314const express = require('express');const app = express();const morgan = require('morgan');const port = 3000;app.use(morgan('dev'));app.get('/', (req, res) =&gt; &#123; res.send('test app');&#125;);app.listen(port, () =&gt; &#123; console.log(`========== [Node Test Server Start(port: $&#123;port&#125;)] ==========`);&#125;); 이 코드를 실행시킨 후 로컬에서 접속하면 다음과 같은 화면을 보게 됩니다. 3. ngrok 테스트이제 ngrok을 사용해보겠습니다.윈도우 cmd창을 열어서 ngrok.exe가 설치된 경로로 이동합니다.그리고 아래와 같은 명령어를 실행합니다.(마지막 숫자는 포트 번호입니다. 저는 샘플 코드를 3000 포트로 동작시켰기 때문에 3000으로 적었습니다.) 1ngrok.exe http 3000 위와 같은 화면이 나오면서 ngrok이 동작합니다. 위에 표시된 주소로 접근하게 되면 위와 같이 동일한 화면을 볼 수 있게 됩니다.이 주소는 localhost로 접속이 되지 않는 곳에서도 접속이 가능합니다.(단, ngrok이 떠있는 동안에만 접속할 수 있습니다.) 또한, ngrok을 정지시킨 후 다시 구동시키면 주소는 변경됩니다.","categories":[{"name":"기타","slug":"기타","permalink":"https://postlude.github.io/categories/%EA%B8%B0%ED%83%80/"}],"tags":[{"name":"tool","slug":"tool","permalink":"https://postlude.github.io/tags/tool/"}]},{"title":"Javascript의 Array 함수들","slug":"javascript-array","date":"2019-07-16T14:29:08.000Z","updated":"2022-02-06T08:39:50.125Z","comments":true,"path":"2019/07/16/javascript-array/","link":"","permalink":"https://postlude.github.io/2019/07/16/javascript-array/","excerpt":"","text":"일을 하던 도중 JavaScript의 배열 관련 함수를 자주 봤었다.정확한 기능을 모르는 것도 있어서 구글링을 하곤 했는데 이게 반복되니 한 번 정리를 해야겠다 싶었다. 1. Array.length기본이라고 말하기도 민망한 property지만 mdn을 보면서 새로운 사실을 알게 되어 정리하려고 한다. JavaScript 배열의 속성을 설정할 때 그 속성이 유효한 배열 인덱스이고 그 인덱스가 현재 배열의 경계(bounds)를 넘어간다면, 엔진은 배열의 length 속성을 그에 맞춰 업데이트 합니다: 하지만, length 속성을 감소시키면 요소(element)를 지웁니다. MDNlength 와 숫자형 속성의 관계 첫 번째 문장은 너무나 당연한 이야기다. 하지만 아래의 문장은 처음 알게 된 내용이었다.즉, 아래와 같은 코드가 가능하다는 이야기 Array.length1234let ary = [1, 2, 3]; // ary.length === 3ary.length = 1;console.log(ary);// [1] 만약, length를 줄인 뒤에 다시 늘린다면 어떻게 될까? Array.length123456let ary = [1, 2, 3]; // ary.length === 3ary.length = 1;console.log(ary);// [1]ary.length = 3;// [1, undefined, undefined] 역시나 undefined로 채워진다. 2. Array.from()Array.from()1234567891011121314console.log(Array.from('foo'));// expected output: Array [\"f\", \"o\", \"o\"]console.log(Array.from([1, 2, 3], x =&gt; x + x));// expected output: Array [2, 4, 6]console.log(Array.from(&#123; a: '1', b: '2' &#125;));// [] : object는 빈 배열로 리턴된다.let obj = &#123; a: '1', b: '2' &#125;;Array.from(Object.keys(obj));// [\"a\", \"b\"]Array.from(Object.values(obj));// [\"1\", \"2\"] 3. Array.of()Array.of()123456Array.of(7); // [7] Array.of(1, 2, 3); // [1, 2, 3]Array.of(undefined); // [undefined]Array(7); // [ , , , , , , ]Array(1, 2, 3); // [1, 2, 3] 4. Array.prototype.concat()기존배열을 변경하지 않습니다.추가된 새로운 배열을 반환합니다.중첩 배열 내부로 재귀하지 않습니다. Array.prototype.concat()12345678910const alpha = ['a', 'b', 'c'];const numeric = [1, 2, 3];alpha.concat(numeric);// 결과: ['a', 'b', 'c', 1, 2, 3]let b = [1, 2, 3];let c = [4, 5, [6, 7]];b.concat(c);// [1, 2, 3, 4, 5, [6, 7]] 5. Array.prototype.copyWithin()배열의 일부를 얕게 복사한 뒤, 동일한 배열의 다른 위치에 덮어쓰고 그 배열을 반환합니다. 이 때, 크기(배열의 길이)를 수정하지 않고 반환합니다. arr.copyWithin(target[, start[, end]]) target 복사한 시퀀스(값)를 넣을 위치를 가리키는 0 기반 인덱스. 음수를 지정하면 인덱스를 배열의 끝에서부터 계산합니다.target이 arr.length보다 크거나 같으면 아무것도 복사하지 않습니다. target이 start 이후라면 복사한 시퀀스를 arr.length에 맞춰 자릅니다. start (Optional) 복사를 시작할 위치를 가리키는 0 기반 인덱스. 음수를 지정하면 인덱스를 배열의 끝에서부터 계산합니다.기본값은 0으로, start를 지정하지 않으면 배열의 처음부터 복사합니다. end (Optional) 복사를 끝낼 위치를 가리키는 0 기반 인덱스. copyWithin은 end 인덱스 이전까지 복사하므로 end 인덱스가 가리키는 요소는 제외합니다. 음수를 지정하면 인덱스를 배열의 끝에서부터 계산합니다.기본값은 arr.length로, end를 지정하지 않으면 배열의 끝까지 복사합니다. Array.prototype.copyWithin()1234567891011[1, 2, 3, 4, 5].copyWithin(-2);// [1, 2, 3, 1, 2][1, 2, 3, 4, 5].copyWithin(0, 3);// [4, 5, 3, 4, 5][1, 2, 3, 4, 5].copyWithin(0, 3, 4);// [4, 2, 3, 4, 5][1, 2, 3, 4, 5].copyWithin(-2, -3, -1);// [1, 2, 3, 3, 4] 6. Array.prototype.entries()배열의 각 인덱스에 대한 키/값 쌍을 가지는 새로운 Array Iterator 객체를 반환합니다. Array.prototype.entries()123456789101112131415161718var array1 = ['a', 'b', 'c'];var iterator1 = array1.entries();console.log(iterator1.next().value);// expected output: Array [0, \"a\"]console.log(iterator1.next().value);// expected output: Array [1, \"b\"]var a = ['a', 'b', 'c'];var iterator = a.entries();for (let e of iterator) &#123; console.log(e);&#125;// [0, 'a']// [1, 'b']// [2, 'c'] 7. Array.prototype.every()배열 안의 모든 요소가 주어진 판별 함수를 통과하는지 테스트합니다.every는 호출한 배열을 변형하지 않습니다. Array.prototype.every()1234567891011function isBelowThreshold(currentValue, index, array) &#123; console.log(index); return currentValue &lt; 40;&#125;var array1 = [1, 30, 45, 29, 10, 13];console.log(array1.every(isBelowThreshold));// 0// 1// false 8. Array.prototype.fill()배열의 시작 인덱스부터 끝 인덱스의 이전까지 정적인 값 하나로 채웁니다. Array.prototype.fill()123456789101112var array1 = [1, 2, 3, 4];// fill with 0 from position 2 until position 4console.log(array1.fill(0, 2, 4));// expected output: [1, 2, 0, 0]// fill with 5 from position 1console.log(array1.fill(5, 1));// expected output: [1, 5, 5, 5]console.log(array1.fill(6));// expected output: [6, 6, 6, 6] 9. Array.prototype.filter()주어진 함수의 테스트를 통과하는 모든 요소를 모아 새로운 배열로 반환합니다.원본 배열은 그대로 유지. Array.prototype.filter()12345678var words = ['spray', 'limit', 'elite', 'exuberant', 'destruction', 'present'];const result = words.filter(word =&gt; word.length &gt; 6);console.log(words);console.log(result);// Array [\"spray\", \"limit\", \"elite\", \"exuberant\", \"destruction\", \"present\"]// Array [\"exuberant\", \"destruction\", \"present\"] 10. Array.prototype.find()주어진 판별 함수를 만족하는 첫 번째 요소의 값을 반환합니다. 그런 요소가 없다면 undefined를 반환합니다. Array.prototype.find()123456789101112131415161718var array1 = [5, 12, 8, 130, 44];var found = array1.find(function(element) &#123; return element &gt; 10;&#125;);console.log(found);// expected output: 12const inventory = [ &#123;name: 'apples', quantity: 2&#125;, &#123;name: 'bananas', quantity: 0&#125;, &#123;name: 'cherries', quantity: 5&#125;];const result = inventory.find(fruit =&gt; fruit.name === 'cherries');console.log(result) // &#123; name: 'cherries', quantity: 5 &#125; 11. Array.prototype.findIndex()주어진 판별 함수를 만족하는 배열의 첫 번째 요소에 대한 인덱스를 반환합니다. 만족하는 요소가 없으면 -1을 반환합니다. Array.prototype.findIndex()123456789var array1 = [5, 12, 8, 130, 44];function isLargeNumber(element) &#123; return element &gt; 13;&#125;console.log(array1.findIndex(isLargeNumber));// expected output: 3 12. Array.prototype.flat()모든 하위 배열 엘리먼트를 지정된 깊이까지 재귀적으로 이어붙여 새로운 배열을 생성합니다. Array.prototype.flat()123456789101112131415var arr1 = [1, 2, [3, 4]];arr1.flat(); // [1, 2, 3, 4]var arr2 = [1, 2, [3, 4, [5, 6]]];arr2.flat();// [1, 2, 3, 4, [5, 6]]var arr3 = [1, 2, [3, 4, [5, 6]]];arr3.flat(2);// [1, 2, 3, 4, 5, 6]var arr4 = [1, 2, , 4, 5];arr4.flat();// [1, 2, 4, 5] 13. Array.prototype.flatMap()매핑함수를 사용해 각 엘리먼트에 대해 map 수행 후, 결과를 새로운 배열로 평평화합니다. Array.prototype.flatMap()12345678910111213141516171819let arr1 = [1, 2, 3, 4];arr1.map(x =&gt; [x * 2]); // [[2], [4], [6], [8]]arr1.flatMap(x =&gt; [x * 2]);// [2, 4, 6, 8]// 한 레벨만 평평화됨arr1.flatMap(x =&gt; [[x * 2]]);// [[2], [4], [6], [8]]let arr2 = [\"it's Sunny in\", \"\", \"California\"];arr2.map(x=&gt;x.split(\" \"));// [[\"it's\",\"Sunny\",\"in\"],[\"\"],[\"California\"]]arr2.flatMap(x =&gt; x.split(\" \"));// [\"it's\",\"Sunny\",\"in\",\"California\"] 14. Array.prototype.forEach()주어진 함수를 배열 요소 각각에 대해 실행합니다.forEach()는 배열을 변형하지 않습니다. 그러나 callback이 변형할 수는 있습니다.예외를 던지지 않고는 forEach()를 중간에 멈출 수 없습니다. 15. Array.prototype.includes()includes() 메서드는 배열이 특정 요소를 포함하고 있는지 판별합니다. arr.includes(valueToFind[, fromIndex]) valueToFind 탐색할 요소. 문자나 문자열을 비교할 때, includes()는 대소문자를 구분합니다. fromIndex (Optional) 이 배열에서 searchElement 검색을 시작할 위치입니다. 음의 값은 array.length + fromIndex의 인덱스를 asc로 검색합니다. 기본값은 0입니다. Array.prototype.includes()123456789101112131415// array length is 3// fromIndex is -1// computed index is 3 + (-1) = 2var arr = ['a', 'b', 'c'];arr.includes('a', -1); // falsearr.includes('a', -2); // falsearr.includes('a', -3); // truevar ary = [ &#123; a: 'a', b: 'b'&#125;, &#123; a: 'aa', b: 'bb'&#125;];ary.includes(&#123; a: 'a', b: 'b'&#125;); // false. 객체는 확인 불가 16. Array.prototype.indexOf()indexOf() 메서드는 배열에서 지정된 요소를 찾을 수있는 첫 번째 인덱스를 반환하고 존재하지 않으면 -1을 반환합니다. arr.indexOf(searchElement[, fromIndex]) searchElement 배열에서 찾을 요소입니다. fromIndex (Optional) 검색을 시작할 색인입니다. 인덱스가 배열의 길이보다 크거나 같은 경우 -1이 반환되므로 배열이 검색되지 않습니다. 제공된 색인 값이 음수이면 배열 끝에서부터의 오프셋 값으로 사용됩니다. 제공된 색인이 음수이면 배열은 여전히 앞에서 뒤로 검색됩니다. 계산 된 인덱스가 0보다 작 으면 전체 배열이 검색됩니다. 기본값 : 0 (전체 배열 검색). Array.prototype.indexOf()123456789101112var array = [2, 9, 9];array.indexOf(2); // 0array.indexOf(7); // -1array.indexOf(9, 2); // 2array.indexOf(2, -1); // -1array.indexOf(2, -3); // 0var ary = [ &#123; a: 'a', b: 'b'&#125;, &#123; a: 'aa', b: 'bb'&#125;];ary.indexOf(&#123; a: 'a', b: 'b'&#125;); // -1. 객체는 확인 불가 17. Array.prototype.join()join() 메서드는 배열의 모든 요소를 연결해 하나의 문자열로 만듭니다. Array.prototype.join()12345678910var elements = ['Fire', 'Air', 'Water'];console.log(elements.join());// expected output: \"Fire,Air,Water\"console.log(elements.join(''));// expected output: \"FireAirWater\"console.log(elements.join('-'));// expected output: \"Fire-Air-Water\" 18. Array.prototype.keys()keys() 메서드는 배열의 각 인덱스를 키 값으로 가지는 새로운 Array Iterator 객체를 반환합니다. Array.prototype.keys()123456var array1 = ['a', 'b', 'c'];var iterator = array1.keys(); for (let key of iterator) &#123;console.log(key); // expected output: 0 1 2&#125; 19. Array.prototype.keys()keys() 메서드는 배열의 각 인덱스를 키 값으로 가지는 새로운 Array Iterator 객체를 반환합니다. Array.prototype.keys()123456789101112var array1 = ['a', 'b', 'c'];var iterator = array1.keys(); for (let key of iterator) &#123; console.log(key); // expected output: 0 1 2&#125;var arr = ['a', , 'c'];var sparseKeys = Object.keys(arr);var denseKeys = [...arr.keys()];console.log(sparseKeys); // ['0', '2']console.log(denseKeys); // [0, 1, 2] 빈 값을 무시하지 않음 20. Array.prototype.lastIndexOf()lastIndexOf() 메서드는 지정된 요소가 배열에서 발견 될 수있는 마지막 인덱스를 반환하고, 존재하지 않으면 -1을 반환합니다. 배열은 fromIndex에서 시작하여 뒤로 검색됩니다. 21. Array.prototype.map()map() 메서드는 배열 내의 모든 요소 각각에 대하여 주어진 함수를 호출한 결과를 모아 새로운 배열을 반환합니다.map이 처리할 요소의 범위는 첫 callback을 호출하기 전에 정해집니다. map이 시작한 이후 배열에 추가되는 요소들은 callback을 호출하지 않습니다. Array.prototype.map()123456789101112131415161718var array1 = [1, 4, 9, 16];// pass a function to mapconst map1 = array1.map(x =&gt; x * 2);console.log(map1); // expected output: Array [2, 8, 18, 32]console.log(array1); // Array [1, 4, 9, 16] 원본 유지// 아래 라인을 보시면...['1', '2', '3'].map(parseInt);// 결과를 [1, 2, 3] 으로 기대할 수 있습니다.// 그러나 실제 결과는 [1, NaN, NaN] 입니다.// parseInt 함수는 보통 하나의 인자만 사용하지만, 두 개를 받을 수 있습니다.// 첫 번째 인자는 변환하고자 하는 표현이고 두 번째는 숫자로 변환할 때 사용할 진법입니다.// Array.prototype.map은 콜백에 세 가지 인자를 전달합니다.// 배열의 값, 값의 인덱스, 그리고 배열// 세 번째 인자는 parseInt가 무시하지만 두 번째 인자는 아닙니다. 22. Array.prototype.pop()pop() 메서드는 배열에서 마지막 요소를 제거하고 그 요소를 반환합니다. 23. Array.prototype.push()push() 메서드는 배열의 끝에 하나 이상의 요소를 추가하고, 배열의 새로운 길이를 반환합니다. 24. Array.prototype.reduce()reduce() 메서드는 배열의 각 요소에 대해 주어진 리듀서(reducer) 함수를 실행하고, 하나의 결과값을 반환합니다. Array.prototype.reduce()123[0, 1, 2, 3, 4].reduce(function(accumulator, currentValue, currentIndex, array) &#123; return accumulator + currentValue;&#125;); callback accumulator currentValue currentIndex array 반환 값 1번째 호출 0 1 1 [0, 1, 2, 3, 4] 1 2번째 호출 1 2 2 [0, 1, 2, 3, 4] 3 3번째 호출 3 3 3 [0, 1, 2, 3, 4] 6 4번째 호출 6 4 4 [0, 1, 2, 3, 4] 10 Array.prototype.reduce()123[0, 1, 2, 3, 4].reduce(function(accumulator, currentValue, currentIndex, array) &#123; return accumulator + currentValue;&#125;, 10); callback accumulator currentValue currentIndex array 반환 값 1번째 호출 10 0 0 [0, 1, 2, 3, 4] 10 2번째 호출 10 1 1 [0, 1, 2, 3, 4] 11 3번째 호출 11 2 2 [0, 1, 2, 3, 4] 13 4번째 호출 13 3 3 [0, 1, 2, 3, 4] 16 5번째 호출 16 4 4 [0, 1, 2, 3, 4] 20 25. Array.prototype.reduceRight()reduceRight() 메서드는 누적기에 대해 함수를 적용하고 배열의 각 값 (오른쪽에서 왼쪽으로)은 값을 단일 값으로 줄여야합니다.reduce와 동일하되 오른쪽에서 왼쪽으로 reducer 함수 호출. 26. Array.prototype.reverse()배열의 순서를 반전합니다. 27. Array.prototype.shift()shift() 메서드는 배열에서 첫 번째 요소를 제거하고, 제거된 요소를 반환합니다. 이 메서드는 배열의 길이를 변하게 합니다. 28. Array.prototype.slice()slice() 메서드는 어떤 배열의 begin부터 end까지(end 미포함)에 대한 얕은 복사본을 새로운 배열 객체로 반환합니다. 원본 배열은 수정되지 않습니다. Array.prototype.slice()123456789101112131415161718var animals = ['ant', 'bison', 'camel', 'duck', 'elephant'];const ary1 = animals.slice(2);const ary2 = animals.slice(2, 4)console.log(animals);console.log(ary1);console.log(ary2);animals[0] = 'aaaaa';console.log(animals);console.log(ary1);// Array [\"ant\", \"bison\", \"camel\", \"duck\", \"elephant\"]// Array [\"camel\", \"duck\", \"elephant\"]// Array [\"camel\", \"duck\"]// Array [\"aaaaa\", \"bison\", \"camel\", \"duck\", \"elephant\"]// Array [\"camel\", \"duck\", \"elephant\"] 29. Array.prototype.some()some() 메서드는 배열 안의 어떤 요소라도 주어진 판별 함수를 통과하는지 테스트합니다.빈 배열에서 호출하면 무조건 false를 반환합니다. Array.prototype.some()12345678910111213var array = [1, 2, 3, 4, 5];var array2 = [1, 2, 3, 3, 5];var even = function(element) &#123; // checks whether an element is even return element % 2 === 0;&#125;;console.log(array.some(even));// expected output: trueconsole.log(array2.some(even));// expected output: true 30. Array.prototype.sort()sort() 메서드는 배열의 요소를 적절한 위치에 정렬한 후 그 배열을 반환합니다. 정렬은 stable sort가 아닐 수 있습니다.기본 정렬 순서는 문자열의 유니코드 코드 포인트를 따릅니다.(오름차순) stable sort : 졍렬시 같은 값에 대하여 원본 순서가 유지되는 정렬unstable sort : 졍렬시 같은 값에 대하여 원본 순서가 유지되지 않는 정렬 31. Array.prototype.splice()splice() 메서드는 배열의 기존 요소를 삭제 또는 교체하거나 새 요소를 추가하여 배열의 내용을 변경합니다.만약 제거할 요소의 수와 추가할 요소의 수가 다른 경우 배열의 길이는 달라집니다. array.splice(start[, deleteCount[, item1[, item2[, …]]]]) start 배열의 변경을 시작할 인덱스입니다. 배열의 길이보다 큰 값이라면 실제 시작 인덱스는 배열의 길이로 설정됩니다.음수인 경우 배열의 끝에서부터 요소를 세어나갑니다(원점 -1, 즉 -n이면 요소 끝의 n번째 요소를 가리키며 array.length - n번째 인덱스와 같음).값의 절대값이 배열의 길이 보다 큰 경우 0으로 설정됩니다. deleteCount (Optional) 배열에서 제거할 요소의 수입니다.deleteCount를 생략하거나 값이 array.length - start보다 크면 start부터의 모든 요소를 제거합니다.deleteCount가 0 이하라면 어떤 요소도 제거하지 않습니다. 이 때는 최소한 하나의 새로운 요소를 지정해야 합니다. item1, item2, … (Optional) 배열에 추가할 요소입니다. 아무 요소도 지정하지 않으면 splice()는 요소를 제거하기만 합니다. Array.prototype.splice()1234567891011121314var months = ['Jan', 'March', 'April', 'June'];months.splice(1, 0, 'Feb');// inserts at index 1console.log(months);// expected output: Array ['Jan', 'Feb', 'March', 'April', 'June']months.splice(4, 1, 'May');// replaces 1 element at index 4console.log(months);// expected output: Array ['Jan', 'Feb', 'March', 'April', 'May']months.splice(1, 2, 'May');console.log(months);// expected output: Array [\"Jan\", \"May\", \"April\", \"May\"] 32. Array.prototype.toLocaleString()toLocaleString() 메서드는 배열의 요소를 나타내는 문자열을 반환합니다.요소는 toLocaleString 메서드를 사용하여 문자열로 변환되고 이 문자열은 locale 고유 문자열(가령 쉼표 “,”)에 의해 분리됩니다. Array.prototype.toLocaleString()123456var array1 = [1, 'a', new Date('21 Dec 1997 14:12:00 UTC')];var localeString = array1.toLocaleString('en', &#123;timeZone: \"UTC\"&#125;);console.log(localeString);// expected output: \"1,a,12/21/1997, 2:12:00 PM\",// This assumes \"en\" locale and UTC timezone - your results may vary 33. Array.prototype.toString()toString() 메서드는 지정된 배열 및 그 요소를 나타내는 문자열을 반환합니다. Array.prototype.splice()1234var array1 = [1, 2, 'a', '1a'];console.log(array1.toString());// expected output: \"1,2,a,1a\" 34. Array.prototype.unshift()unshift() 메서드는 새로운 요소를 배열의 맨 앞쪽에 추가하고, 새로운 길이를 반환합니다. Array.prototype.unshift()12345678910111213141516171819var array1 = [1, 2, 3];console.log(array1.unshift(4, 5));// expected output: 5console.log(array1);// expected output: Array [4, 5, 1, 2, 3]var arr = [1, 2];arr.unshift(0); // result of call is 3, the new array length// arr is [0, 1, 2]arr.unshift(-2, -1); // = 5// arr is [-2, -1, 0, 1, 2]arr.unshift([-3]);// arr is [[-3], -2, -1, 0, 1, 2] 35. Array.prototype.values()values() 메서드는 배열의 각 인덱스에 대한 값을 가지는 새로운 Array Iterator 객체를 반환합니다. Array.prototype.values()123456789101112131415const array1 = ['a', 'b', 'c'];const iterator = array1.values();for (const value of iterator) &#123; console.log(value); // expected output: \"a\" \"b\" \"c\"&#125;var arr = ['w', 'y', 'k', 'o', 'p'];var eArr = arr.values();console.log(eArr.next().value); // wconsole.log(eArr.next().value); // yconsole.log(eArr.next().value); // kconsole.log(eArr.next().value); // oconsole.log(eArr.next().value); // p","categories":[{"name":"JavaScript","slug":"javascript","permalink":"https://postlude.github.io/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://postlude.github.io/tags/javascript/"}]},{"title":"SSH 접속 프로그램 MobaXterm","slug":"mobaxterm","date":"2019-07-02T14:35:42.000Z","updated":"2022-02-06T08:39:50.164Z","comments":true,"path":"2019/07/02/mobaxterm/","link":"","permalink":"https://postlude.github.io/2019/07/02/mobaxterm/","excerpt":"","text":"SSH 접속 툴로 저는 superputty를 사용하고 있었습니다. 이유는 하나 무료이기 때문이었습니다.제가 느낀 superputty의 느낌을 한 줄로 표현하면 이렇습니다. 정말 필요한 최소한의 기능만을 제공하는 툴. 때문에 몇 가지 불편함도 있었습니다. alt + tab 으로 브라우저나 다른 창으로 이동할 때 이상하게 잘 되지 않습니다. 세션을 저장해서 사용할 때 기본적으로 putty 기반으로 해야됩니다. (superputty에서도 가능은 한데 key 파일 설정은 안됐던 것으로 기억합니다.) superputty에서 설정은 한계가 있어서 결국은 putty에서 설정을 해야되는 경우도 생깁니다. 그러던 와중에 우연히 알게 된 툴이 바로 이 MobaXterm이었습니다.(여기에서 다운 받으실 수 있습니다.) MobaXterm 역시 무료로 사용하실 수 있습니다. 또한 기업에서도 홈 에디션을 사용할 수 있습니다.(참고 링크) 물론 홈 에디션의 경우 기능상에 몇 가지 제약이 있긴 하지만 크게 문제가 될 정도는 아닌 것 같습니다. 뿐만 아니라 MobaXterm에는 ssh 접속 기능 뿐만 아니라 여러 가지 다른 기능들이 포함되어 있습니다. 저는 다른 것보다 FTP가 포함되어 있다는 것이 좋았습니다. filezilla 같은 툴을 쓰지 않고 하나의 툴에서 다 해결 할 수 있게 되니까요. 검색을 하면서 약간 무겁다는 단점이 있다고 한 글을 봤는데 크게 불편함을 느낄 정도는 아닌 것 같습니다. 아직 많이 사용해보지는 않았지만 여러모로 superputty보다는 유용하게 쓸 수 있을 것 같습니다.","categories":[{"name":"기타","slug":"기타","permalink":"https://postlude.github.io/categories/%EA%B8%B0%ED%83%80/"}],"tags":[{"name":"tool","slug":"tool","permalink":"https://postlude.github.io/tags/tool/"}]},{"title":"스타트업 6개월 후기","slug":"startup-6month-review","date":"2019-06-15T15:00:00.000Z","updated":"2022-02-06T08:39:50.174Z","comments":true,"path":"2019/06/16/startup-6month-review/","link":"","permalink":"https://postlude.github.io/2019/06/16/startup-6month-review/","excerpt":"","text":"제가 스타트업에서 개발자로 일을 한지 벌써 6개월이라는 시간이 흘렀네요.스스로를 점검할 겸 느낀 점들을 적어볼까 합니다. 1. git… GIT..!!입사 초반에 절실하게 느낀 것은 git에 대한 내용입니다.개발 회사에서 버전 관리 툴을 쓰지 않는 곳은 거의 없을 것이라고 생각하는데요,그 git을 잘 활용할 수 있어야 되겠다는 것을 피부로 많이 느끼게 되었습니다. 개인적으로 github 계정을 가지고 나름 제가 짠 코드를 올리고 있었는데그런 부분들이 많이 도움이 되었습니다. 기본적인 명령어들을 알고 있었기 때문이죠.더불어 제가 개발자로 취업을 준비하시는 분들에게 공부를 권장하고 싶은 것은 stash에 관한 내용입니다. 저만 그럴지는 모르겠으나 저는 git stash를 정~말 많이 사용합니다.그래서 입사 후 얼마 되지 않았을 때 블로그에 정리하는 글을 올리기도 했고요. 또한 저희 회사는 git flow 를 사용하는데 기본적으로 git 명령어를 사용하는 것과 크게 다르지 않기 때문에 적응하는데도 큰 무리가 없었습니다. 2. 평일에도 힘들지만 공부해야 한다.이건 사람마다 공부 패턴이 다르기 때문에 모두 이렇게 해야되는 것은 아닙니다만,하나는 확실합니다. 정말 꾸준히 공부해야합니다. 그 이유를 최근에 명확하게 알게 되었습니다. 바로 언제 바빠질지 모르기 모르기 때문입니다.제 github 활동을 한 번 보겠습니다. …정말 부끄럽기 짝이 없네요.변명을 하자면 5월부터 회사 일이 점점 바빠지기 시작하더니 최근 몇 주간은 너무 바빠서 주말 출근도 할 지경이었습니다.(물론 그 전에는 그냥 게으른 것이었지만..) 어쨌든, 제가 공부를 조금씩 미루면서 스스로에게 한 변명은 이것이었습니다. ‘내일하지 뭐. 나중에 하지 뭐.’ 그런데 그게 회사 일이 바빠지니까 아예 공부할 시간조차 나지 않더군요.그러니 평소에 조금씩이라도 꾸준히 공부해야 된다는 것을 최근 일로 많이 느끼게 되었습니다. 3. 무조건은 없다.개발을 하면서 경계해야되는 부분이죠. 세상에 ‘무조건’ 이라는 것은 없다.그럼에도 불구하고 공부를 하면서 익혔던 부분들을 무의식 중에 적용을 하다가 낭패(?)를 겪은 적이 있습니다. 회사에서 RDB 접속을 위해 Workbench를 사용하고 있습니다.DB라는 것이 항상 신중하게 사용해야되기 때문에 저는 Workbench의 auto commit 기능을 꺼놓고 사용을 하고 있었습니다.그런데 제가 로컬에서 작성한 api로 데이터를 넣은 후 Workbench에서 select문으로 확인하면 데이터가 들어가지 않은 것으로 표시가 되었습니다.계속 헤매다가 다른 분이 도와주셔서 찾은 원인은 바로 그 auto commit 때문이었습니다. 사실 데이터는 제대로 들어갔는데, auto commit이 꺼져있는 상태에서 select 문을 돌리니까마치 dirty read 처럼 데이터가 들어가기 전 내용만 불러오는 현상이 있었습니다. 저는 툴 사용에 있어서 ‘auto commit 은 위험하니까 무조건 끄고 사용해야된다’ 라는 생각을 가지고 있었는데겪어보니 ‘무조건’ 이라는 건 역시나 존재하지 않더군요. 4. 쓸데없는 경력이란 없다.저는 지금 회사에 들어오기 전에는 개발자가 아닌 WAS 엔지니어로 일을 했었습니다.개발이 너무 하고 싶었기에 그 회사를 그만두고 개발자로서 취업 준비를 하고 지금 회사에 들어오게 되었습니다. 이력서에 제 경력을 쓰기는 했습니다만, 실무적으로 크게 도움이 될 것이라고는 생각하지 않았습니다.그런데 이게 왠걸, 도움이 되더군요. 이전 회사에서 PaaS를 조금 접할 수 있는 기회가 있었고, 덕분에 Docker에 대해 알게 되었었는데지금 회사에서도 Docker를 사용하고 있어서 상당히 도움이 많이 되었습니다. 완전히 다른 분야에서 일을 한 것이 아니라면, 정말로 쓸모없는 경력은 없습니다.단, 한 가지 조건은 있겠네요. 바로, 직장에서 무언가 얻는 것이 꾸준히 있어야 한다는 점입니다. 이전 직장은 신입사원이 들어온다고 해서 차분하게 무언가를 알려주거나 교육해주는 시스템은 없었습니다.(물론 중소기업에서 그런게 있는데가 많지는 않겠지만요.) 하지만 그렇다고 제가 스스로 공부도 하지 않고 배우려고 하지 않았다면, 전 직장에서의 경력이 정말로 아무 의미가 없는 시간이 되었을지도 모릅니다.그리고 지금 회사에서 도움이 될 법한 내용들을 알 수 없었을지도 모릅니다. 그러니 회사가 좋든 나쁘든 어떤 곳을 다니는 동안에는 스스로 얻어가는 것이 있어야 할 것 같습니다.월급 이외에 것을 말이죠.그게 지식적인 것이 될수도 있고, 다른 것이 될수도 있겠지만 뭐가 되든 성장한다고 느끼고 있으면 될 것 같습니다. 그래야 나중에 다른 직장을 가더라도 도움이 될테니까요. 5. 개발 외적인 지식이 쓰일데도 있다.이건 그냥 번외 이야기로 제가 겪은 경험을 조금 써보고자 합니다. 최근에 업무로 한 작업 중 일종의 통계를 내는 작업을 한 적이 있습니다.DB에서 해당 내용들을 조회를 해서 결과 값을 계산하는 작업이었습니다. 업무를 받았을 때는 단순히 모든 데이터들의 값들을 하나씩 다 조회해서 전부 계산을 하는 식으로 전달을 받았었는데,이걸 천천히 살펴보다보니 다른 방식으로 계산이 가능할 것 같았습니다. 결과적으로 얻고자 하는 값을 풀어서 보니모든 데이터들을 전부 탐색하는 것이 아니라 전체를 일괄적으로 탐색해서 나온 값을 넣어서도 계산이 가능했습니다. 즉, 아주 간단한 수식 변환으로 DB 탐색의 횟수를 확 줄일 수 있었습니다. 이처럼 개발 관련 지식이 아니더라도 업무에 도움되는 경우가 있었습니다.개인적으로는 꽤 신선한 경험이었습니다. 6. 마무리6개월이라는 시간동안 제가 확실히 느낀 것은 ‘역시 개발이 내 적성에 맞구나’ 라는 것이었습니다.앞으로 6개월의 시간이 더 흘러서 1년의 시간이 지났을 때는조금 더 성장한 모습으로 후기를 쓸 수 있으면 좋겠네요.","categories":[{"name":"기타","slug":"기타","permalink":"https://postlude.github.io/categories/%EA%B8%B0%ED%83%80/"}],"tags":[{"name":"job","slug":"job","permalink":"https://postlude.github.io/tags/job/"}]},{"title":"Git의 여러 가지 되돌리기 기능들(checkout, reset, revert)","slug":"git-checkout-reset-revert","date":"2019-03-21T14:46:42.000Z","updated":"2022-02-06T08:39:50.092Z","comments":true,"path":"2019/03/21/git-checkout-reset-revert/","link":"","permalink":"https://postlude.github.io/2019/03/21/git-checkout-reset-revert/","excerpt":"","text":"git을 사용할 때 명령어를 항상 옳게 사용하면 좋겠지만 사람이라는게 실수를 하기 마련이죠.그러기 위해서 필요한 것이 되돌리는 작업들입니다. 이번 시간에는 git의 여러 가지 되돌리기와 관련된 명령어를 알아보고자 합니다. 1. 수정사항 되돌리기개인적으로 가장 빈번하게 발생하는 경우입니다. 코드를 작성하면서 여러 군데에 로그를 출력하는 코드를 삽입했다던가, 개발 환경에서만 작동하는 코드를 임시로 넣었을 때 그 내용을 되돌리는 경우입니다.물론 기존 커밋과 비교해서 일일이 원본으로 수정해도 되지만, 수정한 내용이 많거나 여러 군데에 흩어져 있을 경우에는 상당히 귀찮은 작업입니다. 소스 트리 같은 툴에서는 간단하게 되돌릴 수 있는데 명령어로도 (당연히) 가능합니다.이때 사용하는 것이 git checkout 입니다. 만약 몰랐던 분이라면 조금 놀라실 수도 있겠습니다. (실은 제가 처음 알았을 때 그랬습니다..ㅎ) checkout을 브랜치를 변경하는 명령어로만 알고 있었으니까요. 예시를 보겠습니다. 우선 초기 상태를 만들기 위해 file1이라는 파일을 하나 생성하고 커밋했습니다. 텍스트 한 줄을 file1에 추가했습니다. status를 보면 modified 된 것을 알 수 있습니다.이 상태에서 추가된 라인을 지워도 원래 상태로 돌아오겠지만, 명령어로도 되돌릴 수 있습니다. status를 보면 원래 상태로 돌아온 것을 알 수 있습니다. checkout 명령어를 이용해 수정한 내용을 되돌릴 때 명령어 뒤에 오는 것은 엄밀히 말해 파일 명이 아닌 pathspec 입니다.즉, 위와 같이 여러 개의 파일이 수정된 상황에서 현재 디렉토리로 path를 주게 되면 모든 수정된 파일을 되돌릴 수 있게 됩니다. 하나 주의해야할 점은 새롭게 추가된 파일, 다시 말하자면 Untracked file의 경우에는 checkout을 하더라도 삭제되지 않습니다. 2. Unstaging역시나 빈번하게 발생하는 상황입니다. add 명령어로 stage 상태로 만든 파일을 다시 unstage 상태로 만드는 것입니다.마찬가지로 소스트리에서는 간단하게 가능하지만 명령어로 하는 방법을 알아보고자 합니다. file1을 수정한 후에 stage 상태로 만들었습니다.그런데 사실 unstaging에 대한 해법은 이미 나와있습니다. 잘 보면 unstaging 하려면 reset 명령어를 이용하라는 메시지가 나와있습니다. reset 명령어로 unstage 상태로 만들었습니다.(테스트해보니 HEAD를 소문자로 입력해도 정상적으로 동작합니다.) 3. Commit 되돌리기3.1. reset3.1.1. mixed앞서 reset 명령어가 등장했으니 좀 더 자세히 알아보겠습니다.reset은 말그대로 커밋을 되돌리는 명령어입니다. 우선 초기 상태를 위와 같이 만들었습니다.git을 연습하기 위한 repository인지라 여러 커밋 내용들이 있는데 아래 커밋들은 무시하고 현재 reset origin이라는 커밋부터 시작한다고 보시면 됩니다. 연습을 위해 몇가지 커밋을 추가하겠습니다. file1, file2를 추가하고 각각 커밋했습니다. reset 명령어 뒤에 체크섬을 주어 초기 커밋으로 되돌리기를 시도했습니다.log를 보면 처음 상태인 reset origin 이후의 커밋들이 전부 사라진 것을 볼 수 있습니다. 그런데 이게 왠걸? file1과 file2는 그대로 존재합니다.status를 보면 어떻게 된 상태인지를 파악할 수 있는데요,기본적으로 reset을 사용하게 되면 해당 커밋 이후의 내용은 현재 내가 수정한 것처럼 내용 자체는 남아있게 됩니다. reset을 사용할 때 아무 옵션을 주지 않으면 --mixed 옵션이 기본적으로 들어가게 되는데 그 동작이 위와 같습니다. 3.1.2. soft이번에는 --soft 옵션을 주고 테스트 해보겠습니다.동일하게 file1, file2를 추가하고, 각각 커밋한 후 진행했습니다. 일단 log상으로는 동일하게 커밋 내용이 사라졌네요. 리셋 이후의 수정한 내용 또한 동일하게 존재합니다. 다만, 해당 내용들이 staging area에 존재하게 됩니다. 3.1.3. hard마지막으로 --hard 옵션을 테스트해보겠습니다.역시나 동일한 커밋 상태에서 진행했습니다. 마찬가지로 커밋이 리셋되었습니다. 네, 보시는 바와 같이 --hard옵션은 해당 커밋 이후의 내용을 모조리 날려버립니다.상당히 강력하면서 위험한 옵션이네요. 3.1.4. 정리정리하자면 다음과 같습니다. reset은 두 가지 용도로 사용 가능합니다. unstaging이나 commit reset. commit reset의 경우 세 가지 옵션이 존재하며(mixed, soft, hard), default는 mixed입니다.공통적으로 리셋시킨 커밋 이후의 커밋은 사라지게 됩니다. mixed option : 리셋 이후의 커밋 내용들이 unstage 상태로 존재 soft option : 리셋 이후의 커밋 내용들이 stage 상태로 존재 hard option : 리셋 이후의 커밋 내용들이 전부 삭제 3.2. revertreset 명령어의 경우 해당 커밋 이후는 전부 삭제가 됩니다.그런데 특정 커밋만 되돌리고 싶은 경우도 발생할 수 있지 않을까요?그럴 때 사용하는 것이 revert 명령어입니다. 기본 상태는 위와 같습니다. revert origin 커밋을 기준으로 file1과 file2를 추가했습니다.이 상태에서 file1을 추가한 커밋만 되돌리고 싶다면 아래와 같이 명령어를 실행하면 됩니다. 1git revert [되돌릴 커밋] 그러면 위와 같은 창이 뜨게 됩니다. revert message를 입력합니다. 로그를 보면 reset과는 다르게 커밋이 사라지지 않고, 오히려 좀 전에 입력한 메시지로 새로운 커밋이 생성되었습니다. 파일 상태를 보면 file1이 사라진 것을 알 수 있습니다.즉, revert 명령어는 해당 커밋을 취소하는 커밋을 생성하는 것입니다. 그런데 이전 커밋을 취소하고 수정 후에 다시 반영하고 싶은 경우도 있지 않을까요?그럴 때는 -n 옵션을 사용하면 됩니다. -n 옵션을 사용하게 되면 이전과는 다르게 바로 커밋 메시지를 작성하는 창으로 전환되지 않습니다.그리고 취소한 변경사항이 stage area에 존재하게 됩니다.취소한 내용이 파일 내용을 수정한 것이라면 원하는 대로 다시 수정해 커밋하면 됩니다. 4. 정리이번 포스팅에서는 git의 여러 가지 되돌리기 기능들을 살펴보았습니다.사실 문서를 보니 제가 포스팅에 작성한 내용 말고도 merge를 되돌린다거나 하는 등의 내용이 있었는데요,그런 내용들은 저도 좀 더 공부를 한 후에 다음에 다른 포스팅으로 정리해보겠습니다.","categories":[{"name":"Git","slug":"git","permalink":"https://postlude.github.io/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://postlude.github.io/tags/git/"}]},{"title":"VirtualBox 사용 시 유용한 몇 가지 팁들","slug":"virtualbox-tips","date":"2019-03-11T06:35:51.000Z","updated":"2022-02-06T08:39:50.201Z","comments":true,"path":"2019/03/11/virtualbox-tips/","link":"","permalink":"https://postlude.github.io/2019/03/11/virtualbox-tips/","excerpt":"","text":"지난 포스트까지 virtualbox에 centos 설치하는 방법까지 알아보았습니다. 이번 포스트에서는 centos에서 작업할 때 유용한 미세먼지(?) 팁 들을 알려드리고자 합니다. 1. 유용한 Virtual Box 기능1.1. 복제가장 많이 쓰는 기능 중에 하나입니다.테스트하기 위해 리눅스용 환경을 만들었다고 가정해보죠. 열심히 테스트를 한 후에 다른 테스트를 하려고 합니다. 이때 지금까지 테스트한 게스트 os를 남겨두고 싶다거나 재사용이 불가능한 상황이라면 처음부터 환경 세팅을 해야하는 끔찍한 일이 발생할수도 있습니다.이럴 때는 처음 환경을 구성해 둔 다음에 복제를 해서 사용하는 것이 편합니다. 사용 방법은 간단합니다. 복제하고자 하는 os를 우클릭 - 복제를 선택합니다. Mac 주소 값을 선택합니다. 새로운 주소로 생성하겠습니다. 복제 방식을 선택합니다. 완전한 복제를 선택하겠습니다. 끝입니다! 간단하죠? 조금 시간이 지나면 복제된 OS가 생성됩니다. 저 같은 경우는 위의 캡쳐와 같이 가장 기본이 되는 OS만 설치한 것을 계속 복제해서 사용하고 있습니다. 1.2. 스냅샷복제와 유사하지만 조금 다른 스냅샷이라는 기능도 있습니다.이 기능은 하나의 게스트 OS에서 중간 저장과 같이 스냅샷을 찍어두고 불러올 수 있는 기능입니다. 간단한 예시를 보겠습니다. 루트 디렉토리 하위에 test 라는 디렉토리를 만들었습니다. 이 상태에서 스냅샷을 찍겠습니다. 게스트 os 옆의 버튼을 클릭하고 스냅샷을 선택합니다. 찍기를 선택합니다. 어떤 시점의 스냅샷인지를 파악하기 위한 정보를 기입하는 창이 나옵니다. 테스트이니 그대로 진행하겠습니다. 제가 지정한 이름(스냅샷 1)으로 스냅샷이 저장되었습니다. 아까 만들었던 /test 디렉토리 하위에 test_file 이라는 이름의 파일을 하나 생성하겠습니다. 실행 중이던 virtualbox를 종료하고 스냅샷을 만들었던 창을 보면 Restore 라는 버튼이 활성화되어 있습니다. 이걸 선택합니다. 현재 상태 스냅샷은 굳이 만들진 않겠습니다. 복구한 게스트 os를 실행하면 이전에 제가 작업한 화면으로 복구됩니다. /test 하위에 test_file도 존재하지 않습니다. 2. 마우스 커서가 동작하지 않을 때이번에 virtualbox에서 리눅스를 설치할 때 한 가지 당황했던 점이 있었습니다.바로 마우스 커서가 보이지 않는 것이었습니다. 그러다보니 설치 시에 모든 선택을 탭으로 이동하여(…) 작업했었는데요, 구글링을 통해 해결 방법을 찾았습니다. 게스트 OS를 실행시키지 않은 상황에서만 설정 변경이 가능합니다. 방법은 간단합니다. 게스트 OS 설정으로 들어가 **[시스템 - 마더보드 - 포인팅 장치]**에서 설정을 변경해주시면 됩니다. 3. 터미널 설정3.1. 화면 꺼짐 설정virtualbox를 이용해 작업을 하다보면 일정 시간이 지났을 때 화면이 잠기게 됩니다. 이 경우 비밀번호를 다시 입력해야되서 상당히 귀찮습니다.그래서 개인적으로는 화면을 항상 켜짐 상태로 설정해둡니다. 3.2. 터미널 백그라운드 색상 설정기본적으로 터미널은 흰 바탕에 검은색 글씨입니다. 흰 바탕색을 오래 보다 보면 눈이 너무 아프기 때문에 배경색을 변경하는 편입니다.터미널에서 **[Preferences - Colors]**에 들어가서 시스템 테마 사용을 체크 해제하고 원하는 색상이나 테마를 선택하시면 됩니다.","categories":[{"name":"기타","slug":"기타","permalink":"https://postlude.github.io/categories/%EA%B8%B0%ED%83%80/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://postlude.github.io/tags/linux/"}]},{"title":"VirtualBox에서 CentOS 설치하기 (2)","slug":"virtualbox-centos-install-2","date":"2019-02-26T04:59:02.000Z","updated":"2022-02-06T08:39:50.175Z","comments":true,"path":"2019/02/26/virtualbox-centos-install-2/","link":"","permalink":"https://postlude.github.io/2019/02/26/virtualbox-centos-install-2/","excerpt":"","text":"지난 포스트에서 윈도우 환경에 VirtualBox로 CentOS 가상 머신을 설치했습니다. 그 상태로도 사용할수는 있지만 화면 사이즈 조절이나 클립보드 복사 등의 기능이 제대로 동작하게 하기 위해서는 게스트 확장 이미지를 설치해야 합니다. 가상 머신을 실행시킨 상태에서 상단의 [장치 - 게스트 확장 CD 이미지 삽입] 을 클릭합니다.아래와 같이 메시지가 뜰 텐데 Run 을 선택해주시면 됩니다. 권한을 묻는 창이 뜬다면 해당 계정의 패스워드를 입력하시면 됩니다. 그러면 아래와 같은 터미널이 뜹니다.정확히는.. 뜰수도 있고 아닐수도 있습니다. 제가 이렇게 말씀드리는 이유는 이번에 뜬 메시지가 예전에 제가 테스트할 때 떴던 메시지와 달랐기 때문입니다.중요한 점은 아래와 같은 메시지라면 설치에 실패한 것입니다. 이전에는 fail 이라는 메시지가 떴었습니다. 이 문제를 해결하기 위해 구글링을 해서 여러 방법을 시도했었는데요, 그 중에 성공한 방법을 전달드리겠습니다. 이 다음부터 설명드리는 방법이 베스트인지는 저도 잘 모르겠습니다. 혹시 더 좋은 방법을 아시는 분들은 알려주시면 감사하겠습니다. 아래의 명령어를 실행합니다. 명령어 실행을 위해 root 권한이 필요합니다. 123cd &#x2F;etc&#x2F;yum.repos.d&#x2F;wget http:&#x2F;&#x2F;download.virtualbox.org&#x2F;virtualbox&#x2F;rpm&#x2F;rhel&#x2F;virtualbox.repoyum install -y kernel source kernel-devel gcc yum 으로 패키지 설치가 끝났다면 다시 게스트 확장 이미지를 실행시켜야 합니다.하지만 동일하게 [장치 - 게스트 확장 CD 이미지 삽입]을 클릭하면 이미 이미지가 삽입되어 있다는 식의 메시지가 뜨게 됩니다.가상 머신의 바탕화면을 보면 CD 모양의 아이콘이 하나 생긴 것을 확인하실 수 있습니다. 처음 게스트 확장 이미지를 삽입했을때 생성된 것입니다. 아이콘을 우클릭해 Open in Terminal 을 클릭합니다. autorun.sh 을 실행합니다. 아까와 유사하지만 조금 다른 터미널 메시지가 뜹니다. 정상적으로 설치가 되었으니 재부팅을 해야합니다. 가상 머신 종료 후 재시작하기 전에 몇 가지 설정을 적용하도록 하겠습니다. [설정 - 일반 - 고급 - 클립보드 공유 : 양방향][설정 - 일반 - 고급 - 드래그 앤 드롭 : 양방향] 클립보드를 설정하면 가상 머신과 호스트 간의 복사 붙여넣기가 가능해집니다.드래그 앤 드롭을 설정하면 마우스로 파일을 끌어다 놓을 수 있게 됩니다…만 크기가 조금만 커져도 시간이 상당히 걸리더군요. 그래서 공유 폴더를 이용하겠습니다. [설정 - 공유 폴더 - 폴더 모양에 + 가 있는 아이콘] 호스트에서 원하는 위치에 빈 폴더를 하나 만들고 해당 경로를 사용하기로 했습니다. 자동 마운트와 항상 사용하기를 체크해주시면 됩니다. 드디어! 설정이 끝났습니다. 가상 머신을 실행하시면 됩니다. 이제 VirtualBox 창 크기를 조절하면 그 크기에 맞게 CentOS 창 사이즈도 변경됩니다.또한, 호스트와 게스트 간의 클립보드 공유가 가능해집니다. 호스트에서 ctrl + c 로 복사한 내용을 게스트 터미널에서 shift + insert 로 붙여넣기 할 수 있습니다.다만, 게스트 터미널에서 호스트로 붙여넣기 하기 위해서는 게스트 터미널의 선택 영역 - 우클릭 - 복사를 해야 복사가 됩니다. 공유 폴더의 경우 /media 하위에 마운트가 됩니다.아래와 같이 호스트 폴더명 앞에 sf_ 붙은 이름으로 마운트가 되어 있습니다. 호스트든 게스트든 마운트한 디렉토리에 파일을 생성하면 어느 쪽에서든 사용가능합니다.다만 소유자가 root로 되어 있으므로 게스트에서 사용시 root 권한이 있는 유저만 사용 가능합니다. 이렇게 VirtualBox에서 CentOS를 사용하기 위한 모든 세팅이 끝났습니다.다음 포스트에서는 VirtualBox의 몇 가지 기능과 사용 팁 등을 말씀드리겠습니다.","categories":[{"name":"기타","slug":"기타","permalink":"https://postlude.github.io/categories/%EA%B8%B0%ED%83%80/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://postlude.github.io/tags/linux/"}]},{"title":"VirtualBox에서 CentOS 설치하기 (1)","slug":"virtualbox-centos-install","date":"2019-02-19T02:06:53.000Z","updated":"2022-02-06T08:39:50.180Z","comments":true,"path":"2019/02/19/virtualbox-centos-install/","link":"","permalink":"https://postlude.github.io/2019/02/19/virtualbox-centos-install/","excerpt":"","text":"(벼르고 벼르던) VirtualBox 세팅에 관한 포스팅을 시작하겠습니다.우선 시작하기에 앞서 포스팅의 목적은 제가 성공한 방법을 정리하고 전달하고자 함입니다.혹 더 좋은 세팅이나 방법을 알고 계시면 전달해주시면 감사하겠습니다. 이 글에서 작성한 내용은 대체로 경험에 의존한 것입니다. 구글링을 해서 찾은 내용을 바탕으로 하긴 했습니다만 근거가 명확하지 않을 수 있습니다. 0. 목적Windows 환경에서 리눅스를 사용하기 위함입니다. 사실 VirtualBox를 이용한 방법 말고도 AWS나 WSL(Windows Subsystem for Linux)을 이용하는 방법도 있습니다.(그리고 사실 더 간단하긴 합니다..)이전에 다른 포스트에서 WSL을 이용하는 방법도 포스팅했으니 참고하시면 될 것 같습니다. 다만, WSL의 경우 docker를 사용하기는 어려운 것 같았습니다. 찾아보니 아직 완전한 리눅스 시스템이 아니라서 그런 것 같습니다.(구글링 했을 때 방법은 있었습니다만 호스트(windows)에서 docker를 똑같이 돌리는 방법으로 봤습니다. 이럴바에야 그냥 window용 도커를 사용하는 것이 나을 것 같았습니다. 그런데 window용 도커는 또 호스트와 마운트가 잘 안되는 것 같더군요. 결국은 로컬에서 도커 테스트시에는 virtualbox를 써야되는 것 같습니다.) 1. 다운로드우선 VirtualBox를 다운받습니다. (링크)VirtualBox는 기본 설치만 할 경우 화면 크기 조정이나 클립보드 공유 등의 기능이 제대로 되지 않습니다. 따라서 VirtualBox뿐만 아니라 Extension Pack도 함께 설치하셔야 합니다. Linux는 CentOS 7을 사용하겠습니다. (링크)미러 중에서 아무거나 받아주시면 됩니다. 2. VirtualBox 설치VirtualBox를 먼저 설치한 후 Extension Pack을 설치해주시면 됩니다.VirtualBox 설치시 라이센스 동의는 스크롤을 맨 밑으로 내리면 ‘동의합니다’가 활성화됩니다. 3. CentOS 설치3.1. 호스트 키 조합 설정본격적으로 CentOS를 설치하기 전에 사전 세팅을 하나만 하도록 하겠습니다.가상머신과 호스트의 마우스 인식을 전환하는 버튼이 기본적으로는 [Right Control] 로 되어 있습니다. 그런데 막상 설치해보면 잘 동작하지 않습니다. 이 키 세팅을 변경하도록 하겠습니다. [파일 - 환경설정 - 입력 - 가상 머신] 으로 들어가면 다음과 같이 ‘호스트 키 조합’ 이 있습니다. 이 조합을 (좌측)Ctrl + Alt 로 변경하겠습니다. 사실 최종적인 세팅을 마치면 가상머신과 호스트 사이를 마우스가 왔다갔다 할 때 아무런 키를 누르지 않아도 됩니다. 하지만 이 설정을 변경하지 않으면 설치시 상당히 불편해지기 때문에 변경하도록 하겠습니다. 3.2. 가상 머신 설정이제 CentOS를 설치하도록 하겠습니다. 새로 만들기를 선택해주시면 됩니다.(캡쳐를 좀 잘못해서 이미 머신이 생성되어 있는 것으로 나왔는데 기본적으로는 아무것도 없습니다.) 가상 머신의 이름을 입력합니다. 나중에 수정가능하니 크게 신경쓰지 않으셔도 됩니다.버전 같은 경우도 크게 상관은 없지만 기왕이면 redhat으로 맞춰주도록 하겠습니다. 메모리를 설정합니다. 중요한 것은 2G 이하로 설정할 경우 가상 머신이 굉장히 느릴 수 있습니다.반면에 너무 메모리를 너무 많이 설정하면 호스트(Windows)가 느려질 수 있습니다. 제 노트북의 메모리는 8G이므로 4G로 설정하겠습니다. 디스크 사이즈를 설정합니다. 20G 정도로 할당해두니 크게 문제없이 사용가능했습니다. 부팅 디스크를 선택합니다. 사전에 다운받은 CentOS 이미지를 선택하면됩니다. 호스트 키를 변경하기 전에 캡쳐를 해서 Right Control 로 표시 되었는데, 앞서 변경하셨다면 Ctrl + Alt로 표시가 될 겁니다.[잡기]를 선택하시면 됩니다. 3.3. CentOS 설치이제 CentOS 를 설치하겠습니다. 키보드 위 방향키를 한 번 눌러서 Install CentOS 7을 선택하시면됩니다.(기본적으로 2번째 Test가 선택되어 있기 때문에 바로 엔터키를 누르시면 안됩니다.) 언어 선택입니다. 저는 기본적으로 영어를 선택했습니다. 이 시점부터는 마우스가 보이는 게 정상입니다. 저도 보이지 않아서 당황했었는데요, 간단하게 설명드리면 [가상 머신 설정 - 시스템 - 마더보드 - 포인팅 장치]에서 변경해주시면 됩니다.이에 대한 부분은 다른 포스트에서 여러 팁과 함께 다룰 예정입니다. 아직까지는 마우스가 호스트로 자연스럽게 빠져나가지 않을 겁니다. 아까 설정한 호스트 키 조합(Ctrl + Alt)으로 Windows로 마우스를 빠져나갈 수 있습니다. 여기서부터는 설치 중인 가상 머신을 강제 종료할 경우 이어서 설치가 되지 않습니다. 처음부터 다시 설치하셔야 합니다. 설정에 관련된 세팅들입니다. 여기서 DATE &amp; TIME, SOFTWARE SELECTION, NETWORK &amp; HOST NAME 정도만 세팅하겠습니다. 먼저 시간 관련 세팅입니다. Asia / Seoul 로 세팅하겠습니다. SOFTWARE SELECTION 입니다. Server with GUI를 선택하셔야 합니다. Minimal로 설치했을 때는 이후 ‘게스트 확장 이미지’를 설치하는 과정에서 제대로 되지 않았습니다.(혹시 Minimal로도 가능한 방법을 아시는 분은 알려주시면 감사하겠습니다.) 네트워크 세팅은 화면 우측에 현재 네트워크의 토글 버튼이 있을텐데 활성화 해주시면 됩니다. 활성화할 경우 가상 머신이 부팅될 때 자동으로 네트워크에 연결됩니다.사실 설치 이후에 설정도 가능한 부분이라 크게 중요하진 않습니다. INSTALLATION DESTINATION은 파티션 분할을 하는 내용입니다. 저는 크게 관여하지 않을 것이라 이미 선택된 Autometic partitioning으로 두겠습니다.Done 을 눌러주시면됩니다. 설정을 마쳤으니 Begin Installation 을 선택하시면 됩니다. 설치가 되는 동안 root 계정에 대한 패스워드와 사용자 계정을 생성할 수 있습니다.사용자 계정을 생성하지 않아도 부팅시에 자동으로 하나의 계정을 생성하게 되므로 여기서 생성하도록 하겠습니다.Make this user administrator를 선택하게 되면 해당 유저가 sudo 권한을 가지게 됩니다. 설치가 완료되면 Reboot을 선택합니다. 재부팅되면 OS를 선택하는 화면이 나옵니다. 위쪽을 선택하시면 됩니다.아래쪽은 rescue 모드라고 해서 윈도우의 안전모드와 비슷한 모드라고 합니다. 라이센스에 대한 내용도 체크해주시면 됩니다. 드디어 CentOS 설치를 마쳤습니다. 이렇게 사용하셔도 문제는 없습니다만 화면 크기 조정이나 여러 부분에서 불편한 점이 있기 때문에 다른 세팅들을 좀 더 할 예정입니다. 다음 포스트에 이어집니다.","categories":[{"name":"기타","slug":"기타","permalink":"https://postlude.github.io/categories/%EA%B8%B0%ED%83%80/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://postlude.github.io/tags/linux/"}]},{"title":"JavaScript의 Property Descriptor","slug":"javascript-property-descriptor","date":"2019-02-04T07:41:50.000Z","updated":"2022-02-06T08:39:50.127Z","comments":true,"path":"2019/02/04/javascript-property-descriptor/","link":"","permalink":"https://postlude.github.io/2019/02/04/javascript-property-descriptor/","excerpt":"","text":"JavaScript의 Property Descriptor에 대해 정리해보겠습니다.Property Descriptor란 말 그대로 Property를 설명해주는 객체를 말합니다.Property Descriptor는 Object.getOwnPropertyDescriptor() 함수를 통해 가져올 수 있습니다. 123456var person = &#123; name: 'postlude'&#125;;var nameDescriptor = Object.getOwnPropertyDescriptor(person, 'name');console.log(nameDescriptor); 1234567// result&#123; value: 'postlude', writable: true, enumerable: true, configurable: true&#125; Property Descriptor는 2가지 타입으로 나뉩니다. data descriptor와 accessor descriptor입니다. A data descriptor is a property that has a value, which may or may not be writable. An accessor descriptor is a property described by a getter-setter pair of functions. A descriptor must be one of these two flavors; it cannot be both. MDNObject.defineProperty() 위의 코드에서 여러 가지로 테스트해보겠습니다. 1. Data Descriptor1.1 Default ValueData Descriptor란 value를 가진 property를 말합니다. 위의 예시에서 nameDescriptor가 Data Descriptor가 그 예시입니다.property를 추가할 때 Object.defineProperty() 함수를 이용하면 property descriptor를 바꿔서 추가할 수 있습니다. 123456Object.defineProperty(person, 'age', &#123; value: 10&#125;);var ageDescriptor = Object.getOwnPropertyDescriptor(person, 'age');console.log(ageDescriptor); 1234567// result&#123; value: 10, writable: false, enumerable: false, configurable: false&#125; Object.defineProperty() 함수를 이용하여 property를 추가할 때 값을 넣지 않으면 리터럴 형식으로 넣는 것과는 다르게 default가 false라는 것을 알 수 있습니다. 1.2 enumerable위와 같은 상태에서 person을 출력해보겠습니다. 12console.log(person);console.log(person.age); 123// result&#123; name: 'postlude' &#125;10 person 을 출력해도 age는 보이지 않습니다. 하지만 Object.defineProperty() 를 이용해 age 라는 property를 추가했기 때문에 명시적으로 호출하면 값을 출력할 수 있습니다.즉, enumerable 은 해당 property가 보일지 말지를 결정하는 속성입니다. 1.3 writable이름에서 바로 느껴지듯이 writable은 해당 속성이 = 를 이용해 새로운 값을 할당할 수 있는가에 대한 속성입니다.(enumerable은 true로 변경하고 진행했습니다.) 12345678910Object.defineProperty(person, 'age', &#123; value: 10, enumerable: true,&#125;);var ageDescriptor = Object.getOwnPropertyDescriptor(person, 'age');console.log(ageDescriptor);console.log(person);person.age = 20;console.log(person.age); 123456789// result&#123; value: 10, writable: false, enumerable: true, configurable: false&#125;&#123; name: 'postlude', age: 10 &#125;10 writable이 false이기 때문에 person.age 를 새로운 값으로 바꿀 수 없습니다. 1.4 configurableconfigurable은 다음과 같이 설명되어 있습니다. true if and only if the type of this property descriptor may be changed and if the property may be deleted from the corresponding object. MDNObject.defineProperty() property descriptor를 변경할 수 있는가에 대한 내용입니다. 12345678Object.defineProperty(person, 'age', &#123; value: 10,&#125;);Object.defineProperty(person, 'age', &#123; value: 20, enumerable: true, writable: true&#125;); 위와 같이 age를 추가한 후에 다시 age를 추가하려고 시도하면 1234// resultObject.defineProperty(person, 'age', &#123; ^TypeError: Cannot redefine property: age 에러가 발생합니다.만약 configurable 이 true라면 12345678910Object.defineProperty(person, 'age', &#123; value: 10,&#125;);Object.defineProperty(person, 'age', &#123; value: 20, enumerable: true, writable: true&#125;);console.log(person); 12// result&#123; name: 'postlude', age: 20 &#125; 정상적으로 동작하게 됩니다. 2. Accessor DescriptorAccessor Descriptor는 getter와 setter로 표현되는 descriptor입니다.Accessor Descriptor도 동일하게 configurable과 enumerable을 가지고 있으며 data descriptor와 동일하게 동작합니다. 1234567891011121314Object.defineProperty(person, 'job', &#123; enumerable: true, get: function() &#123; console.log('job getter'); &#125;, set: function() &#123; console.log('job setter'); &#125;&#125;);var jobDescriptor = Object.getOwnPropertyDescriptor(person, 'job');console.log(jobDescriptor);person.job;person.job = 'developer'; 123456789// result&#123; get: [Function: get], set: [Function: set], enumerable: true, configurable: false&#125;job getterjob setter get으로 지정된 함수는 property를 읽을 때, set으로 지정된 함수는 property를 세팅할 때 자동적으로 호출되는 함수입니다. 3. Private 변수위의 내용을 바탕으로 외부에서 접근 불가능한 private 변수를 JavaScript에서도 비슷하게 흉내낼 수 있습니다. 12345678910111213141516171819Object.defineProperty(person, '_job', &#123; value: 'student', writable: true&#125;)Object.defineProperty(person, 'job', &#123; enumerable: true, get: function() &#123; return this._job; &#125;, set: function(value) &#123; this._job = value; &#125;&#125;);var jobDescriptor = Object.getOwnPropertyDescriptor(person, 'job');console.log(jobDescriptor);console.log(person.job);person.job = 'developer';console.log(person.job); 123456789// result&#123; get: [Function: get], set: [Function: set], enumerable: true, configurable: false&#125;studentdeveloper 실제 데이터는 _job에 들어가지마 enumerable이 false 이기 때문에 외부에서 확인이 불가능합니다.호출할 수 있는 property는 job이고, job을 통해 _job의 값을 읽거나 쓸 수 있게 됩니다.물론 person._job = &#39;test&#39;; 라는 식으로 코드를 작성하면 값을 변경할 수 있게 됩니다. 완벽한 private 변수는 아닌 셈이지요. 사실 private 변수를 이용하기 위해선 클로저로도 가능합니다. 다른 포스트로 알아보겠습니다.","categories":[{"name":"JavaScript","slug":"javascript","permalink":"https://postlude.github.io/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://postlude.github.io/tags/javascript/"}]},{"title":"Windows에서 리눅스(ubuntu) 사용하기","slug":"windows-ubuntu","date":"2019-01-25T02:01:20.000Z","updated":"2022-02-06T08:39:50.207Z","comments":true,"path":"2019/01/25/windows-ubuntu/","link":"","permalink":"https://postlude.github.io/2019/01/25/windows-ubuntu/","excerpt":"","text":"내가 알고있는 윈도우에서 리눅스를 사용하는 방법은 3가지이다. 로컬에 가상 머신으로 띄우기 AWS와 같은 클라우드 사용 ms store에서 ubuntu 다운로드 1번의 경우에는 설정이 너무 오래 걸린다. 따로 포스팅을 할 생각이긴 하지만 어쨌든 오래 걸린다. AWS 같은 경우에는 간단하게 사용 가능하나 단순 테스트나 명령어 연습 같은 용도로 쓰기에는 애매하다. 초보 개발자가 쓰기엔 계정 같은 부분이 역시 어려울 수 있으므로. 최근에 알게 된 것이 3번 방법인데 정말 간단하게 사용하기에는 편할 것 같아서 포스팅하게 되었다. Windows 10 이상만 가능하다고 알고 있습니다. 1. 스토어 접속검색으로 store 를 검색하면 Microsoft Store에 접속할 수 있다. 2. ubuntu 검색 후 다운로드검색해 나온 ubuntu 중 하단 맨 좌측에 있는 ubuntu를 설치 3. 설정설치 완료 후 실행을 누르면 다음과 같은 창이 뜬다. 메시지를 잘 읽어보면 무언가가 enabled 되지 않았다고 뜬다. 설정이 하나가 빠진 것이다. Windows 기능 켜기/끄기로 들어간다. 조금 내려가다보면 Linux용 Windows 하위 시스템 이라는 옵션이 있다. 이걸 설정해야 한다. 재부팅하라는 메시지가 뜨므로 재부팅한다. ubuntu 를 검색해서 다시 실행한다. 좀 기다리면 설치되고 위와 같이 username을 입력하라는 메시지가 뜬다. 유저 이름과 패스워드를 설정하면 이제 리눅스를 사용할 수 있다. 4. 사용4.1 root 계정으로 전환할 땐 어떻게 해야할까?정답은 처음 생성한 계정에 sudo 권한이 있으므로 그냥 sudo su - 명령어로 전환 가능하다. 4.2 윈도우에 있는 파일들은 어떻게 사용할까?기본적으로 드라이브는 /mnt 하위에 마운트되어 있다. 5. 기타일단 리눅스 환경을 빠르게 구성할 수 있고, 설정도 거의 없어서 간단하게 사용할 수 있는 것은 장점인 것 같다. 다만, 탭으로 여러 창을 띄울 수는 없어서 그건 좀 불편할 것 같긴 하다.","categories":[{"name":"기타","slug":"기타","permalink":"https://postlude.github.io/categories/%EA%B8%B0%ED%83%80/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://postlude.github.io/tags/linux/"}]},{"title":"Hexo 블로그에서 카테고리(태그)가 제대로 보이지 않을 때","slug":"hexo-category-not-found","date":"2019-01-20T15:21:44.000Z","updated":"2022-02-06T08:39:50.121Z","comments":true,"path":"2019/01/21/hexo-category-not-found/","link":"","permalink":"https://postlude.github.io/2019/01/21/hexo-category-not-found/","excerpt":"","text":"결론부터 말하자면 예전 Hexo 이미지 업로드 와 굉장히 유사한 상황이 발생했다. 블로그의 Category 항목에서 다른 카테고리로는 이동(검색)이 되는데 유독 Hexo 카테고리만 404 not found가 나오는 현상이 발생한 것이다. 검색을 해보니 다음과 같은 글을 볼 수 있었다. Always remember: Linux (instead of Windows, for example) always split all files/directories with case-sensitive. It mean in same directory can be JavaScript, Javascript and javascript — and all of they will be with exclusive, different pathes. So, actual trouble with Hexo — we want to use case-sensitive in categories/tags with pretty-fine vision (JavaScript), but it will generate case-sensitive links on categories/tags too (categories/JavaScript/) and always create any problems with rendering and SEO. https://github.com/iissnan/hexo-theme-next/issues/1903 글을 읽고 아차 싶어 배포한 github repo에 들어가보니 다음과 같이 되어 있었다. 처음에 카테고리가 Hexo인 글을 쓸 때 ‘h’exo로 만들고 이후에 대문자로 수정했는데 github에서는 동일한 것으로 판단하여 수정되지 않은 것 같았다. 아래 링크에서는 2가지 해결방법을 제시하고 있었다. In main Hexo _config.yml (auto): 123# Writingfilename_case: 1 You can use maps on Categories and Tags (manual): 123456789default_category: uncategorizedcategory_map:NexT: nextHexo: hexotag_map:JavaScript: javascript And in *.md: 123456789categories:- NexT- Hexotags:- JavaScript#ORcategories: [NexT, Hexo]categories: [JavaScript] https://github.com/iissnan/hexo-theme-next/issues/1904 2번 방법은 카테고리가 추가될 때마다 수정을 해야된는 것으로 보여 번거로울 것 같았다. filename_case 옵션의 의미는 다음과 같다고 한다. It will generate case-sensitive in blog vision (JavaScript),but do all links of categories/tags to lowercase (categories/javascript/). 즉, 링크 주소는 무조건 소문자로 만들어 준다는 것. 아무래도 주소는 대문자가 있는 것보단 소문자만 있는 것이 보기 편하므로 옵션을 적용하기로 했다. 그리고 기존에 있던 대문자로된 디렉토리들을 전부 바꿔야하기 때문에 _config.yml에 위의 filename_case 옵션 적용 public/categories/ 하위의 영문으로 된 디렉토리를 전부 삭제하고 hexo d 명령어로 배포만 실시(이렇게 하면 카테고리 디렉토리가 github repo에서 삭제된다.) hexo g -d 명령어로 generate 및 deploy(위의 옵션을 바꿨으므로 포스트에서 대문자로 썼더라도 github에는 모두 소문자로 배포된다.) 이렇게 진행했다. 깔끔하게 해결.","categories":[{"name":"Hexo","slug":"hexo","permalink":"https://postlude.github.io/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://postlude.github.io/tags/hexo/"}]},{"title":"git stash 사용하기","slug":"git-stash","date":"2019-01-20T09:49:54.000Z","updated":"2022-02-06T08:39:50.103Z","comments":true,"path":"2019/01/20/git-stash/","link":"","permalink":"https://postlude.github.io/2019/01/20/git-stash/","excerpt":"","text":"혼자서 작업을 할 때는 사실 stash 기능을 많이 쓸 일은 없었다. 간혹 파일 수정하던 내용을 전부 날릴 때 정도? 그런데 회사에 들어와 작업을 하다보니 내가 작업한 내용을 로컬에 저장하고 다른 작업을 하다가 다시 불러와 작업하는 말 그대로 stash 기능을 제대로 사용할 일이 종종 생겼다. 그래서 이번 기회에 제대로 정리해보고자 한다. 0. 기본 상태 이 상태에서 stash를 여러 파일 상태에 적용해보기 위해 다음과 같은 상태로 만들었다. 기존 파일 수정(file1) 새 파일(file2, file3) 추가 추가한 파일 중 하나만 add 해 stage 상태로 두었다. 1. git stash기본적으로는 git stash 명령어만으로도 작성중이던 내용을 저장할 수 있다. 그러면 위와 같이 git stash list로 저장된 내용을 확인할 수 있다. 그런데 git status로 확인한 내용은 내 생각과는 약간 달랐다. file3은 저장되지 않고 그대로였다. 일단 나중에 다시 보기로 하고 stash에 저장한 내용을 다시 불러와보자. git stash apply [stash 번호]로 stash에 있는 내용을 적용할 수 있다. 그러면 처음 상태로 파일을 되돌릴 수 있다. 2. stash untracked filestash 명령어를 실행했을 때 왜 file3은 stash에 저장되지 않았을까? 이유는 다음과 같았다. 기본적으로 git stash 명령어는 git이 tracking 하고 있는 파일에 한해 적용된다. 즉, file3은 새로 추가되어 untracked file 이라 stash 로 저장되지 않은 것이다. file2는 git add 명령어를 통해 staging area에 있었기 때문에 stash 명령어로 저장이 될 수 있었다. 그럼 이렇게 새로 만든 파일을 stash로 저장하는 방법이 없을까? 당연히(?) 있다. 1git stash -u untracked file인 file3까지 저장되었다. 동일하게 git stash apply명령어로 복원 가능하다. 3. git stash save위 상태에서 git stash list로 저장된 목록을 보면 다음과 같다. 맨 처음에 stash로 저장한 내용을 삭제하지 않았기 때문에 저장한 목록이 2개다. 그런데 0번과 1번중 어떤게 먼저 저장한 것이고 어떤 게 나중에 저장한 것일까? 정답은 0번이 나중에 저장한 내용이다. 즉, 가장 최근에 stash로 저장한 내용이 0번이 된다. 그런데 보기 불편하다. 커밋처럼 메시지를 주고 싶다. 역시 가능하다. 1git stash save &#39;stash message&#39; 일단 apply로 원복시킨 다음에 save했다. save 명령어도 마찬가지로 -u 옵션을 주어야 untracked file도 저장된다. 4. stash push / popsave / apply 와 유사하게 push / pop 명령어도 존재한다. 차이점이 있다면 pop 을 하면 stash list에서 해당 내용이 사라진다. 또한 push를 할 때 save처럼 메시지를 주고 싶다면 -m옵션을 사용해야 한다.(untracked file을 저장하기 위해서는 동일하게 -u옵션을 사용한다.) 역시 기본 상태로 돌려놓고 명령어를 실행했다. 5. save vs. pushstash를 공부하던 도중 다음과 같은 자료를 발견했다. git stash push 로의 이동2017년 10월 말 Git 메일링 리스트에는 엄청난 논의가 있었습니다. 논의는 git stash save 명령을 은퇴시키고 git stash push로 대체하는 내용에 대한 것이었습니다.git stash push 명령의 경우 pathspec 으로 선택하여 Stash하는 옵션이 추가되었는데 git stash save 명령이 지원하지 못하는 것이었습니다. git stash save 명령이 곧바로 삭제되는 것은 아니기에 아직 이 명령을 쓰는 것에 대해 걱정할 필요는 없지만 git stash push 명령으로 대체하는 것에 대해 생각해볼 필요가 있습니다. 7.3 Git 도구 - Stashing과 Cleaninggit-scm.com/book/ko/v2/Git-%EB%8F%84%EA%B5%AC-Stashing%EA%B3%BC-Cleaning 그럼 대체 pathspec이 뭐길래 그럴까? 왠지 path라는 단어에서 느낌이 오기 시작한다. help 문서에도 다음과 같이 나와있다. When pathspec is given to git stash push, the new stash entry records the modified states only for the files that match the pathspec. The index entries and working tree files are then rolled back to the state in HEAD only for these files, too, leaving files that do not match the pathspec intact. 그렇다. stash로 저장할 때 특정 파일만 저장하는 것이 가능하다는 말이다. 마찬가지로 기본 상태에서 명령어를 실행했다. 6. 요약 git stash save : stash 저장. 개별 파일들을 따로 저장은 불가 git stash apply : stash 복원. 복원시 저장된 내용이 삭제되지 않는다. git stash push : stash 저장. 개별 파일 저장 가능. git stash pop : stash 복원. 복원시 저장된 내용이 삭제된다. git stash list : stash로 저장된 내용 확인. git stash clear : stash에 저장된 내용 전부 삭제 git stash drop : 개별 stash 저장 내용을 삭제 ※ 번외 테스트이 글을 쓰던 도중 문득 궁금해졌다. stash가 기본적으로 git이 tracking하는 파일만 적용하기 때문에 add 한 파일까지 적용되는 건데, 만약 add 한 파일을 다시 원복시킨 후에는 stash 명령어로 저장이 될까? 혹시나 했지만 역시나 되지 않는다.","categories":[{"name":"Git","slug":"git","permalink":"https://postlude.github.io/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://postlude.github.io/tags/git/"}]},{"title":"Git Submodule 사용하기","slug":"git-submodule","date":"2019-01-06T07:22:21.000Z","updated":"2022-02-06T08:39:50.109Z","comments":true,"path":"2019/01/06/git-submodule/","link":"","permalink":"https://postlude.github.io/2019/01/06/git-submodule/","excerpt":"","text":"어떤 repo 안에서 다른 repo를 관리하고자 할 때 git submodule을 사용할 수 있다. 예시를 보도록 하자. 0. 기본 상태 1. Submodule로 사용할 디렉토리 추가 및 pushrepository를 따로 사용할 수 도 있지만 같은 repository에서 브랜치만 달리해서 submodule을 추가하는 방식을 사용했다.이를 위해 submodule 디렉토리를 추가하고 파일을 추가했다. 1234mkdir submodulecd submoduletouch test.txtecho &#39;test file&#39; &gt;&gt; test.txt 123456git initgit checkout -b submodulegit add .git commit -m &quot;add test file&quot;git remote add origin https:&#x2F;&#x2F;github.com&#x2F;postlude&#x2F;GithubPractice.gitgit push -u origin submodule 해당 프로젝트의 동일한 repo의 submodule이라는 브랜치에 해당 내용이 올라간 것을 확인할 수 있다. 2. Submodule 연결git submodule add 명령어를 통해 submodule을 추가할 수 있다. 1git submodule add [repo_url] [submodule_directory_name(생략시 repo 이름과 동일하게 생성)] 기본적으로 git submodule add 명령어를 이용하면 해당 repo를 clone 받으면서 submodule을 추가하게 된다.하지만 나는 submodule로 사용할 repo를 로컬에 먼저 작성하고 push를 한 다음에 submodule만 추가하는 방법으로 진행했기 때문에 아래와 같은 명령어를 사용했다. 1git submodule add -b submodule https:&#x2F;&#x2F;github.com&#x2F;postlude&#x2F;GithubPractice.git submodule clone과 유사하게 -b 옵션을 이용해 브랜치를 지정해주고, 디렉토리명 또한 지정해주었다.그러면 .gitmodules 이라는 파일이 생성된 것을 확인할 수 있다..gitmodule의 내용을 보면 다음과 같다. 3. Project commit이제 부모 프로젝트에서 해당 내용들을 반영해보자. 이 내용을 github에서 확인해보면 다음과 같다. 단순히 repo 자체를 넣었을 때와는 다르게 자식 repo의 내용을 github에서 확인할 수 있다. 4. Submodule 수정submodule 디렉토리 하위에 test2.txt라는 파일을 만들었다.부모 repo에서 git add . 후에 다시 status를 봐도 색깔이 변하지 않았다. 즉, 자식 repo에서 commit한 내용만 부모에서 반영할 수 있다. 정상적으로 커밋되었다.이제 이 내용을 push하고 확인해보자. 커밋 내용을 보면 정상적으로 올라간 것 같은데 subdmodule 내용이 보이지 않는다.당연하다. submodule을 push하지 않았기 때문이다.submodule 디렉토리로 이동해 push하고 확인하면 정상적으로 내용을 확인할 수 있다. 5. Submodule이 포함된 repo clone 하기submodule이 포함된 디렉토리를 clone 받으면 submodule에는 아무런 내용도 들어있지 않다. 명령어를 통해 submodule을 clone 받는다. 12git submodule initgit submodule update git submodule update 를 하면 위와 같이 clone을 받는다. ※주의 사항※git submodule update 한 후에 submodule 디렉토리에 들어가면 다음과 같이 되어 있다. 정상적인 작업 브랜치로 보이지 않는다. 찾아보니 다음과 같이 설명되어 있다. git submodule update 명령을 실행시키면 특정 브랜치가 아니라 슈퍼프로젝트에 저장된 커밋을 Checkout해 버린다. 그러면 detached HEAD라고 부르는 상태가 된다. detached HEAD는 HEAD가 브랜치나 태그 같은 간접 레퍼런스를 가리키지 않고 커밋을 가리키는 것을 말한다. 데이터를 잃어 버릴 수도 있기 때문에 일반적으로 detached HEAD 상태는 피해야 한다. git-scm.com서브모듈 사용할 때 주의할 점들 그러므로 아래처럼 브랜치를 만들어 이동해준다. 1git checkout -b submodule 참고 문서 : [git-scm.com](https://git-scm.com/book/ko/v1/Git-%EB%8F%84%EA%B5%AC-%EC%84%9C%EB%B8%8C%EB%AA%A8%EB%93%88)","categories":[{"name":"Git","slug":"git","permalink":"https://postlude.github.io/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://postlude.github.io/tags/git/"}]},{"title":"Hexo 테마 적용과 git submodule","slug":"hexo-themes-and-git-submodule","date":"2019-01-01T14:22:43.000Z","updated":"2022-02-06T08:39:50.123Z","comments":true,"path":"2019/01/01/hexo-themes-and-git-submodule/","link":"","permalink":"https://postlude.github.io/2019/01/01/hexo-themes-and-git-submodule/","excerpt":"","text":"hexo로 블로그를 작성하면서 테마를 적용하기 위해선 다음과 같은 식의 명령어로 테마를 적용하게 된다. 1git clone https:&#x2F;&#x2F;github.com&#x2F;ppoffice&#x2F;hexo-theme-hueman.git themes&#x2F;hueman themes 디렉토리 하위에 hueman 이라는 디렉토리가 생기면서 해당 저장소의 내용을 clone 받게 된다. 즉, 아래와 같이 기본적인 설정과 포스트를 작성하는 전체 git repo 안에 다른 repo가 들어간 모양새가 된다. 상위 git repo만 github에 올려 관리를 하던 도중 평소 작업을 하던 곳이 아닌 다른 pc에서 작업을 하기 위해 이 블로그 repo를 clone 받았다. 그런데 이게 왠걸 themes/hueman 디렉토리에 있는 내용은 전혀 받아지지 않았다. 이 상태에선 npm i 후에 hexo server를 실행해도 WARN No layout: index.html 과 같은 메시지가 나오면서 아무 페이지도 뜨지 않습니다. 혹시나 싶어 github에 들어가보니 다음과 같이 아예 선택이 불가능한 상태로 되어 있었다. 그제야 커밋을 하고 푸쉬를 할 때 뭔가 평소와 달랐던 것이 생각났다. 어떻게 해야 이 문제를 해결할 수 있을까? 1. 원인간단하다. 하나의 git 저장소 안에 또다른 git 저장소가 들어있을 경우 한꺼번에 관리되지 않는다. 2. 상태 및 목표현재 나는 블로그에 관련된 코드는 하나의 repo에서 브랜치만 달리해서 관리하고 있다. (처음 시작할 때 repo를 따로 두는 것을 고민하긴 했는데 아무래도 한 곳에 있는게 관리하기 편할 것 같아 그렇게 결정했다.) 123master 브랜치 : 실제 배포되는 코드가 올라가는 브랜치포스트용 브랜치 : 포스트를 작성하는 브랜치설정 변경용 브랜치 : 여러 설정들을 테스트하거나 변경하기 전에 적용해보기 위한 브랜치 이 상태에서 themes/hueman 과 같이 테마용 브랜치를 추가해 관리하는 것을 목표로 삼았다. 2. 해결 방법 (1)어차피 cli로 themes/hueman 로 들어가면 일반적인 git repo와 동일하게 인식하므로 따로 따로 관리해주면 된다. 2.1. 로컬에서 변경 사항을 커밋 후 remote repo를 설정해준다.12git remote remove origingit remote add origin [github repo 주소] 2.2. 원격 저장소에 push1git push -u origin themes&#x2F;hueman 이렇게 하면 동일 repo의 themes/hueman 브랜치로 themes/hueman 에 있는 내용이 올라가게 된다. 2.3. clone새로운 곳에서 clone을 받을 때는 부모 repo와 자식 repo를 각각 clone 받아야 한다. 나같은 경우에는 자식 repo가 주소는 같고 브랜치만 다르므로 다음과 같이 옵션을 주어서 clone 받아야 한다. 1git clone -b [브랜치 명] [github repo 주소] [디렉토리 명] 여기서 한가지 더 필요한 것이 맨 마지막에 써준 디렉토리 명이다. 그냥 clone을 받게 되면 github repo 명으로 clone을 받게 된다. 내가 원하는 것은 hueman 디렉토리 하위에 바로 clone 받는 것이므로 다음과 같이 명령어를 실행했다. 1git clone -b themes&#x2F;hueman https:&#x2F;&#x2F;github.com&#x2F;postlude&#x2F;postlude.github.io.git themes&#x2F;hueman 이렇게 하면 다음과 같이 깔끔하게 clone 받을 수 있게 된다. 3. 해결 방법 (2) - git submodule 이용해결 방법을 찾던 도중 git에 submodule이라는 것이 있다는 것을 알게 되었다. Git Submodule 사용하기 submodule을 사용하면 부모 repo와 자식 repo가 연결된다는 장점이 있다. 또한 github에서 부모 repo에서 바로 자식 repo의 내용을 확인할 수 있다. 단점은 자식 repo 내용을 수정하고자 할 때 단순 clone과 달리 git checkout 으로 브랜치를 만들어줘야 한다. 4. 결론블로그 repo에서 테마 repo를 사용하는 것이므로 의미적으로는 submodule을 사용하는게 맞을 것 같다. 근데 생각보다 불편하다.. 명령어를 사용하는 것도 오히려 더 많이 써야 하고. 다른 경우라면 모르겠지만 블로그 코드를 관리하는데 있어서는 조금 더 고민을 해봐야 할 것 같다.","categories":[{"name":"Hexo","slug":"hexo","permalink":"https://postlude.github.io/categories/hexo/"}],"tags":[{"name":"git","slug":"git","permalink":"https://postlude.github.io/tags/git/"},{"name":"hexo","slug":"hexo","permalink":"https://postlude.github.io/tags/hexo/"}]},{"title":"hexo 블로그에서 이미지 업로드하기","slug":"hexo-image-upload","date":"2018-12-24T13:39:42.000Z","updated":"2022-02-06T08:39:50.122Z","comments":true,"path":"2018/12/24/hexo-image-upload/","link":"","permalink":"https://postlude.github.io/2018/12/24/hexo-image-upload/","excerpt":"","text":"지난 번 포스트에서 처음으로 마크 다운으로 이미지를 업로드하던 도중 범했던 오류를 정리해보고자 한다. 과정은 다음과 같았다. 이미지를 업로드 하기 위해 source/img 디렉토리를 생성하고 그 디렉토리에 이미지 파일을 넣었다. 그리고 다음과 같이 포스트를 작성했다. 일부러 오류가 났던 내용 그대로 올린 것입니다. 로컬에서는 멀쩡하게 이미지가 잘 나온다. 그런데 배포를 하면 다음과 같이 첫 번째 이미지만 정상적으로 나오고 나머지 이미지가 나오지 않는다. 로컬에서는 정상적으로 나왔기 때문에 원인을 찾기까지 조금 헤맸다. 어처구니 없었던 원인은 다음과 같았다. 코드 상에서 이미지 파일의 이름과 실제 파일 이름의 확장자가 달랐다.(대소문자 구분) 실제 파일의 확장자는 .JPG였고, 코드 상에서는 .jpg로 되어 있었기 때문에 파일을 찾지 못했던 것이었다. 확장자만 바꿔주니 바로 정상적으로 나왔다. +첫 번째 이미지는 그림판에서 저장한 jpg 파일이었고, 2~4번 이미지는 윈도우 캡쳐 도구로 저장한 이미지였다. 캡쳐 도구의 경우 default가 대문자 .JPG로 저장되는 듯.","categories":[{"name":"Hexo","slug":"hexo","permalink":"https://postlude.github.io/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://postlude.github.io/tags/hexo/"}]},{"title":"Windows에서 telnet 명령어 사용하기","slug":"windows-telnet","date":"2018-12-23T14:39:43.000Z","updated":"2022-02-06T08:39:50.205Z","comments":true,"path":"2018/12/23/windows-telnet/","link":"","permalink":"https://postlude.github.io/2018/12/23/windows-telnet/","excerpt":"","text":"리눅스 계열에서는 보통 telnet이 설치되어 있다. (없으면 그냥 yum으로 설치) 사실 윈도우에도 기본적으로 내장되어 있는데 사용하기 위해선 설정이 필요하다. 윈도우 10 기준으로 설명되어 있지만, 다른 버전도 크게 다르진 않습니다. 1. 설정 - Windows 기능 켜기/끄기 제일 아래쪽에 텔넷 클라이언트 cmd 창에서 telnet 명령어 사용 가능","categories":[{"name":"기타","slug":"기타","permalink":"https://postlude.github.io/categories/%EA%B8%B0%ED%83%80/"}],"tags":[]},{"title":"CSS word-break 속성","slug":"css-word-break","date":"2018-12-17T14:03:07.000Z","updated":"2022-02-06T08:39:50.091Z","comments":true,"path":"2018/12/17/css-word-break/","link":"","permalink":"https://postlude.github.io/2018/12/17/css-word-break/","excerpt":"","text":"글자 줄바꿈을 어떻게 할지 정하는 속성. word-break: normal; default 값으로 영어의 경우 띄어쓰기 단위로 줄바꿈 되지만 한글은 중간에서 끊긴다. word-break: break-all; 영어, 한글 모두 중간에서 끊긴다. word-break: keep-all; 영어, 한글 모두 띄어쓰기 단위로 끊긴다. 정확히는 영어는 normal로 적용되고 한글이 띄어쓰기 단위로 끊기는 것. 보다 자세한 설명은 [여기](https://www.w3schools.com/csSref/css3_pr_word-break.asp)에","categories":[{"name":"기타","slug":"기타","permalink":"https://postlude.github.io/categories/%EA%B8%B0%ED%83%80/"}],"tags":[{"name":"css","slug":"css","permalink":"https://postlude.github.io/tags/css/"}]}],"categories":[{"name":"기타","slug":"기타","permalink":"https://postlude.github.io/categories/%EA%B8%B0%ED%83%80/"},{"name":"JavaScript","slug":"javascript","permalink":"https://postlude.github.io/categories/javascript/"},{"name":"K8S","slug":"k8s","permalink":"https://postlude.github.io/categories/k8s/"},{"name":"Git","slug":"git","permalink":"https://postlude.github.io/categories/git/"},{"name":"DB","slug":"db","permalink":"https://postlude.github.io/categories/db/"},{"name":"Docker","slug":"docker","permalink":"https://postlude.github.io/categories/docker/"},{"name":"JAVA","slug":"java","permalink":"https://postlude.github.io/categories/java/"},{"name":"Hexo","slug":"hexo","permalink":"https://postlude.github.io/categories/hexo/"}],"tags":[{"name":"etc","slug":"etc","permalink":"https://postlude.github.io/tags/etc/"},{"name":"javascript","slug":"javascript","permalink":"https://postlude.github.io/tags/javascript/"},{"name":"k8s","slug":"k8s","permalink":"https://postlude.github.io/tags/k8s/"},{"name":"db","slug":"db","permalink":"https://postlude.github.io/tags/db/"},{"name":"git","slug":"git","permalink":"https://postlude.github.io/tags/git/"},{"name":"docker","slug":"docker","permalink":"https://postlude.github.io/tags/docker/"},{"name":"github","slug":"github","permalink":"https://postlude.github.io/tags/github/"},{"name":"kops","slug":"kops","permalink":"https://postlude.github.io/tags/kops/"},{"name":"mongodb","slug":"mongodb","permalink":"https://postlude.github.io/tags/mongodb/"},{"name":"nodejs","slug":"nodejs","permalink":"https://postlude.github.io/tags/nodejs/"},{"name":"job","slug":"job","permalink":"https://postlude.github.io/tags/job/"},{"name":"tool","slug":"tool","permalink":"https://postlude.github.io/tags/tool/"},{"name":"linux","slug":"linux","permalink":"https://postlude.github.io/tags/linux/"},{"name":"java","slug":"java","permalink":"https://postlude.github.io/tags/java/"},{"name":"hexo","slug":"hexo","permalink":"https://postlude.github.io/tags/hexo/"},{"name":"css","slug":"css","permalink":"https://postlude.github.io/tags/css/"}]}